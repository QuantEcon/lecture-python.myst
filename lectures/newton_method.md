---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(newton_method)=
```{raw} html
<div id="qe-notebook-header" align="right" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" width="250px" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div>
```
```{index} single: python
```

# Fast Fixed Point Computation Using Newton's Method

```{contents} Contents
:depth: 2
```

## Overview

The lecture describes application of Newton's method to calculate fixed points in one-dimentional and multi-dimentional settings. 

We consider an easy, one-dimensional fixed point problem where we know the solution first.

We will compute it by both successive approximation and Newton's method.

Then we will generalise Newton's method to a multi-dimentional setting.

The objective is to practice Python coding and investigate these algorithms.

We recall that, to find the fixed point of scalar function $g$, Newton's method iterates on 


```{math}
:label: newtons_method

x_{t+1} = \frac{g(x_t) - g'(x_t) x_t}{ 1 - g'(x_t) },
\qquad x_0 \text{ given}
```

We use the following imports

```{code-cell} python3
import numpy as np
from numpy import exp, sqrt
import matplotlib.pyplot as plt
from collections import namedtuple
from numba import njit
from scipy.optimize import root
import jax
import jax.numpy as jnp

plt.rcParams["figure.figsize"] = (10, 5.7)
```


## The Solow Model

Assuming Cobb-Douglas production technology, the law of motion for capital is

```{math}
:label: motion_law
k_{t+1} = sAk_t^\alpha + (1-\delta) k_t
```

We store the parameters in a `namedtuple`

```{code-cell} python3
SolowParameters = namedtuple("SolowParameters", ('A', 's', 'α', 'δ'))
```

This function creates a `namedtuple` of the right type and has default parameter values.


```{code-cell} python3
def create_solow_params(A=2.0, s=0.3, α=0.3, δ=0.4):
    "Creates a Solow model parameterization with default values."
    return SolowParameters(A=A, s=s, α=α, δ=δ)
```

The next two functions describe the law of motion and the true fixed point $k^*$.

```{code-cell} python3
def g(k, params):
    A, s, α, δ = params
    return A * s * k**α + (1 - δ) * k
    
def exact_fixed_point(params):
    A, s, α, δ = params
    return ((s * A) / δ)**(1/(1 - α))
```
Here is a function to provide a 45 degree plot of the dynamics.

```{code-cell} python3
def plot_45(params, ax, fontsize=14):
    
    k_min, k_max = 0, 3
    k_grid = np.linspace(k_min, k_max, 1200)

    # Plot the functions
    lb = r"$g(k) = sAk^{\alpha} + (1 - \delta)k$"
    ax.plot(k_grid, g(k_grid, params),  lw=2, alpha=0.6, label=lb)
    ax.plot(k_grid, k_grid, "k--", lw=1, alpha=0.7, label="45")

    # Show and annotate the fixed point
    kstar = exact_fixed_point(params)
    fps = (kstar,)
    ax.plot(fps, fps, "go", ms=10, alpha=0.6)
    ax.annotate(r"$k^* = (sA / \delta)^{\frac{1}{1-\alpha}}$", 
             xy=(kstar, kstar),
             xycoords="data",
             xytext=(20, -20),
             textcoords="offset points",
             fontsize=fontsize)

    ax.legend(loc="upper left", frameon=False, fontsize=fontsize)

    ax.set_yticks((0, 1, 2, 3))
    ax.set_yticklabels((0, 1, 2, 3), fontsize=fontsize)
    ax.set_ylim(0, 3)
    ax.set_xlabel("$k_t$", fontsize=fontsize)
    ax.set_ylabel("$k_{t+1}$", fontsize=fontsize)
```

Let's look at the 45 degree diagram for one or two parameterizations.

```{code-cell} python3
params = create_solow_params()
fig, ax = plt.subplots(figsize=(8, 8))
plot_45(params, ax)
plt.show()
```

```{code-cell} python3
params = create_solow_params(α=0.05, δ=0.5)
fig, ax = plt.subplots(figsize=(8, 8))
plot_45(params, ax)
plt.show()
```

Here's a time series from a particular choice of $k_0$.


```{code-cell} python3
def compute_iterates(k_0, f, params, n=25):
    "Compute time series of length n generated by arbitrary function f."
    k = k_0
    k_iterates = []
    for t in range(n):
        k_iterates.append(k)
        k = f(k, params)
    return k_iterates
```

```{code-cell} python3
params = create_solow_params()
k_0 = 0.25
k_series = compute_iterates(k_0, g, params)
k_star = exact_fixed_point(params)

fig, ax = plt.subplots()
ax.plot(k_series, 'o')
ax.plot([k_star] * len(k_series), 'k--')
ax.set_ylim(0, 3)
plt.show()
```

Since we are iterating on $g$, we are also implemening successive approximation.

```{code-cell} python3
k_series = compute_iterates(k_0, g, params, n=10_000)
k_star_approx = k_series[-1]
k_star_approx
```


```{code-cell} python3
k_star
```

## Newton's Method 

To implement Newton's method we observe that

```{math}
:label: newton_method2

g'(k) = \alpha s A k^{1-\alpha} + (1-\delta)

```


```{code-cell} python3
def Dg(k, params):
    A, s, α, δ = params
    return α * A * s * k**(α-1) + (1 - δ)
```


Here's a function $q$ such that iterating with $q$ is equivalent to Newton's method.

```{code-cell} python3
def q(k, params):
    return (g(k, params) - Dg(k, params) * k) / (1 - Dg(k, params))
```

Now let's plot some trajectories.

```{code-cell} python3
def plot_trajectories(params, 
                      k0_a=0.8,  # first initial condition
                      k0_b=3.1,  # second initial condition
                      n=20,      # length of time series
                      fs=14):    # fontsize

    fig, axes = plt.subplots(2, 1, figsize=(10, 6))
    ax1, ax2 = axes

    ks1 = compute_iterates(k0_a, g, params, n)
    ax1.plot(ks1, "-o", label="successive approximation")

    ks2 = compute_iterates(k0_b, g, params, n)
    ax2.plot(ks2, "-o", label="successive approximation")

    ks3 = compute_iterates(k0_a, q, params, n)
    ax1.plot(ks3, "-o", label="newton steps")

    ks4 = compute_iterates(k0_b, q, params, n)
    ax2.plot(ks4, "-o", label="newton steps")

    for ax in axes:
        ax.plot(k_star * np.ones(n), "k--")
        ax.legend(fontsize=fs, frameon=False)
        ax.set_ylim(0.6, 3.2)
        ax.set_yticks((k_star,))
        ax.set_yticklabels(("$k^*$",), fontsize=fs)
        
    plt.show()
```

```{code-cell} python3
params = create_solow_params()
plot_trajectories(params)
```


## Multivariate Newton’s Method

In multi-dimentional setting, the [formula](newtons_method) is written as

```{math}
:label: newton_method_multi

x_{k+1} = (I - J(x_k))^{-1}(Tx_k - J(x_k)) x_k

```
Here $J(x) := $ the Jacobian of $T$ evaluated at $x$.


### A Two Goods Market Equilibrium

Before moving to higher dimensional settings, let's compute the market equilibrium of a two-good problem.

We first consider a market for two related products, good 0 and good 1, with price vector $p = (p_0, p_1)$

Supply of good $i$ at price $p$,

$$ 
q^s_i (p) = b_i \sqrt{p_i} 
$$

Demand of good $i$ at price $p$ is,

$$ 
q^d_i (p) = \exp(-a_{i0} p_0) + \exp(-a_{i1} p_1) + c_i
$$

Here $c_i$, $b_i$ and $a_{ij}$ are parameters.

For example, the two goods might be computer components that are typically used together, in which case they are complements. Hence demand depends on the price of both components.

$$
e_i(p) = q^d_i(p) - q^s_i(p), \qquad i = 0, 1
$$


An equilibrium price vector $p^*$ satisfies $e_i(p^*) = 0$.


We set:

$$
A = \begin{pmatrix}
            a_{00} & a_{01} \\
            a_{10} & a_{11}
        \end{pmatrix},
            \qquad 
    b = \begin{pmatrix}
            b_0 \\
            b_1
        \end{pmatrix}
    \qquad \text{and} \qquad
    c = \begin{pmatrix}
            c_0 \\
            c_1
        \end{pmatrix}
$$


#### A Graphical Exploration

Since our problem is only two dimensional, we can use graphical analysis to visualize and help understand the problem.

Our first step is to define the excess demand function

$$
e(p) = 
    \begin{pmatrix}
    e_0(p) \\
    e_1(p)
    \end{pmatrix}
$$

The function below calculate the excess demand for given parameters

```{code-cell} python3
def e(p, A, b, c):
    return exp(- A @ p) + c - b * sqrt(p)
```


Our default parameter values will be


$$
A = \begin{pmatrix}
            0.5 & 0.4 \\
            0.8 & 0.2
        \end{pmatrix},
            \qquad 
    b = \begin{pmatrix}
            0 \\
            0
        \end{pmatrix}
    \qquad \text{and} \qquad
    c = \begin{pmatrix}
            0 \\
            0
        \end{pmatrix}
$$


```{code-cell} python3
A = np.array([
    [0.5, 0.4],
    [0.8, 0.2]
])
b = np.ones(2)
c = np.ones(2)
```

At a price level of $p = (p_0, p_1)$, the excess demand is 


```{code-cell} python3
ex_demand = e((1.0, 0.5), A, b, c)

print(
f'The excess demand for good 0 is {ex_demand[0]:.3f} \n'
f'The excess demand for good 1 is {ex_demand[1]:.3f}')
```


Next we plot the two functions $e_0$ and $e_1$ on a grid of $(p_0, p_1)$ values, using contour surfaces and lines.

We will use the following function to build the contour plots

```{code-cell} python3

def plot_excess_demand(ax, good=0, grid_size=100, grid_max=4, surface=True):

    # Create a 100x100 grid
    p_grid = np.linspace(0, grid_max, grid_size)
    z = np.empty((100, 100))

    for i, p_1 in enumerate(p_grid):
        for j, p_2 in enumerate(p_grid):
            z[i, j] = e((p_1, p_2), A, b, c)[good]

    if surface:
        cs1 = ax.contourf(p_grid, p_grid, z.T, alpha=0.5)
        plt.colorbar(cs1, ax=ax, format="%.6f")

    ctr1 = ax.contour(p_grid, p_grid, z.T, levels=[0.0])
    ax.set_xlabel(r'p_0')
    ax.set_ylabel(r'p_1')
    ax.set_title(f'Excess Demand for Good {good}')
    plt.clabel(ctr1, inline=1, fontsize=13)
```

Here's our plot of $e_0$:
```{code-cell} python3
fig, ax = plt.subplots()
plot_excess_demand(ax, good=0)
plt.show()
```

Here's our plot of $e_1$:
```{code-cell} python3
fig, ax = plt.subplots()
plot_excess_demand(ax, good=1)
plt.show()
```

We see the black contour line of zero, which tells us when $e_i(p)=0$.
For a price vector $p$ such that $e_i(p)=0$ we know that good $i$ is in equilibrium (demand equals supply).


If these two contour lines cross at some price vector $p^*$, then $p^*$ is an equilibrium price vector.


```{code-cell} python3
fig, ax = plt.subplots(figsize=(10, 5.7))
for good in (0, 1):
    plot_excess_demand(ax, good=good, surface=False)
plt.show()
```

It seems there is an equilibrium close to $p = (1.6, 1.5)$.


#### Using a Multidimensional Root Finder

To solve for $p^*$ more precisely, we use root, a root-finding algorithm from `scipy.optimize`.

We supply $p = (1, 1)$ as our initial guess.

```{code-cell} python3
init_p = np.ones(2)
```

This uses the [modified Powell method](https://docs.scipy.org/doc/scipy/reference/optimize.root-hybr.html#optimize-root-hybr) to find the root

```{code-cell} python3
%%time
solution = root(lambda p: e(p, A, b, c), init_p, method='hybr')
```

Here's the resulting value:

```{code-cell} python3
p = solution.x
p
```

This looks close to our guess from observing the figure. We can plug it back into $e$ to test that $e(p) \approx 0$:

```{code-cell} python3
np.max(np.abs(e(p, A, b, c)))
```

This is indeed a very small error.

In most cases, for root-finding algorithms applied to smooth functions, supplying the Jacobian of the function leads to better convergence properties.

In this case we manually calculate the elements of the Jacobian

$$
J(p) = 
    \begin{pmatrix}
        \frac{\partial e_0}{\partial p_0}(p) & \frac{\partial e_0}{\partial p_1}(p) \\
        \frac{\partial e_1}{\partial p_0}(p) & \frac{\partial e_1}{\partial p_1}(p)
    \end{pmatrix}
$$

```{code-cell} python3
def jacobian(p, A, b, c):
    p_0, p_1 = p
    a_00, a_01 = A[0, :]
    a_10, a_11 = A[1, :]
    j_00 = -a_00 * exp(-a_00 * p_0) - (b[0]/2) * p_0**(-1/2)
    j_01 = -a_01 * exp(-a_01 * p_1)
    j_10 = -a_10 * exp(-a_10 * p_0)
    j_11 = -a_11 * exp(-a_11 * p_1) - (b[1]/2) * p_1**(-1/2)
    J = [[j_00, j_01],
         [j_10, j_11]]
    return np.array(J)
```

```{code-cell} python3
%%time
solution = root(lambda p: e(p, A, b, c),
                init_p, 
                jac=lambda p: jacobian(p, A, b, c), 
                method='hybr')
```

Now the solution is even more accurate (although, in this low-dimensional problem, the difference is quite small):

```{code-cell} python3
p = solution.x
np.max(np.abs(e(p, A, b, c)))
```

We can also use Newton's method in this lower dimensional case. 

We are going to try to compute the equilibrium price using the multivariate version of Newton's method, which means iterating on the equation:

$$
p_{n+1} = p_n - J(p_n)^{-1} e(p_n)
$$

starting from some initial guess of the price vector $p_0$. (Here $J$ is the Jacobian of $e$.)

We use the `jax.jacobian()` function to auto differentiate and calculate the jacobian

```{code-cell} python3

def newton(f, x_0, tol=1e-5, maxIter=100):
    iteration = jax.jit(lambda x: x - jnp.linalg.solve(jax.jacobian(f)(x), f(x)))
    error = tol + 1
    x = x_0
    n = 0
    while error > tol and n <= maxIter:
        n+=1
        y = iteration(x)
        error = jnp.linalg.norm(x - y)
        x = y
        print(f'iteration {n}: error = {error:.5f}')
    if(n == maxIter+1):
        raise Exception('Max iteration reached without convergence')
    return x
```

```{code-cell} python3
@jax.jit
def e(p, A, b, c):
    return jnp.exp(- jnp.dot(A, p)) + c - b * jnp.sqrt(p)
```

We find the convergence is reached within 5 steps

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), init_p).block_until_ready()
```

With the larger overhead for Newton's method, the performance is not better than the optimised `scipy` optimization function.

```{code-cell} python3
p = solution.x
np.max(np.abs(e(p, A, b, c)))
```

However, things will change slightly when we move to higher dimensional problems.


### High-Dimensional Problems

Our next step is to investigate a high-dimensional version of the market described above. This market consists of 2,500 goods.

The excess demand function is essentially the same, but now the matrix $A$ is $5000 \times 5000$ and the parameter vectors $b$ and $c$ are $5000 \times 1$.


```{code-cell} python3
dim = 5000
np.random.seed(123)

# Create a random matrix A and normalize the rows to sum to one
A = np.random.rand(dim, dim)
A = np.asarray(A)
s = np.sum(A, axis=0)
A = A / s

# Set up b and c
b = np.ones(dim)
c = np.ones(dim)
```

Here's the same demand function expressed in matrix syntax:

```{code-cell} python3
@jax.jit
def e(p, A, b, c):
    return jnp.exp(- jnp.dot(A, p)) + c - b * jnp.sqrt(p)
```

Here's our initial condition

```{code-cell} python3
init_p = jnp.ones(dim)
```

The `root` function would cost several minutes to run in this case

```python
%%time
solution = root(lambda p: e(p, A, b, c),
                init_p, 
                jac=lambda p: jax.jacobian(e)(p, A, b, c), 
                method='hybr')
```

Newton's method reaches a relatively small error within a minute

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), init_p).block_until_ready()
```

```{code-cell} python3
np.max(np.abs(e(p, A, b, c)))
```





```{exercise-start}
:label: newton_ex1
```

In this exercise, please try to use different initial values and check how Newton's method will respond to different starting points.

Let's define a three-good problem with the following default values:

$$
A = \begin{pmatrix}
            0.2 & 0.1 & 0.7 \\
            0.3 & 0.2 & 0.5 \\
            0.1 & 0.8 & 0.1 \\
        \end{pmatrix},
            \qquad 
    b = \begin{pmatrix}
            1 \\
            1 \\
            1
        \end{pmatrix}
    \qquad \text{and} \qquad
    c = \begin{pmatrix}
            1 \\
            1 \\
            1
        \end{pmatrix}
$$

Please use the following price vectors as our initial values:

$$
p_1 = (1, 1, 1)
$$

$$
p_2 = (1, 2, 3)
$$

$$
p_3 = (5, 5, 5)
$$

```{exercise-end}
```

```{solution-start} newton_ex1
:class: dropdown
```

```{code-cell} python3
A = np.array([
    [0.2, 0.1, 0.7],
    [0.3, 0.2, 0.5],
    [0.1, 0.8, 0.1]
])

b = np.array([1.0, 1.0, 1.0])
c = np.array([1.0, 1.0, 1.0])
```

```{code-cell} python3
initLs = [np.ones(3), np.array([1.0,2.0,3.0]), np.array([5.0,5.0,5.0])]
```

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), initLs[0]).block_until_ready()
```

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), initLs[1]).block_until_ready()
```

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), initLs[2]).block_until_ready()
```

```{solution-end}
```



```{exercise-start}
:label: newton_ex2
```

```{exercise-end}
```

```{solution-start} newton_ex2
:class: dropdown
```

```{solution-start} newton_ex2
A = np.array([2.0, 1.0])
s = np.array([0.3, 0.2])
α = np.array([0.3, 0.7])
δ = np.array([0.4, 0.5])

def multi(k, A, s, α, δ):
    return A * s * k ** α + (1 - δ) * k

multi(0.25, A, s, α, δ)
```

```{solution-end}
```