---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(newton_method)=
```{raw} html
<div id="qe-notebook-header" align="right" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" width="250px" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div>
```
```{index} single: python
```

# Application of Newton's Method

```{contents} Contents
:depth: 2
```

## Overview

The lecture will apply Newton's method in one-dimensional and multi-dimensional settings to solve fixed-point and root-finding problems. 

We first consider an easy, one-dimensional fixed point problem where we know the solution and solve it using both successive approximation and Newton's method.

Then we generalize Newton's method to multi-dimensional settings to solve market equilibrium with multiple goods.

We use the following imports in this lecture

```{code-cell} python3
import numpy as np
from numpy import exp, sqrt
import matplotlib.pyplot as plt
from collections import namedtuple
from scipy.optimize import root
import jax
import jax.numpy as jnp

plt.rcParams["figure.figsize"] = (10, 5.7)
```

## One-dimensional Fixed Point Computation Using Newton's Method

To find the fixed point of a scalar function $g$, Newton's method iterates on 

```{math}
:label: newtons_method

x_{t+1} = \frac{g(x_t) - g'(x_t) x_t}{ 1 - g'(x_t) },
\qquad x_0 \text{ given}
```

(solow)=
### The Solow Model

Assuming Cobb-Douglas production technology, the law of motion for capital is

```{math}
:label: motion_law
k_{t+1} = sAk_t^\alpha + (1-\delta) k_t
```

We store the parameters in a [`namedtuple`](https://docs.python.org/3/library/collections.html#collections.namedtuple)

```{code-cell} python3
SolowParameters = namedtuple("SolowParameters", ('A', 's', 'α', 'δ'))
```

This function creates a `namedtuple` of the right type and has default parameter values.


```{code-cell} python3
def create_solow_params(A=2.0, s=0.3, α=0.3, δ=0.4):
    "Creates a Solow model parameterization with default values."
    return SolowParameters(A=A, s=s, α=α, δ=δ)
```

The next two functions describe the law of motion and the true fixed point $k^*$.

```{code-cell} python3
def g(k, params):
    A, s, α, δ = params
    return A * s * k**α + (1 - δ) * k
    
def exact_fixed_point(params):
    A, s, α, δ = params
    return ((s * A) / δ)**(1/(1 - α))
```
Here is a function to provide a 45 degree plot of the dynamics.

```{code-cell} python3
def plot_45(params, ax, fontsize=14):
    
    k_min, k_max = 0.0, 3.0
    k_grid = np.linspace(k_min, k_max, 1200)

    # Plot the functions
    lb = r"$g(k) = sAk^{\alpha} + (1 - \delta)k$"
    ax.plot(k_grid, g(k_grid, params),  lw=2, alpha=0.6, label=lb)
    ax.plot(k_grid, k_grid, "k--", lw=1, alpha=0.7, label="45")

    # Show and annotate the fixed point
    kstar = exact_fixed_point(params)
    fps = (kstar,)
    ax.plot(fps, fps, "go", ms=10, alpha=0.6)
    ax.annotate(r"$k^* = (sA / \delta)^{\frac{1}{1-\alpha}}$", 
             xy=(kstar, kstar),
             xycoords="data",
             xytext=(20, -20),
             textcoords="offset points",
             fontsize=fontsize)

    ax.legend(loc="upper left", frameon=False, fontsize=fontsize)

    ax.set_yticks((0, 1, 2, 3))
    ax.set_yticklabels((0.0, 1.0, 2.0, 3.0), fontsize=fontsize)
    ax.set_ylim(0, 3)
    ax.set_xlabel("$k_t$", fontsize=fontsize)
    ax.set_ylabel("$k_{t+1}$", fontsize=fontsize)
```

Let's look at the 45 degree diagram for two parameterizations.

```{code-cell} python3
params = create_solow_params()
fig, ax = plt.subplots(figsize=(8, 8))
plot_45(params, ax)
plt.show()
```

```{code-cell} python3
params = create_solow_params(α=0.05, δ=0.5)
fig, ax = plt.subplots(figsize=(8, 8))
plot_45(params, ax)
plt.show()
```

#### Successive Approximation

First, let's compute the fixed point using successive approximation.

Here's a time series from a particular choice of $k_0$.


```{code-cell} python3
def compute_iterates(k_0, f, params, n=25):
    "Compute time series of length n generated by arbitrary function f."
    k = k_0
    k_iterates = []
    for t in range(n):
        k_iterates.append(k)
        k = f(k, params)
    return k_iterates
```

```{code-cell} python3
params = create_solow_params()
k_0 = 0.25
k_series = compute_iterates(k_0, g, params)
k_star = exact_fixed_point(params)

fig, ax = plt.subplots()
ax.plot(k_series, 'o')
ax.plot([k_star] * len(k_series), 'k--')
ax.set_ylim(0, 3)
plt.show()
```

Since we are iterating on $g$, we are implementing successive approximation

```{code-cell} python3
k_series = compute_iterates(k_0, g, params, n=10_000)
k_star_approx = k_series[-1]
k_star_approx
```

(solved_k)=
```{code-cell} python3
k_star
```

#### Newton's Method 

To implement Newton's method we observe that

```{math}
:label: newton_method2

g'(k) = \alpha s A k^{\alpha-1} + (1-\delta)

```


```{code-cell} python3
def Dg(k, params):
    A, s, α, δ = params
    return α * A * s * k**(α-1) + (1 - δ)
```

Here's a function $q$ such that iterating with $q$ is equivalent to Newton's method.

```{code-cell} python3
def q(k, params):
    return (g(k, params) - Dg(k, params) * k) / (1 - Dg(k, params))
```

Now let's plot some trajectories.

```{code-cell} python3
def plot_trajectories(params, 
                      k0_a=0.8,  # first initial condition
                      k0_b=3.1,  # second initial condition
                      n=20,      # length of time series
                      fs=14):    # fontsize

    fig, axes = plt.subplots(2, 1, figsize=(10, 6))
    ax1, ax2 = axes

    ks1 = compute_iterates(k0_a, g, params, n)
    ax1.plot(ks1, "-o", label="successive approximation")

    ks2 = compute_iterates(k0_b, g, params, n)
    ax2.plot(ks2, "-o", label="successive approximation")

    ks3 = compute_iterates(k0_a, q, params, n)
    ax1.plot(ks3, "-o", label="newton steps")

    ks4 = compute_iterates(k0_b, q, params, n)
    ax2.plot(ks4, "-o", label="newton steps")

    for ax in axes:
        ax.plot(k_star * np.ones(n), "k--")
        ax.legend(fontsize=fs, frameon=False)
        ax.set_ylim(0.6, 3.2)
        ax.set_yticks((k_star,))
        ax.set_yticklabels(("$k^*$",), fontsize=fs)
        ax.set_xticks(np.linspace(0, 19, 20))
        
    plt.show()
```

```{code-cell} python3
params = create_solow_params()
plot_trajectories(params)
```

We can see that Newton's Method reaches convergence faster than the successive approximation.

The above problem can be seen as a root-finding problem since the computation of a fixed point can be seen as approximating $x^*$ iteratively such that $g(x^*) - x^* = 0$.

For one-dimensional root-finding problems, Newton's method iterates on:

$$
x_{t+1} = x_t - \frac{ g(x_t) }{ g'(x_t) },
\qquad x_0 \text{ given}
$$

This is also a more familiar formula for Newton's method.

The following code implements the iteration

```{code-cell} python3
def newton(g, Dg, x_0, tol, params=params, maxIter=10):
    x = x_0

    # Implement the one-dimensional Newton's method
    iteration = lambda x, params: x - g(x, params)/Dg(x, params)

    error = tol + 1
    n = 0
    while error > tol:
        n+=1
        if(n > maxIter):
            raise Exception('Max iteration reached without convergence')
        y = iteration(x, params)
        error = jnp.abs(x - y)
        x = y
        print(f'iteration {n}, error = {error:.5f}')
    return x
```

```{code-cell} python3
# Apply our transformation
k_star_approx_newton = newton(
                        g=lambda x, params: g(x, params) - x,
                        Dg=lambda x, params: Dg(x, params) - 1,
                        x_0=0.8,
                        tol=1e-7)
```

```{code-cell} python3
k_star_approx_newton
```

The result confirms the descent we saw in the graphs above: a very accurate result is reached with only 5 iterations.

The multi-dimensional variant will be left as an [exercise](newton_ex1).

By observing the formula of Newton's method, it is easy to see the possibility to implement Newton's method using Jacobian when we move up the ladder to higher dimensions.

This naturally leads us to use Newton's method to solve multi-dimensional problems for which we will use the powerful auto-differentiation functionality in `jax` to solve intricate calculations.

## Multivariate Newton’s Method

### A Two Goods Market Equilibrium

Before moving to higher dimensional settings, let's compute the market equilibrium of a two-good problem.

We first consider a market for two related products, good 0 and good 1, with price vector $p = (p_0, p_1)$

Supply of good $i$ at price $p$,

$$ 
q^s_i (p) = b_i \sqrt{p_i} 
$$

Demand of good $i$ at price $p$ is,

$$ 
q^d_i (p) = \exp(-a_{i0} p_0) + \exp(-a_{i1} p_1) + c_i
$$

Here $c_i$, $b_i$ and $a_{ij}$ are parameters.

For example, the two goods might be computer components that are typically used together, in which case they are complements. Hence demand depends on the price of both components.

The excess demand function is,

$$
e_i(p) = q^d_i(p) - q^s_i(p), \qquad i = 0, 1
$$


An equilibrium price vector $p^*$ satisfies $e_i(p^*) = 0$.


We set

$$
A = \begin{pmatrix}
            a_{00} & a_{01} \\
            a_{10} & a_{11}
        \end{pmatrix},
            \qquad 
    b = \begin{pmatrix}
            b_0 \\
            b_1
        \end{pmatrix}
    \qquad \text{and} \qquad
    c = \begin{pmatrix}
            c_0 \\
            c_1
        \end{pmatrix}
$$

for this particular question.

#### A Graphical Exploration

Since our problem is only two-dimensional, we can use graphical analysis to visualize and help understand the problem.

Our first step is to define the excess demand function

$$
e(p) = 
    \begin{pmatrix}
    e_0(p) \\
    e_1(p)
    \end{pmatrix}
$$

The function below calculates the excess demand for given parameters

```{code-cell} python3
def e(p, A, b, c):
    return exp(- A @ p) + c - b * sqrt(p)
```


Our default parameter values will be


$$
A = \begin{pmatrix}
            0.5 & 0.4 \\
            0.8 & 0.2
        \end{pmatrix},
            \qquad 
    b = \begin{pmatrix}
            0 \\
            0
        \end{pmatrix}
    \qquad \text{and} \qquad
    c = \begin{pmatrix}
            0 \\
            0
        \end{pmatrix}
$$


```{code-cell} python3
A = np.array([
    [0.5, 0.4],
    [0.8, 0.2]
])
b = np.ones(2)
c = np.ones(2)
```

At a price level of $p = (1, 0.5)$, the excess demand is 


```{code-cell} python3
ex_demand = e((1.0, 0.5), A, b, c)

print(f'The excess demand for good 0 is {ex_demand[0]:.3f} \n'
      f'The excess demand for good 1 is {ex_demand[1]:.3f}')
```


Next we plot the two functions $e_0$ and $e_1$ on a grid of $(p_0, p_1)$ values, using contour surfaces and lines.

We will use the following function to build the contour plots

```{code-cell} python3

def plot_excess_demand(ax, good=0, grid_size=100, grid_max=4, surface=True):

    # Create a 100x100 grid
    p_grid = np.linspace(0, grid_max, grid_size)
    z = np.empty((100, 100))

    for i, p_1 in enumerate(p_grid):
        for j, p_2 in enumerate(p_grid):
            z[i, j] = e((p_1, p_2), A, b, c)[good]

    if surface:
        cs1 = ax.contourf(p_grid, p_grid, z.T, alpha=0.5)
        plt.colorbar(cs1, ax=ax, format="%.6f")

    ctr1 = ax.contour(p_grid, p_grid, z.T, levels=[0.0])
    ax.set_xlabel("$p_0$")
    ax.set_ylabel("$p_1$")
    ax.set_title(f'Excess Demand for Good {good}')
    plt.clabel(ctr1, inline=1, fontsize=13)
```

Here's our plot of $e_0$:
```{code-cell} python3
fig, ax = plt.subplots()
plot_excess_demand(ax, good=0)
plt.show()
```

Here's our plot of $e_1$:
```{code-cell} python3
fig, ax = plt.subplots()
plot_excess_demand(ax, good=1)
plt.show()
```

We see the black contour line of zero, which tells us when $e_i(p)=0$.

For a price vector $p$ such that $e_i(p)=0$ we know that good $i$ is in equilibrium (demand equals supply).


If these two contour lines cross at some price vector $p^*$, then $p^*$ is an equilibrium price vector.


```{code-cell} python3
fig, ax = plt.subplots(figsize=(10, 5.7))
for good in (0, 1):
    plot_excess_demand(ax, good=good, surface=False)
plt.show()
```

It seems there is an equilibrium close to $p = (1.6, 1.5)$.


#### Using a Multidimensional Root Finder

To solve for $p^*$ more precisely, we use a root-finding algorithm from `scipy.optimize`.

We supply $p = (1, 1)$ as our initial guess.

```{code-cell} python3
init_p = np.ones(2)
```

This uses the [modified Powell method](https://docs.scipy.org/doc/scipy/reference/optimize.root-hybr.html#optimize-root-hybr) to find the root

```{code-cell} python3
%%time
solution = root(lambda p: e(p, A, b, c), init_p, method='hybr')
```

Here's the resulting value:

```{code-cell} python3
p = solution.x
p
```

This looks close to our guess from observing the figure. We can plug it back into $e$ to test that $e(p) \approx 0$:

```{code-cell} python3
np.max(np.abs(e(p, A, b, c)))
```

This is indeed a very small error.

In most cases, for root-finding algorithms applied to smooth functions, supplying the Jacobian of the function leads to better convergence properties.

Here we manually calculate the elements of the Jacobian

$$
J(p) = 
    \begin{pmatrix}
        \frac{\partial e_0}{\partial p_0}(p) & \frac{\partial e_0}{\partial p_1}(p) \\
        \frac{\partial e_1}{\partial p_0}(p) & \frac{\partial e_1}{\partial p_1}(p)
    \end{pmatrix}
$$

```{code-cell} python3
def jacobian(p, A, b, c):
    p_0, p_1 = p
    a_00, a_01 = A[0, :]
    a_10, a_11 = A[1, :]
    j_00 = -a_00 * exp(-a_00 * p_0) - (b[0]/2) * p_0**(-1/2)
    j_01 = -a_01 * exp(-a_01 * p_1)
    j_10 = -a_10 * exp(-a_10 * p_0)
    j_11 = -a_11 * exp(-a_11 * p_1) - (b[1]/2) * p_1**(-1/2)
    J = [[j_00, j_01],
         [j_10, j_11]]
    return np.array(J)
```

```{code-cell} python3 
%%time
solution = root(lambda p: e(p, A, b, c),
                init_p, 
                jac=lambda p: jacobian(p, A, b, c), 
                method='hybr')
```

Now the solution is even more accurate (although, in this low-dimensional problem, the difference is quite small):

```{code-cell} python3
p = solution.x
np.max(np.abs(e(p, A, b, c)))
```

#### Using Newton's Method

We can also use Newton's method to find the root. 

We are going to try to compute the equilibrium price using the multivariate version of Newton's method, which means iterating on the equation:

$$
p_{n+1} = p_n - J_e(p_n)^{-1} e(p_n)
$$

starting from some initial guess of the price vector $p_0$. (Here $J_e(p_n)$ is the Jacobian of $e$ evaluated at $p_n$.)

We use the `jax.jacobian()` function to auto-differentiate and calculate the jacobian.

With only slight modification, we can generalize our previous attempt to multi-dimensional problems

```{code-cell} python3
def newton(f, x_0, tol=1e-5, maxIter=10):
    x = x_0
    iteration = jax.jit(lambda x: x - jnp.linalg.solve(jax.jacobian(f)(x), f(x)))
    error = tol + 1
    n = 0
    while error > tol:
        n+=1
        if(n > maxIter):
            raise Exception('Max iteration reached without convergence')
        y = iteration(x)
        if(any(jnp.isnan(y))):
            raise Exception('Solution not found with NaN generated')
        error = jnp.linalg.norm(x - y)
        x = y
        print(f'iteration {n}, error = {error:.5f}')
    return x
```

```{code-cell} python3
@jax.jit
def e(p, A, b, c):
    return jnp.exp(- jnp.dot(A, p)) + c - b * jnp.sqrt(p)
```

We find the convergence is reached in 4 steps

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), init_p).block_until_ready()
```

```{code-cell} python3
np.max(np.abs(e(p, A, b, c)))
```

The error is almost 0. 

With the larger overhead, the speed is not better than the optimized `scipy` function.

However, things will change slightly when we move to higher dimensional problems.



### A High-Dimensional Problem

Our next step is to investigate a larger market with 5000 goods.

The excess demand function is essentially the same, but now the matrix $A$ is $5000 \times 5000$ and the parameter vectors $b$ and $c$ are $5000 \times 1$.


```{code-cell} python3
dim = 5000
np.random.seed(123)

# Create a random matrix A and normalize the rows to sum to one
A = np.random.rand(dim, dim)
A = np.asarray(A)
s = np.sum(A, axis=0)
A = A / s

# Set up b and c
b = np.ones(dim)
c = np.ones(dim)
```

Here's the same demand function expressed in matrix syntax:

```{code-cell} python3
@jax.jit
def e(p, A, b, c):
    return jnp.exp(- jnp.dot(A, p)) + c - b * jnp.sqrt(p)
```

Here's our initial condition

```{code-cell} python3
init_p = jnp.ones(dim)
```

Newton's method reaches a relatively small error within a minute

```{code-cell} python3
%%time
p = newton(lambda p: e(p, A, b, c), init_p).block_until_ready()
```

```{code-cell} python3
np.max(np.abs(e(p, A, b, c)))
```

With the same tolerance, the `root` function would cost minutes to run with jacobian supplied


```{code-cell} python3
%%time
solution = root(lambda p: e(p, A, b, c),
                init_p, 
                jac=lambda p: jax.jacobian(e)(p, A, b, c), 
                method='hybr',
                tol=1e-5)
```

```{code-cell} python3
p = solution.x
np.max(np.abs(e(p, A, b, c)))
```

And the result is less accurate.

## Exercises

```{exercise-start}
:label: newton_ex1
```

Consider a three-dimensional extension of the same fixed point problem we have solved [before](solow) with

$$
A = \begin{pmatrix}
            2 & 3 & 3 \\
            2 & 4 & 2 \\
            1 & 5 & 1 \\
        \end{pmatrix},
            \quad
s = 0.2, \quad α = 0.5, \quad δ = 0.8
$$

In this exercise, solve the fixed point using Newton's method with the following initial values:

$$
\begin{aligned}
    k_1 &= (1, 1, 1) \\
    k_2 &= (3, 5, 5) \\
    k_3 &= (9, 9, 9) \\
    k_4 &= (100, 100, 100)
\end{aligned}
$$


Set the tolerance to $1\text{e-}7$ for more accurate output.

````{hint} 
:class: dropdown

- The computation of fixed point can be seen as computing $k^*$ such that $f(k^*) - k^* = 0$.

- If you are unsure about your solution, you can start with the known solution to check your formula:

```{math}
A = \begin{pmatrix}
            2 & 0 & 0 \\
            0 & 2 & 0 \\
            0 & 0 & 2 \\
        \end{pmatrix}
```

with $s = 0.3$, $α = 0.3$, and $δ = 0.4$ and starting value: 

```{math}
k_0 = \begin{pmatrix}
            1 \\
            1 \\
            1
        \end{pmatrix}
```

The result should converge to the [solved solution in the one-dimensional problem](solved_k).
````

```{exercise-end}
```


```{solution-start} newton_ex1
:class: dropdown
```

Let's first define the parameters for this problem

```{code-cell} python3
A = jnp.array([[2.0, 3.0, 3.0],
               [2.0, 4.0, 2.0],
               [1.0, 5.0, 1.0]])

s = 0.2
α = 0.5
δ = 0.8

initLs = [jnp.ones(3),
          jnp.array([3.0, 5.0, 5.0]),
          jnp.repeat(9.0, 3),
          jnp.repeat(100.0, 3)]
```

Then define the multivariate version of the formula for the [law of motion of captial](motion_law)

```{code-cell} python3
def multivariate_solow(k, A=A, s=s, α=α, δ=δ):
    return (s * jnp.dot(A, k**α) + (1 - δ) * k)
```

Let's run through each starting value and see the output

```{code-cell} python3
attempt = 1
for init in initLs:
    print(f'Attempt {attempt}: Starting value is {init} \n')

    %time k = newton(lambda k: multivariate_solow(k) - k, \
                     init, \
                     tol=1e-7).block_until_ready()

    print('\n' + f'Result = {k} \n')
    print('-'*64)
    attempt +=1
```

We find that the results are invariant to the starting values given the well-defined property of this question.

But the number of iterations it takes to converge is dependent on the starting values.

Substitute it back to the formulate to check our result

```{code-cell} python3
multivariate_solow(k)
```

Note the error is very small.

We can also test our results on the known solution

```{code-cell} python3
A = jnp.array([[2.0, 0.0, 0.0],
               [0.0, 2.0, 0.0],
               [0.0, 0.0, 2.0]])

s = 0.3
α = 0.3
δ = 0.4

init = jnp.repeat(1.0, 3)


%time k = newton(lambda k: multivariate_solow(k, A=A, s=s, α=α, δ=δ) - k, \
                 init, \
                 tol=1e-7).block_until_ready()
```

```{code-cell} python3
k
```

```{solution-end}
```


```{exercise-start}
:label: newton_ex2
```

In this exercise, let's try different initial values and check how Newton's method responds to different starting points.

Let's define a three-good problem with the following default values:

$$
A = \begin{pmatrix}
            0.2 & 0.1 & 0.7 \\
            0.3 & 0.2 & 0.5 \\
            0.1 & 0.8 & 0.1 \\
        \end{pmatrix},
            \qquad 
b = \begin{pmatrix}
            1 \\
            1 \\
            1
        \end{pmatrix}
    \qquad \text{and} \qquad
c = \begin{pmatrix}
            1 \\
            1 \\
            1
        \end{pmatrix}
$$

For this exercise, use the following price vectors as initial values:

$$
p_1 = (1, 1, 1) \\
p_2 = (1, 2, 3) \\
p_3 = (5, 5, 5) \\
$$

Set the tolerance to $1\text{e-}7$ for more accurate output.

```{exercise-end}
```

```{solution-start} newton_ex2
:class: dropdown
```

Define parameters and initial values

```{code-cell} python3
A = np.array([
    [0.2, 0.1, 0.7],
    [0.3, 0.2, 0.5],
    [0.1, 0.8, 0.1]
])

b = np.array([1.0, 1.0, 1.0])
c = np.array([1.0, 1.0, 1.0])

initLs = [np.ones(3),
          np.array([5.0, 5.0, 5.0]),
          np.array([1.0, 2.0, 3.0])]
```

Let’s run through each initial guess and check the output

```{code-cell} python3

%time p = newton(lambda p: e(p, A, b, c), \
                 initLs[0], \
                 tol=1e-7).block_until_ready()
```

```{code-cell} python3

%time p = newton(lambda p: e(p, A, b, c), \
                 initLs[1], \
                 tol=1e-7).block_until_ready()
```

```{code-cell} python3

%time p = newton(lambda p: e(p, A, b, c), \
                 initLs[2], \
                 tol=1e-7).block_until_ready()
print('\n' + f'Result = {p} \n')
```


We can find that Newton's method may fail for some starting values.

Sometimes it may take a few initial guesses to achieve convergence.

Substitute it back to the formula to check our result

```{code-cell} python3
e(p, A, b, c)
```

We can see the result is very accurate.

```{solution-end}
```