
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>81. Maximum Likelihood Estimation &#8212; Intermediate Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" href="_static/styles/quantecon-book-theme.css?digest=3bd9fcddbe64f63e07c8604843d1cc622a07b430" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css?v=982b99e0" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>


    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/scripts/quantecon-book-theme.js?digest=d9faaf6c4b57726f74ba012412af1f5681bdff87"></script>
    <script src="_static/scripts/jquery.js?v=5d32c60e"></script>
    <script src="_static/scripts/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-J0SMYR4SG3"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-J0SMYR4SG3');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-J0SMYR4SG3');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'mle';</script>
    <link rel="canonical" href="https://python.quantecon.org/mle.html" />
    <link rel="icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="82. First-Price and Second-Price Auctions" href="two_auctions.html" />
    <link rel="prev" title="80. Linear Regression in Python" href="ols.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Maximum Likelihood Estimation"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Maximum Likelihood Estimation" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python.quantecon.org/mle.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Intermediate Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>

<!-- Override QuantEcon theme colors -->

    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=mle>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">81.1. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">81.1.1. Prerequisites</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-and-assumptions">81.2. Set up and assumptions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-of-ideas">81.2.1. Flow of ideas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-billionaires">81.2.2. Counting billionaires</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-distributions">81.3. Conditional distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">81.4. Maximum likelihood estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-with-numerical-methods">81.5. MLE with numerical methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-with-statsmodels">81.6. Maximum likelihood estimation with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">81.7. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">81.8. Exercises</a></li>
</ul>
                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo logo-img" alt="logo"></a>
                                    
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Intermediate Quantitative Economics with Python</a></p>

                        <p class="qe-page__header-subheading">Maximum Likelihood Estimation</p>

                    </div>
                    <!-- length 2, since its a string and empty dict has length 2 - {} -->
                        <p class="qe-page__header-authors" font-size="18">
                            
                                
                                    <a href="http://www.tomsargent.com/" target="_blank"><span>Thomas J. Sargent</span></a>
                                
                            
                                
                                    and <a href="https://johnstachurski.net/" target="_blank"><span>John Stachurski</span></a>
                                
                            
                        </p>


                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <section class="tex2jax_ignore mathjax_ignore" id="maximum-likelihood-estimation">
<h1><a class="toc-backref" href="#id3" role="doc-backlink"><span class="section-number">81. </span>Maximum Likelihood Estimation</a><a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h1>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#maximum-likelihood-estimation" id="id3">Maximum Likelihood Estimation</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id4">Overview</a></p></li>
<li><p><a class="reference internal" href="#set-up-and-assumptions" id="id5">Set up and assumptions</a></p></li>
<li><p><a class="reference internal" href="#conditional-distributions" id="id6">Conditional distributions</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id7">Maximum likelihood estimation</a></p></li>
<li><p><a class="reference internal" href="#mle-with-numerical-methods" id="id8">MLE with numerical methods</a></p></li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation-with-statsmodels" id="id9">Maximum likelihood estimation with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code></a></p></li>
<li><p><a class="reference internal" href="#summary" id="id10">Summary</a></p></li>
<li><p><a class="reference internal" href="#exercises" id="id11">Exercises</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id4" role="doc-backlink"><span class="section-number">81.1. </span>Overview</a><a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>In <a class="reference internal" href="ols.html"><span class="doc">Linear Regression in Python</span></a>, we estimated the relationship between
dependent and explanatory variables using linear regression.</p>
<p>But what if a linear relationship is not an appropriate assumption for our model?</p>
<p>One widely used alternative is maximum likelihood estimation, which
involves specifying a class of distributions, indexed by unknown parameters, and then using the data to pin down these parameter values.</p>
<p>The benefit relative to linear regression is that it allows more flexibility in the probabilistic relationships between variables.</p>
<p>Here we illustrate maximum likelihood by replicating Daniel Treisman’s (2016) paper, <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/aer.p20161068">Russia’s Billionaires</a>, which connects the number of billionaires in a country to its economic characteristics.</p>
<p>The paper concludes that Russia has a higher number of billionaires than
economic factors such as market size and tax rate predict.</p>
<p>We’ll require the following imports:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">NamedTuple</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">jax.scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">factorial</span><span class="p">,</span> <span class="n">gammaln</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">Poisson</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.iolib.summary2</span><span class="w"> </span><span class="kn">import</span> <span class="n">summary_col</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>
</pre></div>
</div>
</div>
</div>
<section id="prerequisites">
<h3><span class="section-number">81.1.1. </span>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h3>
<p>We assume familiarity with basic probability and multivariate calculus.</p>
</section>
</section>
<section id="set-up-and-assumptions">
<h2><a class="toc-backref" href="#id5" role="doc-backlink"><span class="section-number">81.2. </span>Set up and assumptions</a><a class="headerlink" href="#set-up-and-assumptions" title="Link to this heading">#</a></h2>
<p>Let’s consider the steps we need to go through in maximum likelihood estimation and how they pertain to this study.</p>
<section id="flow-of-ideas">
<h3><span class="section-number">81.2.1. </span>Flow of ideas<a class="headerlink" href="#flow-of-ideas" title="Link to this heading">#</a></h3>
<p>The first step with maximum likelihood estimation is to choose the probability distribution believed to be generating the data.</p>
<p>More precisely, we need to make an assumption as to which <em>parametric class</em> of distributions is generating the data.</p>
<ul class="simple">
<li><p>e.g., the class of all normal distributions, or the class of all gamma distributions.</p></li>
</ul>
<p>Each such class is a family of distributions indexed by a finite number of parameters.</p>
<ul class="simple">
<li><p>e.g., the class of normal distributions is a family of distributions
indexed by its mean <span class="math notranslate nohighlight">\(\mu \in (-\infty, \infty)\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma \in (0, \infty)\)</span>.</p></li>
</ul>
<p>We’ll let the data pick out a particular element of the class by pinning down the parameters.</p>
<p>The parameter estimates so produced will be called <strong>maximum likelihood estimates</strong>.</p>
</section>
<section id="counting-billionaires">
<h3><span class="section-number">81.2.2. </span>Counting billionaires<a class="headerlink" href="#counting-billionaires" title="Link to this heading">#</a></h3>
<p>Treisman <span id="id1">[<a class="reference internal" href="zreferences.html#id103" title="Daniel Treisman. Russia's billionaires. The American Economic Review, 106(5):236–241, 2016.">Treisman, 2016</a>]</span> is interested in estimating the number of billionaires in different countries.</p>
<p>The number of billionaires is integer-valued.</p>
<p>Hence we consider distributions that take values only in the nonnegative integers.</p>
<p>(This is one reason least squares regression is not the best tool for the present problem, since the dependent variable in linear regression is not restricted
to integer values.)</p>
<p>One integer distribution is the <a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a>, the probability mass function (pmf) of which is</p>
<div class="math notranslate nohighlight">
\[
f(y) = \frac{\mu^{y}}{y!} e^{-\mu},
\qquad y = 0, 1, 2, \ldots, \infty
\]</div>
<p>We can plot the Poisson distribution over <span class="math notranslate nohighlight">\(y\)</span> for different values of <span class="math notranslate nohighlight">\(\mu\)</span> as follows</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">poisson_pmf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">μ</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">μ</span><span class="o">**</span><span class="n">y</span> <span class="o">/</span> <span class="n">factorial</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">μ</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_values</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">μ</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
    <span class="n">distribution</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="n">y_values</span><span class="p">:</span>
        <span class="n">distribution</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">poisson_pmf</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">μ</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">y_values</span><span class="p">,</span>
        <span class="n">distribution</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;$\mu$=</span><span class="si">{</span><span class="n">μ</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
        <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$f(y \mid \mu)$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e2091f3dc673afc107e9a1c133f36b77e7be7f261c31fa9f1358276d973bd5b2.png" src="_images/e2091f3dc673afc107e9a1c133f36b77e7be7f261c31fa9f1358276d973bd5b2.png" />
</div>
</div>
<p>Notice that the Poisson distribution begins to resemble a normal distribution as the mean of <span class="math notranslate nohighlight">\(y\)</span> increases.</p>
<p>Let’s have a look at the distribution of the data we’ll be working with in this lecture.</p>
<p>Treisman’s main source of data is <em>Forbes’</em> annual rankings of billionaires and their estimated net worth.</p>
<p>The dataset <code class="docutils literal notranslate"><span class="pre">mle/fp.dta</span></code> can be downloaded from <a class="reference external" href="https://python.quantecon.org/_static/lecture_specific/mle/fp.dta">here</a>
or its <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/aer.p20161068">AER page</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load in data and view</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_stata</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/QuantEcon/lecture-python.myst/raw/refs/heads/main/lectures/_static/lecture_specific/mle/fp.dta&quot;</span>
<span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>ccode</th>
      <th>year</th>
      <th>cyear</th>
      <th>numbil</th>
      <th>numbil0</th>
      <th>numbilall</th>
      <th>netw</th>
      <th>netw0</th>
      <th>netwall</th>
      <th>...</th>
      <th>gattwto08</th>
      <th>mcapbdol</th>
      <th>mcapbdol08</th>
      <th>lnmcap08</th>
      <th>topintaxnew</th>
      <th>topint08</th>
      <th>rintr</th>
      <th>noyrs</th>
      <th>roflaw</th>
      <th>nrrents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>United States</td>
      <td>2.0</td>
      <td>1990.0</td>
      <td>21990.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>0.0</td>
      <td>3060.000000</td>
      <td>11737.599609</td>
      <td>9.370638</td>
      <td>39.799999</td>
      <td>39.799999</td>
      <td>4.988405</td>
      <td>20.0</td>
      <td>1.61</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>United States</td>
      <td>2.0</td>
      <td>1991.0</td>
      <td>21991.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>0.0</td>
      <td>4090.000000</td>
      <td>11737.599609</td>
      <td>9.370638</td>
      <td>39.799999</td>
      <td>39.799999</td>
      <td>4.988405</td>
      <td>20.0</td>
      <td>1.61</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>United States</td>
      <td>2.0</td>
      <td>1992.0</td>
      <td>21992.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>0.0</td>
      <td>4490.000000</td>
      <td>11737.599609</td>
      <td>9.370638</td>
      <td>39.799999</td>
      <td>39.799999</td>
      <td>4.988405</td>
      <td>20.0</td>
      <td>1.61</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>United States</td>
      <td>2.0</td>
      <td>1993.0</td>
      <td>21993.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>0.0</td>
      <td>5136.198730</td>
      <td>11737.599609</td>
      <td>9.370638</td>
      <td>39.799999</td>
      <td>39.799999</td>
      <td>4.988405</td>
      <td>20.0</td>
      <td>1.61</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>United States</td>
      <td>2.0</td>
      <td>1994.0</td>
      <td>21994.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>0.0</td>
      <td>5067.016113</td>
      <td>11737.599609</td>
      <td>9.370638</td>
      <td>39.799999</td>
      <td>39.799999</td>
      <td>4.988405</td>
      <td>20.0</td>
      <td>1.61</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 36 columns</p>
</div></div></div>
</div>
<p>Using a histogram, we can view the distribution of the number of
billionaires per country, <code class="docutils literal notranslate"><span class="pre">numbil0</span></code>, in 2008 (the United States is
dropped for plotting purposes)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">numbil0_2008</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span>
    <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;year&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2008</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;United States&quot;</span><span class="p">)</span>
<span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;numbil0&quot;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">numbil0_2008</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of billionaires in 2008&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b4c087d43ca660025d7cad553c6862389374a0f17cfcd29aff0684ab6e776bdd.png" src="_images/b4c087d43ca660025d7cad553c6862389374a0f17cfcd29aff0684ab6e776bdd.png" />
</div>
</div>
<p>From the histogram, it appears that the Poisson assumption is not unreasonable (albeit with a very low <span class="math notranslate nohighlight">\(\mu\)</span> and some outliers).</p>
</section>
</section>
<section id="conditional-distributions">
<h2><a class="toc-backref" href="#id6" role="doc-backlink"><span class="section-number">81.3. </span>Conditional distributions</a><a class="headerlink" href="#conditional-distributions" title="Link to this heading">#</a></h2>
<p>In Treisman’s paper, the dependent variable — the number of billionaires <span class="math notranslate nohighlight">\(y_i\)</span> in country <span class="math notranslate nohighlight">\(i\)</span> — is modeled as a function of GDP per capita, population size, and years membership in GATT and WTO.</p>
<p>Hence, the distribution of <span class="math notranslate nohighlight">\(y_i\)</span> needs to be conditioned on the vector of explanatory variables <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>.</p>
<p>The standard formulation — the so-called <em>Poisson regression</em> model — is as follows:</p>
<div class="math notranslate nohighlight" id="equation-poissonreg">
<span class="eqno">(81.1)<a class="headerlink" href="#equation-poissonreg" title="Link to this equation">#</a></span>\[f(y_i \mid \mathbf{x}_i) = \frac{\mu_i^{y_i}}{y_i!} e^{-\mu_i}; \qquad y_i = 0, 1, 2, \ldots , \infty .\]</div>
<div class="math notranslate nohighlight">
\[
\text{where}\ \mu_i
     = \exp(\mathbf{x}_i' \boldsymbol{\beta})
     = \exp(\beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik})
\]</div>
<p>To illustrate the idea that the distribution of <span class="math notranslate nohighlight">\(y_i\)</span> depends on
<span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> let’s run a simple simulation.</p>
<p>We use our <code class="docutils literal notranslate"><span class="pre">poisson_pmf</span></code> function from above and arbitrary values for
<span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_values</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># Define a parameter vector with estimates</span>
<span class="n">β</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.26</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.22</span><span class="p">])</span>

<span class="c1"># Create some observations X</span>
<span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
    <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span>
<span class="p">]</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">:</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">β</span><span class="p">)</span>
    <span class="n">distribution</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="n">y_values</span><span class="p">:</span>
        <span class="n">distribution</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">poisson_pmf</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">μ</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">y_values</span><span class="p">,</span>
        <span class="n">distribution</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;$\mu_i$=</span><span class="si">{</span><span class="n">μ</span><span class="si">:</span><span class="s2">.1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
        <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y \mid x_i$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$f(y \mid x_i; \beta )$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/135060543fdc9ab64c6e35326e5eb54c39e8b11e920511085e98d93464f97eb8.png" src="_images/135060543fdc9ab64c6e35326e5eb54c39e8b11e920511085e98d93464f97eb8.png" />
</div>
</div>
<p>We can see that the distribution of <span class="math notranslate nohighlight">\(y_i\)</span> is conditional on
<span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> (<span class="math notranslate nohighlight">\(\mu_i\)</span> is no longer constant).</p>
</section>
<section id="id2">
<h2><a class="toc-backref" href="#id7" role="doc-backlink"><span class="section-number">81.4. </span>Maximum likelihood estimation</a><a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>In our model for number of billionaires, the conditional distribution
contains 4 (<span class="math notranslate nohighlight">\(k = 4\)</span>) parameters that we need to estimate.</p>
<p>We will label our entire parameter vector as <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\beta} = \begin{bmatrix}
                            \beta_0 \\
                            \beta_1 \\
                            \beta_2 \\
                            \beta_3
                      \end{bmatrix}
\end{split}\]</div>
<p>To estimate the model using MLE, we want to maximize the likelihood that
our estimate <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is the true parameter <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>Intuitively, we want to find the <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that best fits our data.</p>
<p>First, we need to construct the likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\boldsymbol{\beta})\)</span>, which is similar to a joint probability density function.</p>
<p>Assume we have some data <span class="math notranslate nohighlight">\(y_i = \{y_1, y_2\}\)</span> and
<span class="math notranslate nohighlight">\(y_i \sim f(y_i)\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(y_1\)</span> and <span class="math notranslate nohighlight">\(y_2\)</span> are independent, the joint pmf of these
data is <span class="math notranslate nohighlight">\(f(y_1, y_2) = f(y_1) \cdot f(y_2)\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(y_i\)</span> follows a Poisson distribution with <span class="math notranslate nohighlight">\(\lambda = 7\)</span>,
we can visualize the joint pmf like so</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_joint_poisson</span><span class="p">(</span><span class="n">μ</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">y_n</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">yi_values</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y_n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create coordinate points of X and Y</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">yi_values</span><span class="p">,</span> <span class="n">yi_values</span><span class="p">)</span>

    <span class="c1"># Multiply distributions together</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">poisson_pmf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">μ</span><span class="p">)</span> <span class="o">*</span> <span class="n">poisson_pmf</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">μ</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;terrain&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$y_1$&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$y_2$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$f(y_1, y_2)$&quot;</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="n">plot_joint_poisson</span><span class="p">(</span><span class="n">μ</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">y_n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/de2ac51fc7bc423557cc53885e834ddf39d105489b29ee5396f20814d6cdcffd.png" src="_images/de2ac51fc7bc423557cc53885e834ddf39d105489b29ee5396f20814d6cdcffd.png" />
</div>
</div>
<p>Similarly, the joint pmf of our data (which is distributed as a
conditional Poisson distribution) can be written as</p>
<div class="math notranslate nohighlight">
\[
f(y_1, y_2, \ldots, y_n \mid \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n; \boldsymbol{\beta})
    = \prod_{i=1}^{n} \frac{\mu_i^{y_i}}{y_i!} e^{-\mu_i}
\]</div>
<p><span class="math notranslate nohighlight">\(y_i\)</span> is conditional on both the values of <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> and the
parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>The likelihood function is the same as the joint pmf, but treats the
parameter <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> as a random variable and takes the observations
<span class="math notranslate nohighlight">\((y_i, \mathbf{x}_i)\)</span> as given</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\mathcal{L}(\beta \mid y_1, y_2, \ldots, y_n \ ; \ \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n) = &amp;
\prod_{i=1}^{n} \frac{\mu_i^{y_i}}{y_i!} e^{-\mu_i} \\ = &amp;
f(y_1, y_2, \ldots, y_n \mid  \ \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n ; \beta)
\end{split}
\end{split}\]</div>
<p>Now that we have our likelihood function, we want to find the <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that yields the maximum likelihood value</p>
<div class="math notranslate nohighlight">
\[
\underset{\boldsymbol{\beta}}{\max} \mathcal{L}(\boldsymbol{\beta})
\]</div>
<p>In doing so it is generally easier to maximize the log-likelihood (consider
differentiating <span class="math notranslate nohighlight">\(f(x) = x \exp(x)\)</span> vs. <span class="math notranslate nohighlight">\(f(x) = \log(x) + x\)</span>).</p>
<p>Given that taking a logarithm is a monotone increasing transformation, a maximizer of the likelihood function will also be a maximizer of the log-likelihood function.</p>
<p>In our case the log-likelihood is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\log{ \mathcal{L}} (\boldsymbol{\beta}) = \ &amp;
    \log \Big(
        f(y_1 ; \boldsymbol{\beta})
        \cdot
        f(y_2 ; \boldsymbol{\beta})
        \cdot \ldots \cdot
        f(y_n ; \boldsymbol{\beta})
        \Big) \\
        = &amp;
        \sum_{i=1}^{n} \log{f(y_i ; \boldsymbol{\beta})} \\
        = &amp;
        \sum_{i=1}^{n}
        \log \Big( {\frac{\mu_i^{y_i}}{y_i!} e^{-\mu_i}} \Big) \\
        = &amp;
        \sum_{i=1}^{n} y_i \log{\mu_i} -
        \sum_{i=1}^{n} \mu_i -
        \sum_{i=1}^{n} \log y_i!
\end{split}
\end{split}\]</div>
<p>The MLE of the Poisson for <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> can be obtained by solving</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta}{\max} \Big(
\sum_{i=1}^{n} y_i \log{\mu_i} -
\sum_{i=1}^{n} \mu_i -
\sum_{i=1}^{n} \log y_i! \Big)
\]</div>
<p>However, no analytical solution exists to the above problem – to find the MLE
we need to use numerical methods.</p>
</section>
<section id="mle-with-numerical-methods">
<h2><a class="toc-backref" href="#id8" role="doc-backlink"><span class="section-number">81.5. </span>MLE with numerical methods</a><a class="headerlink" href="#mle-with-numerical-methods" title="Link to this heading">#</a></h2>
<p>Many distributions do not have nice, analytical solutions and therefore require
numerical methods to solve for parameter estimates.</p>
<p>One such numerical method is the Newton-Raphson algorithm.</p>
<p>Our goal is to find the maximum likelihood estimate <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>At <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, the first derivative of the log-likelihood
function will be equal to 0.</p>
<p>Let’s illustrate this by supposing</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L(\beta)} = - (\beta - 10) ^2 - 10
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">logL</span><span class="p">(</span><span class="n">β</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">((</span><span class="n">β</span> <span class="o">-</span> <span class="mi">10</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<p>To find the value of the gradient of the above function, we can use <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html">jax.grad</a> which auto-differentiates the given function.</p>
<p>We further use <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html">jax.vmap</a> which vectorizes the given function i.e. the function acting upon scalar inputs can now be used with vector inputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogL</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">logL</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">β</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">logL</span><span class="p">(</span><span class="n">β</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">dlogL</span><span class="p">(</span><span class="n">β</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span>
    <span class="sa">r</span><span class="s2">&quot;$log \mathcal{L(\beta)}$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span>
<span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span>
    <span class="sa">r</span><span class="s2">&quot;$\frac{dlog \mathcal{L(\beta)}}{d \beta}$ &quot;</span><span class="p">,</span>
    <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">labelpad</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(),</span> <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c0a7ed4e0d2b726b9fcb9f51559d87e9188ea238e4b941cb6ebcd4257bdf5d7d.png" src="_images/c0a7ed4e0d2b726b9fcb9f51559d87e9188ea238e4b941cb6ebcd4257bdf5d7d.png" />
</div>
</div>
<p>The plot shows that the maximum likelihood value (the top plot) occurs
when <span class="math notranslate nohighlight">\(\frac{d \log \mathcal{L(\boldsymbol{\beta})}}{d \boldsymbol{\beta}} = 0\)</span> (the bottom
plot).</p>
<p>Therefore, the likelihood is maximized when <span class="math notranslate nohighlight">\(\beta = 10\)</span>.</p>
<p>We can also ensure that this value is a <em>maximum</em> (as opposed to a
minimum) by checking that the second derivative (slope of the bottom
plot) is negative.</p>
<p>The Newton-Raphson algorithm finds a point where the first derivative is
0.</p>
<p>To use the algorithm, we take an initial guess at the maximum value,
<span class="math notranslate nohighlight">\(\beta_0\)</span> (the OLS parameter estimates might be a reasonable
guess), then</p>
<ol class="arabic">
<li><p>Use the updating rule to iterate the algorithm</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{\beta}_{(k+1)} = \boldsymbol{\beta}_{(k)} - H^{-1}(\boldsymbol{\beta}_{(k)})G(\boldsymbol{\beta}_{(k)})
   \]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \begin{aligned}
   G(\boldsymbol{\beta}_{(k)}) = \frac{d \log \mathcal{L(\boldsymbol{\beta}_{(k)})}}{d \boldsymbol{\beta}_{(k)}} \\
   H(\boldsymbol{\beta}_{(k)}) = \frac{d^2 \log \mathcal{L(\boldsymbol{\beta}_{(k)})}}{d \boldsymbol{\beta}_{(k)}d \boldsymbol{\beta}'_{(k)}}
   \end{aligned}
   \end{split}\]</div>
</li>
<li><p>Check whether <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{(k+1)} - \boldsymbol{\beta}_{(k)} &lt; tol\)</span></p>
<ul class="simple">
<li><p>If true, then stop iterating and set
<span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = \boldsymbol{\beta}_{(k+1)}\)</span></p></li>
<li><p>If false, then update <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{(k+1)}\)</span></p></li>
</ul>
</li>
</ol>
<p>As can be seen from the updating equation,
<span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{(k+1)} = \boldsymbol{\beta}_{(k)}\)</span> only when
<span class="math notranslate nohighlight">\(G(\boldsymbol{\beta}_{(k)}) = 0\)</span> i.e. where the first derivative is equal to 0.</p>
<p>(In practice, we stop iterating when the difference is below a small
tolerance threshold.)</p>
<p>Let’s have a go at implementing the Newton-Raphson algorithm.</p>
<p>First, we’ll create a class called <code class="docutils literal notranslate"><span class="pre">PoissonRegression</span></code> so we can
easily recompute the values of the log likelihood, gradient and Hessian
for every iteration</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PoissonRegression</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can define the log likelihood function in Python</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">logL</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">y</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span> <span class="o">@</span> <span class="n">β</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">μ</span><span class="p">)</span> <span class="o">-</span> <span class="n">μ</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">factorial</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>To find the gradient of the <code class="docutils literal notranslate"><span class="pre">poisson_logL</span></code>, we again use <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html">jax.grad</a>.</p>
<p>According to <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev">the documentation</a>,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">jax.jacfwd</span></code> uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">jax.jacrev</span></code> uses reverse-mode, which is more efficient for “wide” Jacobian matrices.</p></li>
</ul>
<p>(The documentation also states that when matrices that are near-square, <code class="docutils literal notranslate"><span class="pre">jax.jacfwd</span></code> probably has an edge over <code class="docutils literal notranslate"><span class="pre">jax.jacrev</span></code>.)</p>
<p>Therefore, to find the Hessian, we can directly use <code class="docutils literal notranslate"><span class="pre">jax.jacfwd</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_logL</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">logL</span><span class="p">)</span>
<span class="n">H_logL</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">G_logL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our function <code class="docutils literal notranslate"><span class="pre">newton_raphson</span></code> will take a <code class="docutils literal notranslate"><span class="pre">PoissonRegression</span></code> object
that has an initial guess of the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_0\)</span>.</p>
<p>The algorithm will update the parameter vector according to the updating
rule, and recalculate the gradient and Hessian matrices at the new
parameter estimates.</p>
<p>Iteration will end when either:</p>
<ul class="simple">
<li><p>The difference between the parameter and the updated parameter is below a tolerance level.</p></li>
<li><p>The maximum number of iterations has been achieved (meaning convergence is not achieved).</p></li>
</ul>
<p>So we can get an idea of what’s going on while the algorithm is running,
an option <code class="docutils literal notranslate"><span class="pre">display=True</span></code> is added to print out values at each
iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">newton_raphson</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">β</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">error</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Initial error value</span>

    <span class="c1"># Print header of output</span>
    <span class="k">if</span> <span class="n">display</span><span class="p">:</span>
        <span class="n">header</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;Iteration_k&quot;</span><span class="si">:</span><span class="s1">&lt;13</span><span class="si">}{</span><span class="s2">&quot;Log-likelihood&quot;</span><span class="si">:</span><span class="s1">&lt;16</span><span class="si">}{</span><span class="s2">&quot;θ&quot;</span><span class="si">:</span><span class="s1">&lt;60</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">header</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">header</span><span class="p">))</span>

    <span class="c1"># While loop runs while any value in error is greater</span>
    <span class="c1"># than the tolerance until max iterations are reached</span>
    <span class="k">while</span> <span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">error</span> <span class="o">&gt;</span> <span class="n">tol</span><span class="p">)</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">G</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">H_logL</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">model</span><span class="p">)),</span> <span class="n">G_logL</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">β_new</span> <span class="o">=</span> <span class="n">β</span> <span class="o">-</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">H</span><span class="p">),</span> <span class="n">G</span><span class="p">))</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">β_new</span> <span class="o">-</span> <span class="n">β</span><span class="p">)</span>
        <span class="n">β</span> <span class="o">=</span> <span class="n">β_new</span>

        <span class="k">if</span> <span class="n">display</span><span class="p">:</span>
            <span class="n">β_list</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">t</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">β</span><span class="o">.</span><span class="n">flatten</span><span class="p">())]</span>
            <span class="n">update</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">&lt;13</span><span class="si">}{</span><span class="n">logL</span><span class="p">(</span><span class="n">β</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">)</span><span class="si">:</span><span class="s2">&lt;16.8</span><span class="si">}{</span><span class="n">β_list</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>

        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of iterations: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;β_hat = </span><span class="si">{</span><span class="n">β</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">β</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s try out our algorithm with a small dataset of 5 observations and 3
variables in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Take a guess at initial βs</span>
<span class="n">init_β</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>

<span class="c1"># Create an object with Poisson model values</span>
<span class="n">poi</span> <span class="o">=</span> <span class="n">PoissonRegression</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Use newton_raphson to find the MLE</span>
<span class="n">β_hat</span> <span class="o">=</span> <span class="n">newton_raphson</span><span class="p">(</span><span class="n">poi</span><span class="p">,</span> <span class="n">init_β</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration_k  Log-likelihood  θ                                                           
-----------------------------------------------------------------------------------------
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0            -4.3447633      [&#39;-1.49&#39;, &#39;0.265&#39;, &#39;0.244&#39;]
1            -3.5742409      [&#39;-3.38&#39;, &#39;0.528&#39;, &#39;0.474&#39;]
2            -3.3999527      [&#39;-5.06&#39;, &#39;0.782&#39;, &#39;0.702&#39;]
3            -3.3788645      [&#39;-5.92&#39;, &#39;0.909&#39;, &#39;0.82&#39;]
4            -3.3783555      [&#39;-6.07&#39;, &#39;0.933&#39;, &#39;0.843&#39;]
5            -3.3783557      [&#39;-6.08&#39;, &#39;0.933&#39;, &#39;0.843&#39;]
6            -3.3783557      [&#39;-6.08&#39;, &#39;0.933&#39;, &#39;0.843&#39;]
Number of iterations: 7
β_hat = [-6.078486   0.9334028  0.8432968]
</pre></div>
</div>
</div>
</div>
<p>As this was a simple model with few observations, the algorithm achieved
convergence in only 7 iterations.</p>
<p>You can see that with each iteration, the log-likelihood value increased.</p>
<p>Remember, our objective was to maximize the log-likelihood function,
which the algorithm has worked to achieve.</p>
<p>Also, note that the increase in <span class="math notranslate nohighlight">\(\log \mathcal{L}(\boldsymbol{\beta}_{(k)})\)</span>
becomes smaller with each iteration.</p>
<p>This is because the gradient is approaching 0 as we reach the maximum,
and therefore the numerator in our updating equation is becoming smaller.</p>
<p>The gradient vector should be close to 0 at <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_logL</span><span class="p">(</span><span class="n">β_hat</span><span class="p">,</span> <span class="n">poi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([ 7.4505806e-09, -2.9802322e-07,  3.7252903e-08], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>The iterative process can be visualized in the following diagram, where
the maximum is found at <span class="math notranslate nohighlight">\(\beta = 10\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">logL</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">10</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">10</span>


<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">find_tangent</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">logL</span><span class="p">(</span><span class="n">β</span><span class="p">)</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">logL</span><span class="p">(</span><span class="n">β</span> <span class="o">+</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">β</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">β</span> <span class="o">+</span> <span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">]),</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">c</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">β</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">logL</span><span class="p">(</span><span class="n">β</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">β</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
    <span class="n">β_line</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">β</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">β</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">find_tangent</span><span class="p">(</span><span class="n">β</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">β_line</span> <span class="o">+</span> <span class="n">c</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">β_line</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;purple&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">β</span> <span class="o">+</span> <span class="mf">2.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="sa">rf</span><span class="s2">&quot;$G(</span><span class="si">{</span><span class="n">β</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="o">-</span><span class="mi">24</span><span class="p">,</span> <span class="n">logL</span><span class="p">(</span><span class="n">β</span><span class="p">),</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">logL</span><span class="p">(</span><span class="n">β</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="n">β</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">24</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">),</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">13</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span>
    <span class="sa">r</span><span class="s2">&quot;$log \mathcal{L(\beta)}$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4d524484ebbcc53ab7cdeaf17c2785dc6971167d463f02fe6c510592ebf10ce6.png" src="_images/4d524484ebbcc53ab7cdeaf17c2785dc6971167d463f02fe6c510592ebf10ce6.png" />
</div>
</div>
<p>Note that our implementation of the Newton-Raphson algorithm is rather
basic — for more robust implementations see,
for example, <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html">scipy.optimize</a>.</p>
</section>
<section id="maximum-likelihood-estimation-with-statsmodels">
<h2><a class="toc-backref" href="#id9" role="doc-backlink"><span class="section-number">81.6. </span>Maximum likelihood estimation with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code></a><a class="headerlink" href="#maximum-likelihood-estimation-with-statsmodels" title="Link to this heading">#</a></h2>
<p>Now that we know what’s going on under the hood, we can apply MLE to an interesting application.</p>
<p>We’ll use the Poisson regression model in <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> to obtain
a richer output with standard errors, test values, and more.</p>
<p><code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> uses the same algorithm as above to find the maximum
likelihood estimates.</p>
<p>Before we begin, let’s re-estimate our simple model with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>
to confirm we obtain the same coefficients and log-likelihood value.</p>
<p>Now, as <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> accepts only NumPy arrays, we can use <code class="docutils literal notranslate"><span class="pre">np.array</span></code> method
to convert them to NumPy arrays.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="n">y_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">X_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">stats_poisson</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">y_numpy</span><span class="p">,</span> <span class="n">X_numpy</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stats_poisson</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.675671
         Iterations 7
                          Poisson Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                    5
Model:                        Poisson   Df Residuals:                        2
Method:                           MLE   Df Model:                            2
Date:                Fri, 12 Sep 2025   Pseudo R-squ.:                  0.2546
Time:                        00:53:11   Log-Likelihood:                -3.3784
converged:                       True   LL-Null:                       -4.5325
Covariance Type:            nonrobust   LLR p-value:                    0.3153
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -6.0785      5.279     -1.151      0.250     -16.425       4.268
x1             0.9334      0.829      1.126      0.260      -0.691       2.558
x2             0.8433      0.798      1.057      0.291      -0.720       2.407
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>Now let’s replicate results from Daniel Treisman’s paper, <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/aer.p20161068">Russia’s
Billionaires</a>,
mentioned earlier in the lecture.</p>
<p>Treisman starts by estimating equation <a class="reference internal" href="#equation-poissonreg">(81.1)</a>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is <span class="math notranslate nohighlight">\({number\ of\ billionaires}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{i1}\)</span> is <span class="math notranslate nohighlight">\(\log{GDP\ per\ capita}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{i2}\)</span> is <span class="math notranslate nohighlight">\(\log{population}_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{i3}\)</span> is <span class="math notranslate nohighlight">\({years\ in\ GATT}_i\)</span> – years membership in GATT and WTO (to proxy access to international markets)</p></li>
</ul>
<p>The paper only considers the year 2008 for estimation.</p>
<p>We will set up our variables for estimation like so (you should have the
data assigned to <code class="docutils literal notranslate"><span class="pre">df</span></code> from earlier in the lecture)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Keep only year 2008</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;year&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2008</span><span class="p">]</span>

<span class="c1"># Add a constant</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;const&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Variable sets</span>
<span class="n">reg1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;const&quot;</span><span class="p">,</span> <span class="s2">&quot;lngdppc&quot;</span><span class="p">,</span> <span class="s2">&quot;lnpop&quot;</span><span class="p">,</span> <span class="s2">&quot;gattwto08&quot;</span><span class="p">]</span>
<span class="n">reg2</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;const&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lngdppc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnpop&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gattwto08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnmcap08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;rintr&quot;</span><span class="p">,</span>
    <span class="s2">&quot;topint08&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">reg3</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;const&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lngdppc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnpop&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gattwto08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnmcap08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;rintr&quot;</span><span class="p">,</span>
    <span class="s2">&quot;topint08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;nrrents&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roflaw&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Then we can use the <code class="docutils literal notranslate"><span class="pre">Poisson</span></code> function from <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> to fit the
model.</p>
<p>We’ll use robust standard errors as in the author’s paper</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify model</span>
<span class="n">poisson_reg</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s2">&quot;numbil0&quot;</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="n">reg1</span><span class="p">],</span> <span class="n">missing</span><span class="o">=</span><span class="s2">&quot;drop&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">cov_type</span><span class="o">=</span><span class="s2">&quot;HC0&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">poisson_reg</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 2.226090
         Iterations 9
                          Poisson Regression Results                          
==============================================================================
Dep. Variable:                numbil0   No. Observations:                  197
Model:                        Poisson   Df Residuals:                      193
Method:                           MLE   Df Model:                            3
Date:                Fri, 12 Sep 2025   Pseudo R-squ.:                  0.8574
Time:                        00:53:11   Log-Likelihood:                -438.54
converged:                       True   LL-Null:                       -3074.7
Covariance Type:                  HC0   LLR p-value:                     0.000
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const        -29.0495      2.578    -11.268      0.000     -34.103     -23.997
lngdppc        1.0839      0.138      7.834      0.000       0.813       1.355
lnpop          1.1714      0.097     12.024      0.000       0.980       1.362
gattwto08      0.0060      0.007      0.868      0.386      -0.008       0.019
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>Success! The algorithm was able to achieve convergence in 9 iterations.</p>
<p>Our output indicates that GDP per capita, population, and years of
membership in the General Agreement on Tariffs and Trade (GATT) are
positively related to the number of billionaires a country has, as
expected.</p>
<p>Let’s also estimate the author’s more full-featured models and display
them in a single table</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">regs</span> <span class="o">=</span> <span class="p">[</span><span class="n">reg1</span><span class="p">,</span> <span class="n">reg2</span><span class="p">,</span> <span class="n">reg3</span><span class="p">]</span>
<span class="n">reg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Model 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 3&quot;</span><span class="p">]</span>
<span class="n">info_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Pseudo R-squared&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">prsquared</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="s2">&quot;No. observations&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">nobs</span><span class="p">)</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">regressor_order</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;const&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lngdppc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnpop&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gattwto08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnmcap08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;rintr&quot;</span><span class="p">,</span>
    <span class="s2">&quot;topint08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;nrrents&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roflaw&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">reg</span> <span class="ow">in</span> <span class="n">regs</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s2">&quot;numbil0&quot;</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="n">reg</span><span class="p">],</span> <span class="n">missing</span><span class="o">=</span><span class="s2">&quot;drop&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">cov_type</span><span class="o">=</span><span class="s2">&quot;HC0&quot;</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">disp</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="n">results_table</span> <span class="o">=</span> <span class="n">summary_col</span><span class="p">(</span>
    <span class="n">results</span><span class="o">=</span><span class="n">results</span><span class="p">,</span>
    <span class="n">float_format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%0.3f</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">stars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">model_names</span><span class="o">=</span><span class="n">reg_names</span><span class="p">,</span>
    <span class="n">info_dict</span><span class="o">=</span><span class="n">info_dict</span><span class="p">,</span>
    <span class="n">regressor_order</span><span class="o">=</span><span class="n">regressor_order</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">results_table</span><span class="o">.</span><span class="n">add_title</span><span class="p">(</span>
    <span class="s2">&quot;Table 1 - Explaining the Number of Billionaires </span><span class="se">\</span>
<span class="s2">                        in 2008&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Table 1 - Explaining the Number of Billionaires                         in 2008
=================================================
                  Model 1    Model 2    Model 3  
-------------------------------------------------
const            -29.050*** -19.444*** -20.858***
                 (2.578)    (4.820)    (4.255)   
lngdppc          1.084***   0.717***   0.737***  
                 (0.138)    (0.244)    (0.233)   
lnpop            1.171***   0.806***   0.929***  
                 (0.097)    (0.213)    (0.195)   
gattwto08        0.006      0.007      0.004     
                 (0.007)    (0.006)    (0.006)   
lnmcap08                    0.399**    0.286*    
                            (0.172)    (0.167)   
rintr                       -0.010     -0.009    
                            (0.010)    (0.010)   
topint08                    -0.051***  -0.058*** 
                            (0.011)    (0.012)   
nrrents                                -0.005    
                                       (0.010)   
roflaw                                 0.203     
                                       (0.372)   
No. observations 197        131        131       
Pseudo R-squared 0.86       0.90       0.90      
=================================================
Standard errors in parentheses.
* p&lt;.1, ** p&lt;.05, ***p&lt;.01
</pre></div>
</div>
</div>
</div>
<p>The output suggests that the frequency of billionaires is positively
correlated with GDP per capita, population size, stock market
capitalization, and negatively correlated with top marginal income tax
rate.</p>
<p>To analyze our results by country, we can plot the difference between
the predicted and actual values, then sort from highest to lowest and
plot the first 15</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;const&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lngdppc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnpop&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gattwto08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lnmcap08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;rintr&quot;</span><span class="p">,</span>
    <span class="s2">&quot;topint08&quot;</span><span class="p">,</span>
    <span class="s2">&quot;nrrents&quot;</span><span class="p">,</span>
    <span class="s2">&quot;roflaw&quot;</span><span class="p">,</span>
    <span class="s2">&quot;numbil0&quot;</span><span class="p">,</span>
    <span class="s2">&quot;country&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">data</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Use last model (model 3)</span>
<span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;prediction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="c1"># Calculate difference</span>
<span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;difference&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;numbil0&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;prediction&quot;</span><span class="p">]</span>

<span class="c1"># Sort in descending order</span>
<span class="n">results_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;difference&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot the first 15 data points</span>
<span class="n">results_df</span><span class="p">[:</span><span class="mi">15</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="s2">&quot;country&quot;</span><span class="p">,</span> <span class="s2">&quot;difference&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of billionaires above predicted level&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Country&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4964baf27b3b59541e2e9de78d6c12af663cebded329393f4ab90ec2539ed5a2.png" src="_images/4964baf27b3b59541e2e9de78d6c12af663cebded329393f4ab90ec2539ed5a2.png" />
</div>
</div>
<p>As we can see, Russia has by far the highest number of billionaires in
excess of what is predicted by the model (around 50 more than expected).</p>
<p>Treisman uses this empirical result to discuss possible reasons for
Russia’s excess of billionaires, including the origination of wealth in
Russia, the political climate, and the history of privatization in the
years after the USSR.</p>
</section>
<section id="summary">
<h2><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">81.7. </span>Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>In this lecture, we used Maximum Likelihood Estimation to estimate the
parameters of a Poisson model.</p>
<p><code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> contains other built-in likelihood models such as
<a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Probit.html">Probit</a>
and
<a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html">Logit</a>.</p>
<p>For further flexibility, <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> provides a way to specify the
distribution manually using the <code class="docutils literal notranslate"><span class="pre">GenericLikelihoodModel</span></code> class - an
example notebook can be found
<a class="reference external" href="https://www.statsmodels.org/dev/examples/notebooks/generated/generic_mle.html">here</a>.</p>
</section>
<section id="exercises">
<h2><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">81.8. </span>Exercises</a><a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="exercise admonition" id="mle_ex1">

<p class="admonition-title"><span class="caption-number">Exercise 81.1 </span></p>
<section id="exercise-content">
<p>Suppose we wanted to estimate the probability of an event <span class="math notranslate nohighlight">\(y_i\)</span>
occurring, given some observations.</p>
<p>We could use a probit regression model, where the pmf of <span class="math notranslate nohighlight">\(y_i\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f(y_i; \boldsymbol{\beta}) = \mu_i^{y_i} (1-\mu_i)^{1-y_i}, \quad y_i = 0,1 \\
\text{where} \quad \mu_i = \Phi(\mathbf{x}_i' \boldsymbol{\beta})
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\Phi\)</span> represents the <em>cumulative normal distribution</em> and
constrains the predicted <span class="math notranslate nohighlight">\(y_i\)</span> to be between 0 and 1 (as required
for a probability).</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is a vector of coefficients.</p>
<p>Following the example in the lecture, write a class to represent the
Probit model.</p>
<p>To begin, find the log-likelihood function and derive the gradient and
Hessian.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">jax.scipy.stats</span></code> module <code class="docutils literal notranslate"><span class="pre">norm</span></code> contains the functions needed to
compute the cdf and pdf of the normal distribution.</p>
</section>
</div>
<div class="solution dropdown admonition" id="mle-solution-1">

<p class="admonition-title">Solution to<a class="reference internal" href="#mle_ex1"> Exercise 81.1</a></p>
<section id="solution-content">
<p>The log-likelihood can be written as</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L} = \sum_{i=1}^n
\big[
y_i \log \Phi(\mathbf{x}_i' \boldsymbol{\beta}) +
(1 - y_i) \log (1 - \Phi(\mathbf{x}_i' \boldsymbol{\beta})) \big]
\]</div>
<p>Using the <strong>fundamental theorem of calculus</strong>, the derivative of a
cumulative probability distribution is its marginal distribution</p>
<div class="math notranslate nohighlight">
\[
\frac{ \partial} {\partial s} \Phi(s) = \phi(s)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> is the marginal normal distribution.</p>
<p>The gradient vector of the Probit model is</p>
<div class="math notranslate nohighlight">
\[
\frac {\partial \log \mathcal{L}} {\partial \boldsymbol{\beta}} =
\sum_{i=1}^n \Big[
y_i \frac{\phi(\mathbf{x}'_i \boldsymbol{\beta})}{\Phi(\mathbf{x}'_i \boldsymbol{\beta)}} -
(1 - y_i) \frac{\phi(\mathbf{x}'_i \boldsymbol{\beta)}}{1 - \Phi(\mathbf{x}'_i \boldsymbol{\beta)}}
\Big] \mathbf{x}_i
\]</div>
<p>The Hessian of the Probit model is</p>
<div class="math notranslate nohighlight">
\[
\frac {\partial^2 \log \mathcal{L}} {\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}'} =
-\sum_{i=1}^n \phi (\mathbf{x}_i' \boldsymbol{\beta})
\Big[
y_i \frac{ \phi (\mathbf{x}_i' \boldsymbol{\beta}) + \mathbf{x}_i' \boldsymbol{\beta} \Phi (\mathbf{x}_i' \boldsymbol{\beta}) } { [\Phi (\mathbf{x}_i' \boldsymbol{\beta})]^2 } +
(1 - y_i) \frac{ \phi (\mathbf{x}_i' \boldsymbol{\beta}) - \mathbf{x}_i' \boldsymbol{\beta} (1 - \Phi (\mathbf{x}_i' \boldsymbol{\beta})) } { [1 - \Phi (\mathbf{x}_i' \boldsymbol{\beta})]^2 }
\Big]
\mathbf{x}_i \mathbf{x}_i'
\]</div>
<p>Using these results, we can write a class for the Probit model as
follows</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ProbitRegression</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">X</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">logL</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">y</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span> <span class="o">@</span> <span class="n">β</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span> <span class="o">@</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">μ</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">@</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">μ</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_logL</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">logL</span><span class="p">)</span>
<span class="n">H_logL</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">G_logL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</div>
<div class="exercise admonition" id="mle_ex2">

<p class="admonition-title"><span class="caption-number">Exercise 81.2 </span></p>
<section id="exercise-content">
<p>Use the following dataset and initial values of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> to
estimate the MLE with the Newton-Raphson algorithm developed earlier in
the lecture</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} =
\begin{bmatrix}
1 &amp; 2 &amp; 4 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 4 &amp; 3 \\
1 &amp; 5 &amp; 6 \\
1 &amp; 3 &amp; 5
\end{bmatrix}
\quad
y =
\begin{bmatrix}
1 \\
0 \\
1 \\
1 \\
0
\end{bmatrix}
\quad
\boldsymbol{\beta}_{(0)} =
\begin{bmatrix}
0.1 \\
0.1 \\
0.1
\end{bmatrix}
\end{split}\]</div>
<p>Verify your results with <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> - you can import the Probit
function with the following import statement</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.discrete.discrete_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Probit</span>
</pre></div>
</div>
</div>
</div>
<p>Note that the simple Newton-Raphson algorithm developed in this lecture
is very sensitive to initial values, and therefore you may fail to
achieve convergence with different starting values.</p>
</section>
</div>
<div class="solution dropdown admonition" id="mle-solution-3">

<p class="admonition-title">Solution to<a class="reference internal" href="#mle_ex2"> Exercise 81.2</a></p>
<section id="solution-content">
<p>Here is one solution</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Take a guess at initial βs</span>
<span class="n">β</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>

<span class="c1"># Create a model of Probit regression</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">ProbitRegression</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Run Newton-Raphson algorithm</span>
<span class="n">newton_raphson</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration_k  Log-likelihood  θ                                                           
-----------------------------------------------------------------------------------------
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0            -2.3796887      [&#39;-1.34&#39;, &#39;0.775&#39;, &#39;-0.157&#39;]
1            -2.3687525      [&#39;-1.53&#39;, &#39;0.775&#39;, &#39;-0.0981&#39;]
2            -2.3687296      [&#39;-1.55&#39;, &#39;0.778&#39;, &#39;-0.0971&#39;]
3            -2.3687291      [&#39;-1.55&#39;, &#39;0.778&#39;, &#39;-0.0971&#39;]
Number of iterations: 4
β_hat = [-1.5462587   0.77778953 -0.09709755]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([-1.5462587 ,  0.77778953, -0.09709755], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use statsmodels to verify results</span>
<span class="n">y_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">X_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Probit</span><span class="p">(</span><span class="n">y_numpy</span><span class="p">,</span> <span class="n">X_numpy</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.473746
         Iterations 6
                          Probit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:                    5
Model:                         Probit   Df Residuals:                        2
Method:                           MLE   Df Model:                            2
Date:                Fri, 12 Sep 2025   Pseudo R-squ.:                  0.2961
Time:                        00:53:12   Log-Likelihood:                -2.3687
converged:                       True   LL-Null:                       -3.3651
Covariance Type:            nonrobust   LLR p-value:                    0.3692
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -1.5463      1.866     -0.829      0.407      -5.204       2.111
x1             0.7778      0.788      0.986      0.324      -0.768       2.323
x2            -0.0971      0.590     -0.165      0.869      -1.254       1.060
==============================================================================
</pre></div>
</div>
</div>
</div>
</section>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                    <p>A theme by <a href="https://quantecon.org">QuantEcon</a></p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="sir_model.html">
   1. Modeling COVID 19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   2. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="qr_decomp.html">
   3. QR Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eig_circulant.html">
   4. Circulant Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svd_intro.html">
   5. Singular Value Decomposition (SVD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="var_dmd.html">
   6. VARs and DMDs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newton_method.html">
   7. Using Newton’s Method to Solve Economic Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementary Statistics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="prob_matrix.html">
   8. Elementary Probability with Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stats_examples.html">
   9. Some Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   10. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prob_meaning.html">
   11. Two Meanings of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multi_hyper.html">
   12. Multivariate Hypergeometric Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_normal.html">
   13. Multivariate Normal Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hoist_failure.html">
   14. Fault Tree Uncertainties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="back_prop.html">
   15. Introduction to Artificial Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rand_resp.html">
   16. Randomized Response Surveys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="util_rand_resp.html">
   17. Expected Utilities of Random Responses
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayes Law
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="bayes_nonconj.html">
   18. Non-Conjugate Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_bayes.html">
   19. Posterior Distributions for  AR(1) Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_turningpts.html">
   20. Forecasting  an AR(1) Process
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics and Information
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="divergence_measures.html">
   21. Statistical Divergence Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process.html">
   22. Likelihood Ratio Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process_2.html">
   23. Heterogeneous Beliefs and Financial Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_var.html">
   24. Likelihood Processes For VAR Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="imp_sample.html">
   25. Mean of a Likelihood Ratio Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman.html">
   26. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman_2.html">
   27. A Bayesian Formulation of Friedman and Wald’s Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exchangeable.html">
   28. Exchangeability and Bayesian Updating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_bayes.html">
   29. Likelihood Ratio Processes and Bayesian Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mix_model.html">
   30. Incorrect Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="navy_captain.html">
   31. Bayesian versus Frequentist Decision Rules
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Programming
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="opt_transport.html">
   32. Optimal Transport
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="von_neumann_model.html">
   33. Von Neumann Growth Model (and a Generalization)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="finite_markov.html">
   34. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inventory_dynamics.html">
   35. Inventory Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_models.html">
   36. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="samuelson.html">
   37. Samuelson Multiplier-Accelerator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kesten_processes.html">
   38. Kesten Processes and Firm Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wealth_dynamics.html">
   39. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman.html">
   40. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman_2.html">
   41. Another Look at the Kalman Filter
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Search
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model.html">
   42. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_separation.html">
   43. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_fitted_vfi.html">
   44. Job Search III: Fitted Value Function Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_correlated.html">
   45. Job Search IV: Correlated Wage Offers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="career.html">
   46. Job Search V: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jv.html">
   47. Job Search VI: On-the-Job Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_q.html">
   48. Job Search VII: A McCall Worker Q-Learns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="odu.html">
   49. Job Search VII: Search with Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Consumption, Savings and Capital
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_1.html">
   50. Cass-Koopmans Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_2.html">
   51. Cass-Koopmans Competitive Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_fiscal.html">
   52. Cass-Koopmans Model with Distorting Taxes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_fiscal_2.html">
   53. Two-Country Model with Distorting Taxes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ak2.html">
   54. Transitions in an Overlapping Generations Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_problem.html">
   55. Cake Eating I: Introduction to Optimal Saving
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_numerical.html">
   56. Cake Eating II: Numerical Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth.html">
   57. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth_fast.html">
   58. Optimal Growth II: Accelerating the Code with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coleman_policy_iter.html">
   59. Optimal Growth III: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="egm_policy_iter.html">
   60. Optimal Growth IV: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp.html">
   61. The Income Fluctuation Problem I: Basic Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp_advanced.html">
   62. The Income Fluctuation Problem II: Stochastic Returns on Assets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lqcontrol.html">
   63. LQ Control: Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lagrangian_lqdp.html">
   64. Lagrangian for LQ Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cross_product_trick.html">
   65. Eliminating Cross Products
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income.html">
   66. The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income_cons.html">
   67. Permanent Income II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lq_inventories.html">
   68. Production Smoothing via Inventories
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lake_model.html">
   69. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rational_expectations.html">
   70. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="re_with_feedback.html">
   71. Stability in Linear Rational Expectations Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_perf.html">
   72. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncertainty_traps.html">
   73. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aiyagari.html">
   74. The Aiyagari Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ak_aiyagari.html">
   75. A Long-Lived, Heterogeneous Agent, Overlapping Generations Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markov_asset.html">
   76. Asset Pricing: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ge_arrow.html">
   77. Competitive Equilibria with Arrow Securities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrison_kreps.html">
   78. Heterogeneous Beliefs and Bubbles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data and Empirics
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_panel.html">
   79. Pandas for Panel Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ols.html">
   80. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   81. Maximum Likelihood Estimation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Auctions
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="two_auctions.html">
   82. First-Price and Second-Price Auctions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="house_auction.html">
   83. Multiple Good Allocation Mechanisms
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   84. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   85. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   86. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                </ul>

                <ul class="qe-toolbar__links">
                    <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search..." aria-label="Search..." autocomplete="off" accesskey="k">
                            <i data-feather="search" id="search-icon"></i>
                        </form>
                    </li>
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/mle.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <!--
                    # Enable if looking for link to specific document hosted on GitHub
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst/blob/main/lectures/mle.md" download><i data-feather="github"></i></a></li>
                    -->
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="/_pdf/quantecon-python.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/main/mle.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python.notebooks" data-urlpath="tree/lecture-python.notebooks/mle.ipynb" data-branch=main>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/main/mle.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "mle";
                const repoURL = "https://github.com/QuantEcon/lecture-python.notebooks";
                const urlPath = "tree/lecture-python.notebooks/mle.ipynb";
                const branch = "main"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
  </body>
</html>