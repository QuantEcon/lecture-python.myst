{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a861246a",
   "metadata": {},
   "source": [
    "(optgrowth)=\n",
    "```{raw} jupyter\n",
    "<div id=\"qe-notebook-header\" align=\"right\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" width=\"250px\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# {index}`Optimal Savings III: Stochastic Returns <single: Optimal Savings III: Stochastic Returns>`\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture, we continue our study of optimal savings problems, building on\n",
    "{doc}`os` and {doc}`os_numerical`.\n",
    "\n",
    "The key difference from the previous lectures is that wealth now evolves\n",
    "stochastically.\n",
    "\n",
    "We can think of wealth as a harvest that regrows if we save some seeds.\n",
    "\n",
    "Specifically, if we save and invest part of today's harvest $x_t$, it grows into next\n",
    "period's harvest $x_{t+1}$ according to a stochastic production process.\n",
    "\n",
    "The extensions in this lecture introduce several new elements:\n",
    "\n",
    "* nonlinear returns to saving, through a production function, and\n",
    "* stochastic returns, due to shocks to production.\n",
    "\n",
    "Despite these additions, the model remains relatively tractable.\n",
    "\n",
    "As a first pass, we will solve the model using dynamic programming and value function iteration (VFI).\n",
    "\n",
    "```{note}\n",
    "In later lectures we'll explore more efficient methods for this class of problems.\n",
    "\n",
    "At the same time, VFI is foundational and globally convergent.\n",
    "\n",
    "Hence we want to be sure we can use this method too.\n",
    "```\n",
    "\n",
    "More information on this savings problem can be found in\n",
    "\n",
    "* {cite}`Ljungqvist2012`, Section 3.1\n",
    "* [EDTC](https://johnstachurski.net/edtc.html), Chapter 1\n",
    "* {cite}`Sundaram1996`, Chapter 12\n",
    "\n",
    "Let's start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e879067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize_scalar\n",
    "from typing import NamedTuple, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721d800",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "```{index} single: Optimal Savings; Model\n",
    "```\n",
    "\n",
    "Here we described the new model and the optimization problem.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Consider an agent who owns an amount $x_t \\in \\mathbb R_+ := [0, \\infty)$ of a consumption good at time $t$.\n",
    "\n",
    "This output can either be consumed or saved and used for production.\n",
    "\n",
    "Production is stochastic, in that it also depends on a shock $\\xi_{t+1}$ realized at the end of the current period.\n",
    "\n",
    "Next period output is\n",
    "\n",
    "$$\n",
    "x_{t+1} := f(s_t) \\xi_{t+1}\n",
    "$$\n",
    "\n",
    "where $f \\colon \\mathbb R_+ \\to \\mathbb R_+$ is the **production function** and\n",
    "\n",
    "```{math}\n",
    ":label: outcsdp0\n",
    "\n",
    "s_t = x_t - c_t \n",
    "```\n",
    "\n",
    "is **current savings**.\n",
    "\n",
    "and all variables are required to be nonnegative.\n",
    "\n",
    "In what follows,\n",
    "\n",
    "* The sequence $\\{\\xi_t\\}$ is assumed to be IID.\n",
    "* The common distribution of each $\\xi_t$ will be denoted by $\\phi$.\n",
    "* The production function $f$ is assumed to be increasing and continuous.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "\n",
    "Taking $x_0$ as given, the agent wishes to maximize\n",
    "\n",
    "```{math}\n",
    ":label: texs0_og2\n",
    "\n",
    "\\mathbb E \\left[ \\sum_{t = 0}^{\\infty} \\beta^t u(c_t) \\right]\n",
    "```\n",
    "\n",
    "subject to\n",
    "\n",
    "```{math}\n",
    ":label: og_conse\n",
    "\n",
    "x_{t+1} = f(x_t - c_t) \\xi_{t+1}\n",
    "\\quad \\text{and} \\quad\n",
    "0 \\leq c_t \\leq x_t\n",
    "\\quad \\text{for all } t\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "* $u$ is a bounded, continuous and strictly increasing utility function and\n",
    "* $\\beta \\in (0, 1)$ is a discount factor.\n",
    "\n",
    "In summary, the agent's aim is to select a path $c_0, c_1, c_2, \\ldots$ for consumption that is\n",
    "\n",
    "1. nonnegative,\n",
    "1. feasible,\n",
    "1. optimal, in the sense that it maximizes {eq}`texs0_og2` relative to all other feasible consumption sequences, and\n",
    "1. **adapted**, in the sense that the current action $c_t$ depends only on current and historical outcomes, not on future outcomes such as $\\xi_{t+1}$.\n",
    "\n",
    "In the present context\n",
    "\n",
    "* $x_t$ is called the **state** variable --- it summarizes the \"state of the world\" at the start of each period.\n",
    "* $c_t$ is called the **control** variable --- a value chosen by the agent each period after observing the state.\n",
    "\n",
    "\n",
    "\n",
    "### The Policy Function Approach\n",
    "\n",
    "```{index} single: Optimal Savings; Policy Function Approach\n",
    "```\n",
    "\n",
    "One way to think about solving this problem is to look for the best **policy function**.\n",
    "\n",
    "A policy function is a map from past and present observables into current action.\n",
    "\n",
    "We'll be particularly interested in **Markov policies**, which are maps from the current state $x_t$ into a current action $c_t$.\n",
    "\n",
    "For dynamic programming problems such as this one, the optimal policy is always a Markov policy (see, e.g., [DP1](https://dp.quantecon.org/)).\n",
    "\n",
    "In other words, the current state $x_t$ provides a sufficient statistic for the history\n",
    "in terms of making an optimal decision today.\n",
    "\n",
    "In our context, a Markov policy is a function $\\sigma \\colon\n",
    "\\mathbb R_+ \\to \\mathbb R_+$, with the understanding that states are mapped to actions via\n",
    "\n",
    "$$\n",
    "c_t = \\sigma(x_t) \\quad \\text{for all } t\n",
    "$$\n",
    "\n",
    "In what follows, we will call $\\sigma$ a **feasible consumption policy** if it satisfies\n",
    "\n",
    "```{math}\n",
    ":label: idp_fp_og2\n",
    "\n",
    "0 \\leq \\sigma(x) \\leq x\n",
    "\\quad \\text{for all} \\quad\n",
    "x \\in \\mathbb R_+\n",
    "```\n",
    "\n",
    "In other words, a feasible consumption policy is a Markov policy that respects the resource constraint.\n",
    "\n",
    "The set of all feasible consumption policies will be denoted by $\\Sigma$.\n",
    "\n",
    "Each $\\sigma \\in \\Sigma$ determines a [continuous state Markov process](https://python-advanced.quantecon.org/stationary_densities.html) $\\{x_t\\}$ for output via\n",
    "\n",
    "```{math}\n",
    ":label: firstp0_og2\n",
    "\n",
    "x_{t+1} = f(x_t - \\sigma(x_t)) \\xi_{t+1},\n",
    "\\quad x_0 \\text{ given}\n",
    "```\n",
    "\n",
    "This is the time path for output when we choose and stick with the policy $\\sigma$.\n",
    "\n",
    "We insert this process into the objective function to get\n",
    "\n",
    "```{math}\n",
    ":label: texss\n",
    "\n",
    "\\mathbb E\n",
    "\\left[ \\,\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t u(c_t) \\,\n",
    "\\right] =\n",
    "\\mathbb E\n",
    "\\left[ \\,\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t u(\\sigma(x_t)) \\,\n",
    "\\right]\n",
    "```\n",
    "\n",
    "This is the total expected present value of following policy $\\sigma$ forever,\n",
    "given initial income $x_0$.\n",
    "\n",
    "The aim is to select a policy that makes this number as large as possible.\n",
    "\n",
    "The next section covers these ideas more formally.\n",
    "\n",
    "\n",
    "\n",
    "### Optimality\n",
    "\n",
    "The lifetime value $v_{\\sigma}$ associated with a given policy $\\sigma$ is the mapping defined by\n",
    "\n",
    "```{math}\n",
    ":label: vfcsdp00\n",
    "\n",
    "v_{\\sigma}(x) =\n",
    "\\mathbb E \\left[ \\sum_{t = 0}^{\\infty} \\beta^t u(\\sigma(x_t)) \\right]\n",
    "```\n",
    "\n",
    "when $\\{x_t\\}$ is given by {eq}`firstp0_og2` with $x_0 = x$.\n",
    "\n",
    "In other words, it is the lifetime value of following policy $\\sigma$ forever, starting at initial condition $x$.\n",
    "\n",
    "The **value function** is then defined as\n",
    "\n",
    "```{math}\n",
    ":label: vfcsdp0\n",
    "\n",
    "v^*(x) := \\sup_{\\sigma \\in \\Sigma} \\; v_{\\sigma}(x)\n",
    "```\n",
    "\n",
    "The value function gives the maximal value that can be obtained from state $x$,\n",
    "after considering all feasible policies.\n",
    "\n",
    "A policy $\\sigma \\in \\Sigma$ is called **optimal** if it attains the supremum in\n",
    "{eq}`vfcsdp0` for all $x \\in \\mathbb R_+$.\n",
    "\n",
    "\n",
    "### The Bellman Equation\n",
    "\n",
    "The following equation is called the **Bellman equation** associated with this\n",
    "dynamic programming problem.\n",
    "\n",
    "```{math}\n",
    ":label: fpb30\n",
    "\n",
    "v(x) = \\max_{0 \\leq c \\leq x}\n",
    "    \\left\\{\n",
    "        u(c) + \\beta \\int v(f(x - c) z) \\phi(dz)\n",
    "    \\right\\}\n",
    "\\qquad (x \\in \\mathbb R_+)\n",
    "```\n",
    "\n",
    "This is a *functional equation in* $v$, in the sense that a given $v$ can either\n",
    "satisfy it or not satisfy it.\n",
    "\n",
    "The term $\\int v(f(x - c) z) \\phi(dz)$ can be understood as the expected next period value when\n",
    "\n",
    "* $v$ is used to measure value\n",
    "* the state is $x$\n",
    "* consumption is set to $c$\n",
    "\n",
    "As shown in [EDTC](https://johnstachurski.net/edtc.html), Theorem 10.1.11 and a range of other texts,\n",
    "the value function $v^*$ satisfies the Bellman equation.\n",
    "\n",
    "In other words, {eq}`fpb30` holds when $v=v^*$.\n",
    "\n",
    "The intuition is that maximal value from a given state can be obtained by optimally trading off\n",
    "\n",
    "* current reward from a given action, vs\n",
    "* expected discounted future value of the state resulting from that action\n",
    "\n",
    "The Bellman equation is important because it \n",
    "\n",
    "1. gives us more information about the value function and\n",
    "2. suggests a way of computing the value function, which we discuss below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Greedy Policies\n",
    "\n",
    "The value function can be used to compute optimal policies.\n",
    "\n",
    "Given a continuous function $v$ on $\\mathbb R_+$, we say that\n",
    "$\\sigma \\in \\Sigma$ is $v$-**greedy** if $\\sigma(x)$ is a solution to\n",
    "\n",
    "```{math}\n",
    ":label: defgp20\n",
    "\n",
    "\\max_{0 \\leq c \\leq x}\n",
    "    \\left\\{\n",
    "    u(c) + \\beta \\int v(f(x - c) z) \\phi(dz)\n",
    "    \\right\\}\n",
    "```\n",
    "\n",
    "for every $x \\in \\mathbb R_+$.\n",
    "\n",
    "In other words, $\\sigma \\in \\Sigma$ is $v$-greedy if it optimally\n",
    "trades off current and future rewards when $v$ is taken to be the value\n",
    "function.\n",
    "\n",
    "In our setting, we have the following key result\n",
    "\n",
    "* A feasible consumption policy is optimal if and only if it is $v^*$-greedy.\n",
    "\n",
    "The intuition is similar to the intuition for the Bellman equation, which was\n",
    "provided after {eq}`fpb30`.\n",
    "\n",
    "See, for example, Theorem 10.1.11 of [EDTC](https://johnstachurski.net/edtc.html).\n",
    "\n",
    "Hence, once we have a good approximation to $v^*$, we can compute the\n",
    "(approximately) optimal policy by computing the corresponding greedy policy.\n",
    "\n",
    "The advantage is that we are now solving a much lower dimensional optimization\n",
    "problem.\n",
    "\n",
    "\n",
    "### The Bellman Operator\n",
    "\n",
    "How, then, should we compute the value function?\n",
    "\n",
    "One way is to use the so-called **Bellman operator**.\n",
    "\n",
    "(The term **operator** is usually reserved for functions that send functions into functions!)\n",
    "\n",
    "The Bellman operator is denoted by $T$ and defined by\n",
    "\n",
    "```{math}\n",
    ":label: fcbell20_optgrowth\n",
    "\n",
    "Tv(x) := \\max_{0 \\leq c \\leq x}\n",
    "\\left\\{\n",
    "    u(c) + \\beta \\int v(f(x - c) z) \\phi(dz)\n",
    "\\right\\}\n",
    "\\qquad (x \\in \\mathbb R_+)\n",
    "```\n",
    "\n",
    "In other words, $T$ sends the function $v$ into the new function $Tv$ defined by {eq}`fcbell20_optgrowth`.\n",
    "\n",
    "By construction, the set of solutions to the Bellman equation {eq}`fpb30`\n",
    "*exactly coincides with* the set of fixed points of $T$.\n",
    "\n",
    "For example, if $Tv = v$, then, for any $x \\geq 0$,\n",
    "\n",
    "$$\n",
    "v(x)\n",
    "= Tv(x)\n",
    "= \\max_{0 \\leq c \\leq x}\n",
    "\\left\\{\n",
    "    u(c) + \\beta \\int v^*(f(x - c) z) \\phi(dz)\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "which says precisely that $v$ is a solution to the Bellman equation.\n",
    "\n",
    "It follows that $v^*$ is a fixed point of $T$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Review of Theoretical Results\n",
    "\n",
    "```{index} single: Dynamic Programming; Theory\n",
    "```\n",
    "\n",
    "One can also show that $T$ is a contraction mapping on the set of\n",
    "continuous bounded functions on $\\mathbb R_+$ under the supremum distance\n",
    "\n",
    "$$\n",
    "\\rho(g, h) = \\sup_{x \\geq 0} |g(x) - h(x)|\n",
    "$$\n",
    "\n",
    "See  [EDTC](https://johnstachurski.net/edtc.html), lemma 10.1.18.\n",
    "\n",
    "Hence, it has exactly one fixed point in this set, which we know is equal to the value function.\n",
    "\n",
    "It follows that\n",
    "\n",
    "* The value function $v^*$ is bounded and continuous.\n",
    "* Starting from any bounded and continuous $v$, the sequence $v, Tv, T^2v, \\ldots$\n",
    "  generated by iteratively applying $T$ converges uniformly to $v^*$.\n",
    "\n",
    "This iterative method is called **value function iteration**.\n",
    "\n",
    "We also know that a feasible policy is optimal if and only if it is $v^*$-greedy.\n",
    "\n",
    "It's not too hard to show that a $v^*$-greedy policy exists\n",
    "(see  [EDTC](https://johnstachurski.net/edtc.html), theorem 10.1.11 if you get stuck).\n",
    "\n",
    "Hence, at least one optimal policy exists.\n",
    "\n",
    "Our problem now is how to compute it.\n",
    "\n",
    "### {index}`Unbounded Utility <single: Unbounded Utility>`\n",
    "\n",
    "```{index} single: Dynamic Programming; Unbounded Utility\n",
    "```\n",
    "\n",
    "The results stated above assume that $u$ is bounded.\n",
    "\n",
    "In practice economists often work with unbounded utility functions --- and so will we.\n",
    "\n",
    "In the unbounded setting, various optimality theories exist.\n",
    "\n",
    "Nevertheless, their main conclusions are usually in line with those stated for\n",
    "the bounded case just above (as long as we drop the word \"bounded\").\n",
    "\n",
    "```{note}\n",
    "\n",
    "Consult the following references for more on the unbounded case:\n",
    "\n",
    "* The lecture {doc}`ifp_advanced`.\n",
    "* Section 12.2 of [EDTC](https://johnstachurski.net/edtc.html).\n",
    "```\n",
    "\n",
    "\n",
    "## Computation\n",
    "\n",
    "```{index} single: Dynamic Programming; Computation\n",
    "```\n",
    "\n",
    "Let's now look at computing the value function and the optimal policy.\n",
    "\n",
    "Our implementation in this lecture will focus on clarity and\n",
    "flexibility.\n",
    "\n",
    "(In subsequent lectures we will focus on efficiency and speed.)\n",
    "\n",
    "We will use fitted value function iteration, which was\n",
    "already described in {doc}`os_numerical`.\n",
    "\n",
    "\n",
    "### Scalar Maximization\n",
    "\n",
    "To maximize the right hand side of the Bellman equation {eq}`fpb30`, we are going to use\n",
    "the `minimize_scalar` routine from SciPy.\n",
    "\n",
    "To keep the interface tidy, we will wrap `minimize_scalar` in an outer function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83999236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(g, a, b, args):\n",
    "    \"\"\"\n",
    "    Maximize the function g over the interval [a, b].\n",
    "\n",
    "    We use the fact that the maximizer of g on any interval is\n",
    "    also the minimizer of -g.  The tuple args collects any extra\n",
    "    arguments to g.\n",
    "\n",
    "    Returns the maximal value and the maximizer.\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1fe9e",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will assume for now that $\\phi$ is the distribution of $\\xi := \\exp(\\mu + \\nu \\zeta)$ where\n",
    "\n",
    "* $\\zeta$ is standard normal,\n",
    "* $\\mu$ is a shock location parameter and\n",
    "* $\\nu$ is a shock scale parameter.\n",
    "\n",
    "We will store the primitives of the model in a `NamedTuple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a08299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    u: Callable        # utility function\n",
    "    f: Callable        # production function\n",
    "    β: float           # discount factor\n",
    "    μ: float           # shock location parameter\n",
    "    ν: float           # shock scale parameter\n",
    "    grid: np.ndarray   # state grid\n",
    "    shocks: np.ndarray # shock draws\n",
    "\n",
    "\n",
    "def create_model(u: Callable,\n",
    "                 f: Callable,\n",
    "                 β: float = 0.96,\n",
    "                 μ: float = 0.0,\n",
    "                 ν: float = 0.1,\n",
    "                 grid_max: float = 4.0,\n",
    "                 grid_size: int = 120,\n",
    "                 shock_size: int = 250,\n",
    "                 seed: int = 1234) -> Model:\n",
    "    \"\"\"\n",
    "    Creates an instance of the optimal savings model.\n",
    "    \"\"\"\n",
    "    # Set up grid\n",
    "    grid = np.linspace(1e-4, grid_max, grid_size)\n",
    "\n",
    "    # Store shocks (with a seed, so results are reproducible)\n",
    "    np.random.seed(seed)\n",
    "    shocks = np.exp(μ + ν * np.random.randn(shock_size))\n",
    "\n",
    "    return Model(u, f, β, μ, ν, grid, shocks)\n",
    "\n",
    "\n",
    "def state_action_value(c: float,\n",
    "                       model: Model,\n",
    "                       x: float,\n",
    "                       v_array: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Right hand side of the Bellman equation.\n",
    "    \"\"\"\n",
    "    u, f, β, shocks = model.u, model.f, model.β, model.shocks\n",
    "    grid = model.grid\n",
    "\n",
    "    v = interp1d(grid, v_array)\n",
    "\n",
    "    return u(c) + β * np.mean(v(f(x - c) * shocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52921484",
   "metadata": {},
   "source": [
    "In the second last line we are using linear interpolation.\n",
    "\n",
    "In the last line, the expectation in {eq}`fcbell20_optgrowth` is\n",
    "computed via [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_integration), using the approximation\n",
    "\n",
    "$$\n",
    "\\int v(f(x - c) z) \\phi(dz) \\approx \\frac{1}{n} \\sum_{i=1}^n v(f(x - c) \\xi_i)\n",
    "$$\n",
    "\n",
    "where $\\{\\xi_i\\}_{i=1}^n$ are IID draws from $\\phi$.\n",
    "\n",
    "Monte Carlo is not always the most efficient way to compute integrals numerically\n",
    "but it does have some theoretical advantages in the present setting.\n",
    "\n",
    "(For example, it preserves the contraction mapping property of the Bellman operator --- see, e.g., {cite}`pal2013`.)\n",
    "\n",
    "### The Bellman Operator\n",
    "\n",
    "The next function implements the Bellman operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(v: np.ndarray, model: Model) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function\n",
    "    and also computes a v-greedy policy.\n",
    "\n",
    "      * model is an instance of Model\n",
    "      * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    grid = model.grid\n",
    "    v_new = np.empty_like(v)\n",
    "    v_greedy = np.empty_like(v)\n",
    "\n",
    "    for i in range(len(grid)):\n",
    "        x = grid[i]\n",
    "\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        c_star, v_max = maximize(state_action_value, 1e-10, x, (model, x, v))\n",
    "        v_new[i] = v_max\n",
    "        v_greedy[i] = c_star\n",
    "\n",
    "    return v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc94b8c",
   "metadata": {},
   "source": [
    "(benchmark_cake_mod)=\n",
    "### An Example\n",
    "\n",
    "Let's suppose now that\n",
    "\n",
    "$$\n",
    "f(x-c) = (x-c)^{\\alpha}\n",
    "\\quad \\text{and} \\quad\n",
    "u(c) = \\ln c\n",
    "$$\n",
    "\n",
    "For this particular problem, an exact analytical solution is available (see\n",
    "{cite}`Ljungqvist2012`, section 3.1.2), with\n",
    "\n",
    "```{math}\n",
    ":label: dpi_tv\n",
    "\n",
    "v^*(x) =\n",
    "\\frac{\\ln (1 - \\alpha \\beta) }{ 1 - \\beta} +\n",
    "\\frac{(\\mu + \\alpha \\ln (\\alpha \\beta))}{1 - \\alpha}\n",
    " \\left[\n",
    "     \\frac{1}{1- \\beta} - \\frac{1}{1 - \\alpha \\beta}\n",
    " \\right] +\n",
    " \\frac{1}{1 - \\alpha \\beta} \\ln x\n",
    "```\n",
    "\n",
    "and optimal consumption policy\n",
    "\n",
    "$$\n",
    "\\sigma^*(x) = (1 - \\alpha \\beta ) x\n",
    "$$\n",
    "\n",
    "It is valuable to have these closed-form solutions because it lets us check\n",
    "whether our code works for this particular case.\n",
    "\n",
    "In Python, the functions above can be expressed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23425962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_star(x, α, β, μ):\n",
    "    \"\"\"\n",
    "    True value function\n",
    "    \"\"\"\n",
    "    c1 = np.log(1 - α * β) / (1 - β)\n",
    "    c2 = (μ + α * np.log(α * β)) / (1 - α)\n",
    "    c3 = 1 / (1 - β)\n",
    "    c4 = 1 / (1 - α * β)\n",
    "    return c1 + c2 * (c3 - c4) + c4 * np.log(x)\n",
    "\n",
    "def σ_star(x, α, β):\n",
    "    \"\"\"\n",
    "    True optimal policy\n",
    "    \"\"\"\n",
    "    return (1 - α * β) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e2f93",
   "metadata": {},
   "source": [
    "Next let's create an instance of the model with the above primitives and assign it to the variable `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d32d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 0.4\n",
    "def fcd(s):\n",
    "    return s**α\n",
    "\n",
    "model = create_model(u=np.log, f=fcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29120856",
   "metadata": {},
   "source": [
    "Now let's see what happens when we apply our Bellman operator to the exact\n",
    "solution $v^*$ in this case.\n",
    "\n",
    "In theory, since $v^*$ is a fixed point, the resulting function should again be $v^*$.\n",
    "\n",
    "In practice, we expect some small numerical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8673983",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = model.grid\n",
    "\n",
    "v_init = v_star(grid, α, model.β, model.μ)    # Start at the solution\n",
    "v_greedy, v = T(v_init, model)             # Apply T once\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(-35, -24)\n",
    "ax.plot(grid, v, lw=2, alpha=0.6, label='$Tv^*$')\n",
    "ax.plot(grid, v_init, lw=2, alpha=0.6, label='$v^*$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca779af5",
   "metadata": {},
   "source": [
    "The two functions are essentially indistinguishable, so we are off to a good start.\n",
    "\n",
    "Now let's have a look at iterating with the Bellman operator, starting\n",
    "from an arbitrary initial condition.\n",
    "\n",
    "The initial condition we'll start with is, somewhat arbitrarily, $v(x) = 5 \\ln (x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97938d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 5 * np.log(grid)  # An initial condition\n",
    "n = 35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v, color=plt.cm.jet(0),\n",
    "        lw=2, alpha=0.6, label='Initial condition')\n",
    "\n",
    "for i in range(n):\n",
    "    v_greedy, v = T(v, model)  # Apply the Bellman operator\n",
    "    ax.plot(grid, v, color=plt.cm.jet(i / n), lw=2, alpha=0.6)\n",
    "\n",
    "ax.plot(grid, v_star(grid, α, model.β, model.μ), 'k-', lw=2,\n",
    "        alpha=0.8, label='True value function')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(ylim=(-40, 10), xlim=(np.min(grid), np.max(grid)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750e9b0",
   "metadata": {},
   "source": [
    "The figure shows\n",
    "\n",
    "1. the first 36 functions generated by the fitted value function iteration algorithm, with hotter colors given to higher iterates\n",
    "1. the true value function $v^*$ drawn in black\n",
    "\n",
    "The sequence of iterates converges towards $v^*$.\n",
    "\n",
    "We are clearly getting closer.\n",
    "\n",
    "### Iterating to Convergence\n",
    "\n",
    "We can write a function that iterates until the difference is below a particular\n",
    "tolerance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_model(og,\n",
    "                tol=1e-4,\n",
    "                max_iter=1000,\n",
    "                verbose=True,\n",
    "                print_skip=25):\n",
    "    \"\"\"\n",
    "    Solve model by iterating with the Bellman operator.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up loop\n",
    "    v = og.u(og.grid)  # Initial condition\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_greedy, v_new = T(v, og)\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "        v = v_new\n",
    "\n",
    "    if error > tol:\n",
    "        print(\"Failed to converge!\")\n",
    "    elif verbose:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea54678",
   "metadata": {},
   "source": [
    "Let's use this function to compute an approximate solution at the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ecb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_greedy, v_solution = solve_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a21c22",
   "metadata": {},
   "source": [
    "Now we check our result by plotting it against the true value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_solution, lw=2, alpha=0.6,\n",
    "        label='Approximate value function')\n",
    "\n",
    "ax.plot(grid, v_star(grid, α, model.β, model.μ), lw=2,\n",
    "        alpha=0.6, label='True value function')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(-35, -24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d07ec",
   "metadata": {},
   "source": [
    "The figure shows that we are pretty much on the money.\n",
    "\n",
    "### The Policy Function\n",
    "\n",
    "```{index} single: Optimal Savings; Policy Function\n",
    "```\n",
    "\n",
    "The policy `v_greedy` computed above corresponds to an approximate optimal policy.\n",
    "\n",
    "The next figure compares it to the exact solution, which, as mentioned\n",
    "above, is $\\sigma(x) = (1 - \\alpha \\beta) x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_greedy, lw=2,\n",
    "        alpha=0.6, label='approximate policy function')\n",
    "\n",
    "ax.plot(grid, σ_star(grid, α, model.β), '--',\n",
    "        lw=2, alpha=0.6, label='true policy function')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e058f5",
   "metadata": {},
   "source": [
    "The figure shows that we've done a good job in this instance of approximating\n",
    "the true policy.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "\n",
    "```{exercise}\n",
    ":label: og_ex1\n",
    "\n",
    "A common choice for utility function in this kind of work is the CRRA\n",
    "specification\n",
    "\n",
    "$$\n",
    "u(c) = \\frac{c^{1 - \\gamma}} {1 - \\gamma}\n",
    "$$\n",
    "\n",
    "Maintaining the other defaults, including the Cobb-Douglas production\n",
    "function,  solve the optimal savings model with this\n",
    "utility specification.\n",
    "\n",
    "Setting $\\gamma = 1.5$, compute and plot an estimate of the optimal policy.\n",
    "\n",
    "```\n",
    "\n",
    "```{solution-start} og_ex1\n",
    ":class: dropdown\n",
    "```\n",
    "\n",
    "Here we set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfee43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 1.5   # Preference parameter\n",
    "\n",
    "def u_crra(c):\n",
    "    return (c**(1 - γ) - 1) / (1 - γ)\n",
    "\n",
    "model = create_model(u=u_crra, f=fcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e3404",
   "metadata": {},
   "source": [
    "Now let's run it, with a timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e203a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v_greedy, v_solution = solve_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a60f7f",
   "metadata": {},
   "source": [
    "Let's plot the policy function just to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(grid, v_greedy, lw=2,\n",
    "        alpha=0.6, label='Approximate optimal policy')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff8384",
   "metadata": {},
   "source": [
    "```{solution-end}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   10,
   65,
   72,
   458,
   474,
   488,
   534,
   556,
   579,
   618,
   634,
   638,
   644,
   653,
   665,
   674,
   693,
   709,
   739,
   743,
   745,
   749,
   761,
   775,
   786,
   818,
   825,
   829,
   832,
   836,
   844
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}