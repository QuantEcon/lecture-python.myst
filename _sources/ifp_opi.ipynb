{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f472e5e9",
   "metadata": {},
   "source": [
    "# The Income Fluctuation Problem II: Optimistic Policy Iteration\n",
    "\n",
    "```{include} _admonition/gpu.md\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In {doc}`ifp_discrete` we studied the income fluctuation problem and solved it\n",
    "using value function iteration (VFI).\n",
    "\n",
    "In this lecture we'll solve the same problem using **optimistic policy\n",
    "iteration** (OPI), which is very general, typically faster than VFI and only\n",
    "slightly more complex.\n",
    "\n",
    "OPI combines elements of both value function iteration and policy iteration.\n",
    "\n",
    "A detailed discussion of the algorithm can be found in [DP1](https://dp.quantecon.org).\n",
    "\n",
    "Here our aim is to implement OPI and test whether or not it yields significant\n",
    "speed improvements over standard VFI for the income fluctuation problem.\n",
    "\n",
    "In addition to Anaconda, this lecture will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc10b30",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install quantecon jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34be1b",
   "metadata": {},
   "source": [
    "We will use the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56764309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import NamedTuple\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d6d44",
   "metadata": {},
   "source": [
    "## Model and Primitives\n",
    "\n",
    "The model and parameters are the same as in {doc}`ifp_discrete`.\n",
    "\n",
    "We repeat the key elements here for convenience.\n",
    "\n",
    "The household's problem is to maximize\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\, \\sum_{t=0}^{\\infty} \\beta^t u(c_t)\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "    a_{t+1} + c_t \\leq R a_t + y_t\n",
    "$$\n",
    "\n",
    "where $u(c) = c^{1-\\gamma}/(1-\\gamma)$.\n",
    "\n",
    "Here's the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fcb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    β: float              # Discount factor\n",
    "    R: float              # Gross interest rate\n",
    "    γ: float              # CRRA parameter\n",
    "    a_grid: jnp.ndarray   # Asset grid\n",
    "    y_grid: jnp.ndarray   # Income grid\n",
    "    Q: jnp.ndarray        # Markov matrix for income\n",
    "\n",
    "\n",
    "def create_consumption_model(\n",
    "        R=1.01,                    # Gross interest rate\n",
    "        β=0.98,                    # Discount factor\n",
    "        γ=2,                       # CRRA parameter\n",
    "        a_min=0.01,                # Min assets\n",
    "        a_max=10.0,                # Max assets\n",
    "        a_size=150,                # Grid size\n",
    "        ρ=0.9, ν=0.1, y_size=100   # Income parameters\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates an instance of the consumption-savings model.\n",
    "\n",
    "    \"\"\"\n",
    "    a_grid = jnp.linspace(a_min, a_max, a_size)\n",
    "    mc = qe.tauchen(n=y_size, rho=ρ, sigma=ν)\n",
    "    y_grid, Q = jnp.exp(mc.state_values), jax.device_put(mc.P)\n",
    "    return Model(β, R, γ, a_grid, y_grid, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe5df",
   "metadata": {},
   "source": [
    "## Operators and Policies\n",
    "\n",
    "We repeat some functions from {doc}`ifp_discrete`.\n",
    "\n",
    "Here is the right hand side of the Bellman equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(v, model, i, j, ip):\n",
    "    \"\"\"\n",
    "    The right-hand side of the Bellman equation before maximization, which takes\n",
    "    the form\n",
    "\n",
    "        B(a, y, a′) = u(Ra + y - a′) + β Σ_y′ v(a′, y′) Q(y, y′)\n",
    "\n",
    "    The indices are (i, j, ip) -> (a, y, a′).\n",
    "    \"\"\"\n",
    "    β, R, γ, a_grid, y_grid, Q = model\n",
    "    a, y, ap  = a_grid[i], y_grid[j], a_grid[ip]\n",
    "    c = R * a + y - ap\n",
    "    EV = jnp.sum(v[ip, :] * Q[j, :])\n",
    "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -jnp.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a7eab",
   "metadata": {},
   "source": [
    "Now we successively apply `vmap` to vectorize over all indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_1    = jax.vmap(B,   in_axes=(None, None, None, None, 0))\n",
    "B_2    = jax.vmap(B_1, in_axes=(None, None, None, 0,    None))\n",
    "B_vmap = jax.vmap(B_2, in_axes=(None, None, 0,    None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1b85b",
   "metadata": {},
   "source": [
    "Here's the Bellman operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(v, model):\n",
    "    \"The Bellman operator.\"\n",
    "    a_indices = jnp.arange(len(model.a_grid))\n",
    "    y_indices = jnp.arange(len(model.y_grid))\n",
    "    B_values = B_vmap(v, model, a_indices, y_indices, a_indices)\n",
    "    return jnp.max(B_values, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce811f8",
   "metadata": {},
   "source": [
    "Here's the function that computes a $v$-greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy(v, model):\n",
    "    \"Computes a v-greedy policy, returned as a set of indices.\"\n",
    "    a_indices = jnp.arange(len(model.a_grid))\n",
    "    y_indices = jnp.arange(len(model.y_grid))\n",
    "    B_values = B_vmap(v, model, a_indices, y_indices, a_indices)\n",
    "    return jnp.argmax(B_values, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3d0c0",
   "metadata": {},
   "source": [
    "Now we define the policy operator $T_\\sigma$, which is the Bellman operator with\n",
    "policy $\\sigma$ fixed.\n",
    "\n",
    "For a given policy $\\sigma$, the policy operator is defined by\n",
    "\n",
    "$$\n",
    "    (T_\\sigma v)(a, y) = u(Ra + y - \\sigma(a, y)) + \\beta \\sum_{y'} v(\\sigma(a, y), y') Q(y, y')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5841f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_σ(v, σ, model, i, j):\n",
    "    \"\"\"\n",
    "    The σ-policy operator for indices (i, j) -> (a, y).\n",
    "    \"\"\"\n",
    "    β, R, γ, a_grid, y_grid, Q = model\n",
    "\n",
    "    # Get values at current state\n",
    "    a, y = a_grid[i], y_grid[j]\n",
    "    # Get policy choice\n",
    "    ap = a_grid[σ[i, j]]\n",
    "\n",
    "    # Compute current reward\n",
    "    c = R * a + y - ap\n",
    "    r = jnp.where(c > 0, c**(1-γ)/(1-γ), -jnp.inf)\n",
    "\n",
    "    # Compute expected value\n",
    "    EV = jnp.sum(v[σ[i, j], :] * Q[j, :])\n",
    "\n",
    "    return r + β * EV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eeb80f",
   "metadata": {},
   "source": [
    "Apply vmap to vectorize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_σ_1    = jax.vmap(T_σ,   in_axes=(None, None, None, None, 0))\n",
    "T_σ_vmap = jax.vmap(T_σ_1, in_axes=(None, None, None, 0,    None))\n",
    "\n",
    "def T_σ_vec(v, σ, model):\n",
    "    \"\"\"Vectorized version of T_σ.\"\"\"\n",
    "    a_size, y_size = len(model.a_grid), len(model.y_grid)\n",
    "    a_indices = jnp.arange(a_size)\n",
    "    y_indices = jnp.arange(y_size)\n",
    "    return T_σ_vmap(v, σ, model, a_indices, y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe05ad3",
   "metadata": {},
   "source": [
    "Now we need a function to apply the policy operator m times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_policy_operator(σ, v, m, model):\n",
    "    \"\"\"\n",
    "    Apply the policy operator T_σ exactly m times to v.\n",
    "    \"\"\"\n",
    "    def update(i, v):\n",
    "        return T_σ_vec(v, σ, model)\n",
    "\n",
    "    v = jax.lax.fori_loop(0, m, update, v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e283386",
   "metadata": {},
   "source": [
    "## Value Function Iteration\n",
    "\n",
    "For comparison, here's VFI from {doc}`ifp_discrete`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def value_function_iteration(model, tol=1e-5, max_iter=10_000):\n",
    "    \"\"\"\n",
    "    Implements VFI using successive approximation.\n",
    "    \"\"\"\n",
    "    def body_fun(k_v_err):\n",
    "        k, v, error = k_v_err\n",
    "        v_new = T(v, model)\n",
    "        error = jnp.max(jnp.abs(v_new - v))\n",
    "        return k + 1, v_new, error\n",
    "\n",
    "    def cond_fun(k_v_err):\n",
    "        k, v, error = k_v_err\n",
    "        return jnp.logical_and(error > tol, k < max_iter)\n",
    "\n",
    "    v_init = jnp.zeros((len(model.a_grid), len(model.y_grid)))\n",
    "    k, v_star, error = jax.lax.while_loop(cond_fun, body_fun,\n",
    "                                          (1, v_init, tol + 1))\n",
    "    return v_star, get_greedy(v_star, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcde32f",
   "metadata": {},
   "source": [
    "## Optimistic Policy Iteration\n",
    "\n",
    "Now we implement OPI.\n",
    "\n",
    "The algorithm alternates between\n",
    "\n",
    "1. Performing $m$ policy operator iterations to update the value function\n",
    "2. Computing a new greedy policy based on the updated value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def optimistic_policy_iteration(model, m=10, tol=1e-5, max_iter=10_000):\n",
    "    \"\"\"\n",
    "    Implements optimistic policy iteration with step size m.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Model\n",
    "        The consumption-savings model\n",
    "    m : int\n",
    "        Number of policy operator iterations per step\n",
    "    tol : float\n",
    "        Tolerance for convergence\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    \"\"\"\n",
    "    v_init = jnp.zeros((len(model.a_grid), len(model.y_grid)))\n",
    "\n",
    "    def condition_function(inputs):\n",
    "        i, v, error = inputs\n",
    "        return jnp.logical_and(error > tol, i < max_iter)\n",
    "\n",
    "    def update(inputs):\n",
    "        i, v, error = inputs\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, model)\n",
    "        v = iterate_policy_operator(σ, v, m, model)\n",
    "        error = jnp.max(jnp.abs(v - last_v))\n",
    "        i += 1\n",
    "        return i, v, error\n",
    "\n",
    "    num_iter, v, error = jax.lax.while_loop(condition_function,\n",
    "                                            update,\n",
    "                                            (0, v_init, tol + 1))\n",
    "\n",
    "    return v, get_greedy(v, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5544ca",
   "metadata": {},
   "source": [
    "## Timing Comparison\n",
    "\n",
    "Let's create a model and compare the performance of VFI and OPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_consumption_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7eb11",
   "metadata": {},
   "source": [
    "First, let's time VFI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting VFI.\")\n",
    "start = time()\n",
    "v_star_vfi, σ_star_vfi = value_function_iteration(model)\n",
    "v_star_vfi.block_until_ready()\n",
    "vfi_time_with_compile = time() - start\n",
    "print(f\"VFI completed in {vfi_time_with_compile:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7646b",
   "metadata": {},
   "source": [
    "Run it again to eliminate compile time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb21190",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "v_star_vfi, σ_star_vfi = value_function_iteration(model)\n",
    "v_star_vfi.block_until_ready()\n",
    "vfi_time = time() - start\n",
    "print(f\"VFI completed in {vfi_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9815fd",
   "metadata": {},
   "source": [
    "Now let's time OPI with different values of m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting OPI with m=50.\")\n",
    "start = time()\n",
    "v_star_opi, σ_star_opi = optimistic_policy_iteration(model, m=50)\n",
    "v_star_opi.block_until_ready()\n",
    "opi_time_with_compile = time() - start\n",
    "print(f\"OPI completed in {opi_time_with_compile:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0fbf5",
   "metadata": {},
   "source": [
    "Run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39839964",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "v_star_opi, σ_star_opi = optimistic_policy_iteration(model, m=50)\n",
    "v_star_opi.block_until_ready()\n",
    "opi_time = time() - start\n",
    "print(f\"OPI completed in {opi_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8ca3f",
   "metadata": {},
   "source": [
    "Check that we get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd800c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Values match: {jnp.allclose(v_star_vfi, v_star_opi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9c7ac",
   "metadata": {},
   "source": [
    "The value functions match, confirming both algorithms converge to the same solution.\n",
    "\n",
    "Let's visually compare the asset dynamics under both policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde554ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# VFI policy\n",
    "for j, label in zip([0, -1], ['low income', 'high income']):\n",
    "    a_next_vfi = model.a_grid[σ_star_vfi[:, j]]\n",
    "    axes[0].plot(model.a_grid, a_next_vfi, label=label)\n",
    "axes[0].plot(model.a_grid, model.a_grid, 'k--', linewidth=0.5, alpha=0.5)\n",
    "axes[0].set(xlabel='current assets', ylabel='next period assets', title='VFI')\n",
    "axes[0].legend()\n",
    "\n",
    "# OPI policy\n",
    "for j, label in zip([0, -1], ['low income', 'high income']):\n",
    "    a_next_opi = model.a_grid[σ_star_opi[:, j]]\n",
    "    axes[1].plot(model.a_grid, a_next_opi, label=label)\n",
    "axes[1].plot(model.a_grid, model.a_grid, 'k--', linewidth=0.5, alpha=0.5)\n",
    "axes[1].set(xlabel='current assets', ylabel='next period assets', title='OPI')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3afa89",
   "metadata": {},
   "source": [
    "The policies are visually indistinguishable, confirming both methods produce the same solution.\n",
    "\n",
    "Here's the speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Speedup factor: {vfi_time / opi_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eff489",
   "metadata": {},
   "source": [
    "Let's try different values of m to see how it affects performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vals = [1, 5, 10, 25, 50, 100, 200, 400]\n",
    "opi_times = []\n",
    "\n",
    "for m in m_vals:\n",
    "    start = time()\n",
    "    v_star, σ_star = optimistic_policy_iteration(model, m=m)\n",
    "    v_star.block_until_ready()\n",
    "    elapsed = time() - start\n",
    "    opi_times.append(elapsed)\n",
    "    print(f\"OPI with m={m:3d} completed in {elapsed:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab882adb",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(m_vals, opi_times, 'o-', label='OPI')\n",
    "ax.axhline(vfi_time, linestyle='--', color='red', label='VFI')\n",
    "ax.set_xlabel('m (policy steps per iteration)')\n",
    "ax.set_ylabel('time (seconds)')\n",
    "ax.legend()\n",
    "ax.set_title('OPI execution time vs step size m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21110bbd",
   "metadata": {},
   "source": [
    "Here's a summary of the results\n",
    "\n",
    "* OPI outperforms VFI for a large range of $m$ values.\n",
    "\n",
    "* For very large $m$, OPI performance begins to degrade as we spend too much\n",
    "  time iterating the policy operator.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "```{exercise}\n",
    ":label: ifp_opi_ex1\n",
    "\n",
    "The speed gains achieved by OPI are quite robust to parameter changes.\n",
    "\n",
    "Confirm this by experimenting with different parameter values for the income process ($\\rho$ and $\\nu$).\n",
    "\n",
    "Measure how they affect the relative performance of VFI vs OPI.\n",
    "\n",
    "Try:\n",
    "* $\\rho \\in \\{0.8, 0.9, 0.95\\}$\n",
    "* $\\nu \\in \\{0.05, 0.1, 0.2\\}$\n",
    "\n",
    "For each combination, compute the speedup factor (VFI time / OPI time) and report your findings.\n",
    "```\n",
    "\n",
    "```{solution-start} ifp_opi_ex1\n",
    ":class: dropdown\n",
    "```\n",
    "\n",
    "Here's one solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c155bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ρ_vals = [0.8, 0.9, 0.95]\n",
    "ν_vals = [0.05, 0.1, 0.2]\n",
    "\n",
    "results = []\n",
    "\n",
    "for ρ in ρ_vals:\n",
    "    for ν in ν_vals:\n",
    "        print(f\"\\nTesting ρ={ρ}, ν={ν}\")\n",
    "\n",
    "        # Create model\n",
    "        model = create_consumption_model(ρ=ρ, ν=ν)\n",
    "\n",
    "        # Time VFI\n",
    "        start = time()\n",
    "        v_vfi, σ_vfi = value_function_iteration(model)\n",
    "        v_vfi.block_until_ready()\n",
    "        vfi_t = time() - start\n",
    "\n",
    "        # Time OPI\n",
    "        start = time()\n",
    "        v_opi, σ_opi = optimistic_policy_iteration(model, m=10)\n",
    "        v_opi.block_until_ready()\n",
    "        opi_t = time() - start\n",
    "\n",
    "        speedup = vfi_t / opi_t\n",
    "        results.append((ρ, ν, speedup))\n",
    "        print(f\"  VFI: {vfi_t:.2f}s, OPI: {opi_t:.2f}s, Speedup: {speedup:.2f}x\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary of speedup factors:\")\n",
    "for ρ, ν, speedup in results:\n",
    "    print(f\"ρ={ρ}, ν={ν}: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80768a6",
   "metadata": {},
   "source": [
    "```{solution-end}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   37,
   41,
   45,
   52,
   78,
   105,
   113,
   128,
   132,
   136,
   140,
   147,
   151,
   158,
   169,
   189,
   193,
   203,
   207,
   217,
   223,
   243,
   254,
   291,
   297,
   299,
   303,
   310,
   314,
   320,
   324,
   331,
   335,
   341,
   345,
   347,
   353,
   374,
   380,
   382,
   386,
   397,
   401,
   410,
   444,
   477
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}