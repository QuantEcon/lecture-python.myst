{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed34761",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **singular value decomposition** (SVD) is a work-horse in applications of least squares projection that\n",
    "form  foundations for many statistical and  machine learning methods.\n",
    "\n",
    "After defining the SVD, we'll describe how it connects to \n",
    "\n",
    "* **four fundamental spaces** of linear algebra\n",
    "* under-determined and over-determined **least squares regressions**  \n",
    "* **principal components analysis** (PCA)\n",
    "\n",
    "Like principal components analysis (PCA), DMD can be thought of as a data-reduction procedure that  represents salient patterns by projecting data onto a limited set of factors.\n",
    "\n",
    "In a sequel to this lecture about  {doc}`Dynamic Mode Decompositions <var_dmd>`, we'll describe how SVD's provide ways rapidly to compute reduced-order approximations to first-order Vector Autoregressions (VARs).\n",
    "\n",
    "##  The Setting\n",
    "\n",
    "Let $X$ be an $m \\times n$ matrix of rank $p$.\n",
    "\n",
    "Necessarily, $p \\leq \\min(m,n)$.\n",
    "\n",
    "In  much of this lecture, we'll think of $X$ as a matrix of **data** in which\n",
    "\n",
    "  * each column is an **individual** -- a time period or person, depending on the application\n",
    "  \n",
    "  * each row is a **random variable** describing an attribute of a time period or a person, depending on the application\n",
    "  \n",
    "  \n",
    "We'll be interested in  two  situations\n",
    "\n",
    "  * A **short and fat** case in which $m << n$, so that there are many more columns (individuals) than rows (attributes).\n",
    "\n",
    "  * A  **tall and skinny** case in which $m >> n$, so that there are many more rows  (attributes) than columns (individuals). \n",
    "    \n",
    "   \n",
    "We'll apply a **singular value decomposition** of $X$ in both situations.\n",
    "\n",
    "In the $ m < < n$ case  in which there are many more individuals $n$ than attributes $m$, we can calculate sample moments of  a joint distribution  by taking averages  across observations of functions of the observations. \n",
    "\n",
    "In this $ m < < n$ case,  we'll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the $m > > n$  case in which there are many more attributes $m$ than individuals $n$ and when we are in a time-series setting in which $n$ equals the number of time periods covered in the data set $X$, we'll proceed in a different way. \n",
    "\n",
    "We'll again use a **singular value decomposition**,  but now to construct a **dynamic mode decomposition** (DMD)\n",
    "\n",
    "## Singular Value Decomposition\n",
    "\n",
    "A **singular value decomposition** of an $m \\times n$ matrix $X$ of rank $p \\leq \\min(m,n)$ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^\\top \n",
    "$$ (eq:SVD101)\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^\\top  &  = I  &  \\quad U^\\top  U = I \\cr    \n",
    "VV^\\top  & = I & \\quad V^\\top  V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    " \n",
    "and\n",
    " \n",
    "* $U$ is an $m \\times m$ orthogonal  matrix of **left singular vectors** of $X$\n",
    "* Columns of $U$ are eigenvectors of $X^\\top  X$\n",
    "* $V$ is an $n \\times n$ orthogonal matrix of **right singular values** of $X$\n",
    "* Columns of $V$  are eigenvectors of $X X^\\top $\n",
    "* $\\Sigma$ is an $m \\times n$ matrix in which the first $p$ places on its main diagonal are positive numbers $\\sigma_1, \\sigma_2, \\ldots, \\sigma_p$ called **singular values**; remaining entries of $\\Sigma$ are all zero\n",
    "\n",
    "* The $p$ singular values are positive square roots of the eigenvalues of the $m \\times m$ matrix  $X X^\\top $ and also of the $n \\times n$ matrix $X^\\top  X$\n",
    "\n",
    "* We adopt a convention that when $U$ is a complex valued matrix, $U^\\top $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $U$, meaning that \n",
    "$U_{ij}^\\top $ is the complex conjugate of $U_{ji}$. \n",
    "\n",
    "* Similarly, when $V$ is a complex valued matrix, $V^\\top $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $V$\n",
    "\n",
    "\n",
    "The matrices $U,\\Sigma,V$ entail linear transformations that reshape in vectors in the following ways:\n",
    "\n",
    "* multiplying vectors  by the unitary matrices $U$ and $V$ **rotates** them, but leaves **angles between vectors** and **lengths of vectors** unchanged.\n",
    "* multiplying vectors by the diagonal  matrix $\\Sigma$ leaves **angles between vectors** unchanged but **rescales** vectors.\n",
    "\n",
    "Thus, representation {eq}`eq:SVD101` asserts that multiplying an $n \\times 1$  vector $y$ by the $m \\times n$ matrix $X$\n",
    "amounts to performing the following three multiplcations of $y$ sequentially:\n",
    "\n",
    "* **rotating** $y$ by computing $V^\\top  y$\n",
    "* **rescaling** $V^\\top  y$ by multipying it by $\\Sigma$\n",
    "* **rotating** $\\Sigma V^\\top  y$ by multiplying it by $U$\n",
    "\n",
    "This structure of the $m \\times n$ matrix  $X$ opens the door to constructing systems\n",
    "of data **encoders** and **decoders**.  \n",
    "\n",
    "Thus, \n",
    "\n",
    "* $V^\\top  y$ is an encoder\n",
    "* $\\Sigma$ is an operator to be applied to the encoded data\n",
    "* $U$ is a decoder to be applied to the output from applying operator $\\Sigma$ to the encoded data\n",
    "\n",
    "We'll apply this circle of ideas  later in this lecture when we study Dynamic Mode Decomposition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Road Ahead**\n",
    "\n",
    "What we have described above  is called a **full** SVD.\n",
    "\n",
    "\n",
    "\n",
    "In a **full** SVD, the  shapes of $U$, $\\Sigma$, and $V$ are $\\left(m, m\\right)$, $\\left(m, n\\right)$, $\\left(n, n\\right)$, respectively. \n",
    "\n",
    "Later we'll also describe an **economy** or **reduced** SVD.\n",
    "\n",
    "Before we study a **reduced** SVD we'll say a little more about properties of a **full** SVD.\n",
    "\n",
    "\n",
    "## Four Fundamental Subspaces\n",
    "\n",
    "\n",
    "Let  ${\\mathcal C}$ denote a column space, ${\\mathcal N}$ denote a null space, and ${\\mathcal R}$ denote a row space.  \n",
    "\n",
    "\n",
    "Let's start by recalling the four fundamental subspaces of an $m \\times n$\n",
    "matrix $X$ of rank $p$.\n",
    "\n",
    "* The **column space** of $X$, denoted ${\\mathcal C}(X)$, is the span of the  columns of  $X$, i.e., all vectors $y$ that can be written as linear combinations of columns of $X$. Its dimension is $p$.\n",
    "* The **null space** of $X$, denoted ${\\mathcal N}(X)$ consists of all vectors $y$ that satisfy \n",
    "$X y = 0$. Its dimension is $m-p$.\n",
    "* The **row space** of $X$, denoted ${\\mathcal R}(X)$ is the column space of $X^\\top $. It consists of all\n",
    "vectors $z$ that can be written as  linear combinations of rows of $X$. Its dimension is $p$.\n",
    "* The **left null space** of $X$, denoted ${\\mathcal N}(X^\\top )$, consist of all vectors $z$ such that\n",
    "$X^\\top  z =0$.  Its dimension is $n-p$.  \n",
    "\n",
    "For a  full SVD of a matrix $X$, the matrix $U$ of left singular vectors  and the matrix $V$ of right singular vectors contain orthogonal bases for all four subspaces.\n",
    "\n",
    "They form two pairs of orthogonal subspaces\n",
    "that we'll describe now.\n",
    "\n",
    "Let $u_i, i = 1, \\ldots, m$ be the $m$ column vectors of $U$ and let\n",
    "$v_i, i = 1, \\ldots, n$ be the $n$ column vectors of $V$.  \n",
    "\n",
    "Let's write the full SVD of X as\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} U_L & U_R \\end{bmatrix} \\begin{bmatrix} \\Sigma_p & 0 \\cr 0 & 0 \\end{bmatrix}\n",
    "     \\begin{bmatrix} V_L & V_R \\end{bmatrix}^\\top \n",
    "$$ (eq:fullSVDpartition)\n",
    "\n",
    "where  $ \\Sigma_p$ is  a $p \\times p$ diagonal matrix with the $p$ singular values on the diagonal and \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "U_L & = \\begin{bmatrix}u_1 & \\cdots  & u_p \\end{bmatrix},  \\quad U_R  = \\begin{bmatrix}u_{p+1} & \\cdots u_m \\end{bmatrix}  \\cr\n",
    "V_L & = \\begin{bmatrix}v_1 & \\cdots  & v_p \\end{bmatrix} , \\quad U_R  = \\begin{bmatrix}v_{p+1} & \\cdots u_n \\end{bmatrix} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Representation {eq}`eq:fullSVDpartition` implies that\n",
    "\n",
    "$$\n",
    "X \\begin{bmatrix} V_L & V_R \\end{bmatrix} = \\begin{bmatrix} U_L & U_R \\end{bmatrix} \\begin{bmatrix} \\Sigma_p & 0 \\cr 0 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X V_L & = U_L \\Sigma_p \\cr \n",
    "X V_R & = 0 \n",
    "\\end{aligned}\n",
    "$$ (eq:Xfour1a)\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X v_i & = \\sigma_i u_i , \\quad i = 1, \\ldots, p \\cr\n",
    "X v_i & = 0 ,  \\quad i = p+1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$ (eq:orthoortho1)\n",
    "\n",
    "Equations {eq}`eq:orthoortho1` tell how the transformation $X$ maps a pair of orthonormal  vectors $v_i, v_j$ for $i$ and $j$ both less than or equal to the rank $p$ of $X$ into a pair of orthonormal vectors $u_i, u_j$.\n",
    "\n",
    "Equations {eq}`eq:Xfour1a` assert that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\mathcal C}(X) & = {\\mathcal C}(U_L) \\cr\n",
    "{\\mathcal N}(X) & = {\\mathcal C} (V_R)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Taking transposes on both sides of representation {eq}`eq:fullSVDpartition` implies \n",
    "\n",
    "\n",
    "$$\n",
    "X^\\top  \\begin{bmatrix} U_L & U_R \\end{bmatrix} = \\begin{bmatrix} V_L & V_R \\end{bmatrix} \\begin{bmatrix} \\Sigma_p & 0 \\cr 0 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^\\top  U_L & = V_L \\Sigma_p \\cr\n",
    "X^\\top  U_R & = 0 \n",
    "\\end{aligned}\n",
    "$$  (eq:Xfour1b)\n",
    "\n",
    "or \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "X^\\top  u_i & = \\sigma_i v_i, \\quad i=1, \\ldots, p \\cr\n",
    "X^\\top  u_i & = 0 \\quad i= p+1, \\ldots, m \n",
    "\\end{aligned}\n",
    "$$ (eq:orthoortho2)\n",
    "\n",
    "Notice how equations {eq}`eq:orthoortho2` assert that  the transformation $X^\\top $ maps a pairsof distinct orthonormal  vectors $u_i, u_j$  for $i$ and $j$ both less than or equal to the rank $p$ of $X$ into a pair of distinct orthonormal vectors $v_i, v_j$ .\n",
    "\n",
    "\n",
    "Equations {eq}`eq:Xfour1b` assert that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\mathcal R}(X) & \\equiv  {\\mathcal C}(X^\\top ) = {\\mathcal C} (V_L) \\cr\n",
    "{\\mathcal N}(X^\\top ) & = {\\mathcal C}(U_R) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Thus, taken together, the systems of quations {eq}`eq:Xfour1a` and {eq}`eq:Xfour1b`\n",
    "describe the  four fundamental subspaces of $X$ in the following ways:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\mathcal C}(X) & = {\\mathcal C}(U_L) \\cr \n",
    "{\\mathcal N}(X^\\top ) & = {\\mathcal C}(U_R) \\cr\n",
    "{\\mathcal R}(X) & \\equiv  {\\mathcal C}(X^\\top ) = {\\mathcal C} (V_L) \\cr\n",
    "{\\mathcal N}(X) & = {\\mathcal C} (V_R) \\cr\n",
    "\n",
    "\\end{aligned}\n",
    "$$ (eq:fourspaceSVD)\n",
    "\n",
    "\n",
    "\n",
    "Since $U$ and $V$ are both orthonormal matrices, collection {eq}`eq:fourspaceSVD` asserts that\n",
    "\n",
    " * $U_L$ is an orthonormal basis for the column space of $X$\n",
    " * $U_R$ is an orthonormal basis for the null space of $X^\\top $\n",
    " * $V_L$ is an orthonormal basis for the row space of $X$\n",
    " * $V_R$ is an orthonormal basis for the null space of $X$\n",
    " \n",
    "\n",
    "We have verified the four claims in {eq}`eq:fourspaceSVD` simply  by performing the multiplications called for by the right side of {eq}`eq:fullSVDpartition` and reading them. \n",
    "\n",
    "The claims in {eq}`eq:fourspaceSVD` and the fact that $U$ and $V$ are both unitary (i.e, orthonormal) matrices  imply\n",
    "that\n",
    "\n",
    "* the column space of $X$ is orthogonal to the null space of of $X^\\top $\n",
    "* the null space of $X$ is orthogonal to the row space of $X$\n",
    "\n",
    "Sometimes these properties are described with the following two pairs of orthogonal complement subspaces:\n",
    "\n",
    "* ${\\mathcal C}(X)$ is the orthogonal complement of $ {\\mathcal N}(X^\\top )$ \n",
    "* ${\\mathcal R}(X)$ is the orthogonal complement  ${\\mathcal N}(X)$  \n",
    "\n",
    "\n",
    "\n",
    "Let's do an example.\n",
    "\n",
    "In addition to regular packages contained in Anaconda by default, this example and some other parts of  lecture also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09e881a",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting quandl\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading Quandl-3.7.0-py2.py3-none-any.whl (26 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflection>=0.3.1 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (0.5.1)\r\n",
      "Requirement already satisfied: numpy>=1.8 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (1.23.5)\r\n",
      "Requirement already satisfied: pandas>=0.14 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (1.4.4)\r\n",
      "Requirement already satisfied: six in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (1.16.0)\r\n",
      "Collecting more-itertools\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading more_itertools-9.0.0-py3-none-any.whl (52 kB)\r\n",
      "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/52.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (2.28.1)\r\n",
      "Requirement already satisfied: python-dateutil in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from pandas>=0.14->quandl) (2022.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests>=2.7.0->quandl) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests>=2.7.0->quandl) (1.26.11)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests>=2.7.0->quandl) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests>=2.7.0->quandl) (2022.9.14)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: more-itertools, quandl\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed more-itertools-9.0.0 quandl-3.7.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2782c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5143ec2",
   "metadata": {},
   "source": [
    "Having imported these modules, let's do the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8398e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of matrix:\n",
      " 2\n",
      "S: \n",
      " [2.69e+01 1.86e+00 7.83e-16 3.27e-16 4.69e-17]\n",
      "U:\n",
      " [[-0.27 -0.73 -0.47  0.06 -0.42]\n",
      " [-0.35 -0.42  0.1  -0.18  0.81]\n",
      " [-0.43 -0.11  0.75 -0.27 -0.4 ]\n",
      " [-0.51  0.19  0.06  0.83  0.05]\n",
      " [-0.59  0.5  -0.45 -0.45 -0.04]]\n",
      "Column space:\n",
      " [[-0.27 -0.35]\n",
      " [ 0.73  0.42]\n",
      " [ 0.02  0.06]\n",
      " [ 0.52 -0.83]\n",
      " [-0.37 -0.08]]\n",
      "Left null space:\n",
      " [[-0.47  0.06 -0.42]\n",
      " [ 0.1  -0.18  0.81]\n",
      " [ 0.75 -0.27 -0.4 ]\n",
      " [ 0.06  0.83  0.05]\n",
      " [-0.45 -0.45 -0.04]]\n",
      "V.T:\n",
      " [[-0.27  0.73  0.02  0.52 -0.37]\n",
      " [-0.35  0.42  0.06 -0.83 -0.08]\n",
      " [-0.43  0.11  0.29  0.18  0.82]\n",
      " [-0.51 -0.19 -0.83  0.06  0.04]\n",
      " [-0.59 -0.5   0.46  0.07 -0.42]]\n",
      "Row space:\n",
      " [[-0.27 -0.35 -0.43 -0.51 -0.59]\n",
      " [-0.73 -0.42 -0.11  0.19  0.5 ]]\n",
      "Right null space:\n",
      " [[-0.43  0.11  0.29  0.18  0.82]\n",
      " [-0.51 -0.19 -0.83  0.06  0.04]\n",
      " [-0.59 -0.5   0.46  0.07 -0.42]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Define the matrix\n",
    "A = np.array([[1, 2, 3, 4, 5], \n",
    "              [2, 3, 4, 5, 6], \n",
    "              [3, 4, 5, 6, 7],\n",
    "              [4, 5, 6, 7, 8],\n",
    "              [5, 6, 7, 8, 9]])\n",
    "\n",
    "# Compute the SVD of the matrix\n",
    "U, S, V = np.linalg.svd(A,full_matrices=True)\n",
    "\n",
    "# Compute the rank of the matrix\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "\n",
    "# Print the rank of the matrix\n",
    "print(\"Rank of matrix:\\n\", rank)\n",
    "print(\"S: \\n\", S)\n",
    "\n",
    "# Compute the four fundamental subspaces\n",
    "row_space = U[:, :rank]\n",
    "col_space = V[:, :rank]\n",
    "null_space = V[:, rank:]\n",
    "left_null_space = U[:, rank:]\n",
    "\n",
    "\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Column space:\\n\", col_space)\n",
    "print(\"Left null space:\\n\", left_null_space)\n",
    "print(\"V.T:\\n\", V.T)\n",
    "print(\"Row space:\\n\", row_space.T)\n",
    "print(\"Right null space:\\n\", null_space.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434b5a6",
   "metadata": {},
   "source": [
    "## Eckart-Young Theorem\n",
    "\n",
    "Suppose that we want to construct  the best rank $r$ approximation of an $m \\times n$ matrix $X$.\n",
    "\n",
    "By best we mean a  matrix $X_r$ of rank $r < p$ that, among all rank $r$ matrices, minimizes\n",
    "\n",
    "$$ || X - X_r || $$\n",
    "\n",
    "where $ || \\cdot || $ denotes a norm of a matrix $X$ and where $X_r$ belongs to the space of all rank $r$ matrices\n",
    "of dimension $m \\times n$.\n",
    "\n",
    "\n",
    "\n",
    "Three popular **matrix norms**  of an $m \\times n$ matrix $X$ can be expressed in terms of the singular values of $X$\n",
    "\n",
    "* the **spectral** or $l^2$ norm $|| X ||_2 = \\max_{y \\in \\textbf{R}^n} \\frac{||X y ||}{||y||} = \\sigma_1$\n",
    "* the **Frobenius** norm $||X ||_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_p^2}$\n",
    "* the **nuclear** norm $ || X ||_N = \\sigma_1 + \\cdots + \\sigma_p $\n",
    "\n",
    "The Eckart-Young theorem states that for each of these three norms, same rank $r$ matrix is best and that it equals \n",
    "\n",
    "$$\n",
    "\\hat X_r = \\sigma_1 U_1 V_1^\\top  + \\sigma_2 U_2 V_2^\\top  + \\cdots + \\sigma_r U_r V_r^\\top \n",
    "$$ (eq:Ekart)\n",
    "\n",
    "\n",
    "You can read about the Eckart-Young theorem and some of its uses here <https://en.wikipedia.org/wiki/Low-rank_approximation>.\n",
    "\n",
    "We'll make use of this theorem when we discuss principal components analysis (PCA) and also dynamic mode decomposition (DMD).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Full and Reduced SVD's\n",
    "\n",
    "Up to now we have described properties of a **full** SVD in which shapes of $U$, $\\Sigma$, and $V$ are $\\left(m, m\\right)$, $\\left(m, n\\right)$, $\\left(n, n\\right)$, respectively. \n",
    "\n",
    "There is  an alternative bookkeeping convention called an **economy** or **reduced** SVD in which the shapes of $U, \\Sigma$ and $V$ are different from what they are in a full SVD.\n",
    "\n",
    "Thus, note that because we assume that $X$ has rank $p$, there are only $p$ nonzero singular values, where $p=\\textrm{rank}(X)\\leq\\min\\left(m, n\\right)$.  \n",
    "\n",
    "A **reduced** SVD uses this fact to express $U$, $\\Sigma$, and $V$ as matrices with shapes $\\left(m, p\\right)$, $\\left(p, p\\right)$, $\\left( n, p\\right)$.\n",
    "\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    "<https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html>\n",
    "\n",
    "For a full SVD, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^\\top  &  = I  &  \\quad U^\\top  U = I \\cr    \n",
    "VV^\\top  & = I & \\quad V^\\top  V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But not all these properties hold for a  **reduced** SVD.\n",
    "\n",
    "Which properties hold depend on whether we are in a **tall-skinny** case or a **short-fat** case.\n",
    "\n",
    " * In a **tall-skinny** case in which $m > > n$, for a **reduced** SVD\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^\\top  &  \\neq I  &  \\quad U^\\top  U = I \\cr    \n",
    "VV^\\top  & = I & \\quad V^\\top  V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " * In a **short-fat** case in which $m < < n$, for a **reduced** SVD\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^\\top  &  = I  &  \\quad U^\\top  U = I \\cr    \n",
    "VV^\\top  & = I & \\quad V^\\top  V \\neq I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When we study Dynamic Mode Decomposition below, we shall want to remember these properties when we use a  reduced SVD to compute some DMD representations.\n",
    "\n",
    "\n",
    "Let's do an  exercise  to compare **full** and **reduced** SVD's.\n",
    "\n",
    "To review, \n",
    "\n",
    "\n",
    " * in a **full** SVD\n",
    "\n",
    "    -  $U$ is $m \\times m$\n",
    "    -  $\\Sigma$ is $m \\times n$\n",
    "    -  $V$ is $n \\times n$\n",
    "\n",
    " * in a **reduced** SVD\n",
    "\n",
    "    -  $U$ is $m \\times p$\n",
    "    - $\\Sigma$ is $p\\times p$\n",
    "    -  $V$ is $n \\times p$ \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "First, let's study a case in which $m = 5 > n = 2$.\n",
    "\n",
    "(This is a small example of the **tall-skinny** case that will concern us when we study **Dynamic Mode Decompositions** below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20101be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U, S, V =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.29,  0.85,  0.44,  0.02,  0.05],\n",
       "        [-0.53,  0.12, -0.52, -0.46, -0.47],\n",
       "        [-0.38, -0.48,  0.71, -0.24, -0.25],\n",
       "        [-0.49, -0.12, -0.12,  0.84, -0.16],\n",
       "        [-0.5 , -0.15, -0.13, -0.17,  0.83]]),\n",
       " array([2.38, 0.76]),\n",
       " array([[-0.66, -0.75],\n",
       "        [ 0.75, -0.66]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V =')\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f6260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uhat, Shat, Vhat = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.29,  0.85],\n",
       "        [-0.53,  0.12],\n",
       "        [-0.38, -0.48],\n",
       "        [-0.49, -0.12],\n",
       "        [-0.5 , -0.15]]),\n",
       " array([2.38, 0.76]),\n",
       " array([[-0.66, -0.75],\n",
       "        [ 0.75, -0.66]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Uhat, Shat, Vhat = ')\n",
    "Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387ad9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank of X = 2\n"
     ]
    }
   ],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "print(f'rank of X = {rr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96858919",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "\n",
    "* Where $U$ is constructed via a full SVD, $U^\\top  U = I_{p\\times p}$ and  $U U^\\top  = I_{m \\times m}$ \n",
    "* Where $\\hat U$ is constructed via a reduced SVD, although $\\hat U^\\top  \\hat U = I_{p\\times p}$ it happens that  $\\hat U \\hat U^\\top  \\neq I_{m \\times m}$ \n",
    "\n",
    "We illustrate these properties for our example with the following code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cad2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUT, UTU = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.00e+00,  1.54e-16,  2.07e-16, -7.94e-17, -5.64e-17],\n",
       "        [ 1.54e-16,  1.00e+00, -1.20e-16, -8.15e-17, -7.45e-17],\n",
       "        [ 2.07e-16, -1.20e-16,  1.00e+00, -6.46e-17, -9.65e-17],\n",
       "        [-7.94e-17, -8.15e-17, -6.46e-17,  1.00e+00, -7.24e-17],\n",
       "        [-5.64e-17, -7.45e-17, -9.65e-17, -7.24e-17,  1.00e+00]]),\n",
       " array([[ 1.00e+00,  6.37e-17,  3.34e-17,  2.88e-18,  1.01e-17],\n",
       "        [ 6.37e-17,  1.00e+00, -3.52e-16, -1.93e-16, -1.93e-16],\n",
       "        [ 3.34e-17, -3.52e-16,  1.00e+00, -7.29e-17, -1.12e-16],\n",
       "        [ 2.88e-18, -1.93e-16, -7.29e-17,  1.00e+00, -9.11e-17],\n",
       "        [ 1.01e-17, -1.93e-16, -1.12e-16, -9.11e-17,  1.00e+00]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTU = U.T@U\n",
    "UUT = U@U.T\n",
    "print('UUT, UTU = ')\n",
    "UUT, UTU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a1b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UhatUhatT, UhatTUhat= \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.8 ,  0.26, -0.3 ,  0.05,  0.02],\n",
       "        [ 0.26,  0.29,  0.14,  0.25,  0.24],\n",
       "        [-0.3 ,  0.14,  0.38,  0.24,  0.26],\n",
       "        [ 0.05,  0.25,  0.24,  0.26,  0.26],\n",
       "        [ 0.02,  0.24,  0.26,  0.26,  0.27]]),\n",
       " array([[1.00e+00, 6.37e-17],\n",
       "        [6.37e-17, 1.00e+00]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UhatUhatT = Uhat@Uhat.T\n",
    "UhatTUhat = Uhat.T@Uhat\n",
    "print('UhatUhatT, UhatTUhat= ')\n",
    "UhatUhatT, UhatTUhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4595f",
   "metadata": {},
   "source": [
    "**Remarks:** \n",
    "\n",
    "The cells above illustrate application of the  `fullmatrices=True` and `full-matrices=False` options.\n",
    "Using `full-matrices=False` returns a reduced singular value decomposition.\n",
    "\n",
    "The **full** and **reduced** SVd's both accurately  decompose an $m \\times n$ matrix $X$ \n",
    "\n",
    "When we study Dynamic Mode Decompositions below, it  will be important for us to remember the preceding properties of full and reduced SVD's in such tall-skinny cases.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now let's turn to a short-fat case.\n",
    "\n",
    "To illustrate this case,  we'll set $m = 2 < 5 = n $ and compute both full and reduced SVD's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0bd1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U, S, V = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.42, -0.91],\n",
       "        [-0.91,  0.42]]),\n",
       " array([1.93, 0.66]),\n",
       " array([[-0.53, -0.59, -0.29, -0.5 , -0.2 ],\n",
       "        [ 0.24, -0.29,  0.13,  0.35, -0.85],\n",
       "        [-0.38,  0.03,  0.91, -0.15, -0.04],\n",
       "        [-0.7 ,  0.15, -0.18,  0.67, -0.  ],\n",
       "        [ 0.16, -0.74,  0.18,  0.39,  0.49]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(2,5)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V = ')\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7728e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uhat, Shat, Vhat = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.42, -0.91],\n",
       "        [-0.91,  0.42]]),\n",
       " array([1.93, 0.66]),\n",
       " array([[-0.53, -0.59, -0.29, -0.5 , -0.2 ],\n",
       "        [ 0.24, -0.29,  0.13,  0.35, -0.85]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Uhat, Shat, Vhat = ')\n",
    "Uhat, Shat, Vhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c769717",
   "metadata": {},
   "source": [
    "Let's verify that our reduced SVD accurately represents $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510f05f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SShat=np.diag(Shat)\n",
    "np.allclose(X, Uhat@SShat@Vhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954f943",
   "metadata": {},
   "source": [
    "## Polar Decomposition\n",
    "\n",
    "A **reduced** singular value decomposition (SVD) of $X$ is related to a **polar decomposition** of $X$\n",
    "\n",
    "$$\n",
    "X  = SQ   \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " S & = U\\Sigma U^\\top  \\cr\n",
    "Q & = U V^\\top  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here \n",
    "\n",
    "* $S$ is  an $m \\times m$  **symmetric** matrix \n",
    "* $Q$ is an $m \\times n$  **orthogonal** matrix\n",
    "\n",
    "and in our reduced SVD\n",
    "\n",
    "* $U$ is an $m \\times p$ orthonormal matrix\n",
    "* $\\Sigma$ is a $p \\times p$ diagonal matrix\n",
    "* $V$ is an $n \\times p$ orthonormal \n",
    "\n",
    "## Application: Principal Components Analysis (PCA)\n",
    "\n",
    "Let's begin with a case in which $n >> m$, so that we have many  more individuals $n$ than attributes $m$.\n",
    "\n",
    "The  matrix $X$ is **short and fat**  in an  $n >> m$ case as opposed to a **tall and skinny** case with $m > > n $ to be discussed later.\n",
    "\n",
    "We regard  $X$ as an  $m \\times n$ matrix of **data**:\n",
    "\n",
    "$$\n",
    "X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where for $j = 1, \\ldots, n$ the column vector $X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix}$ is a  vector of observations on variables $\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix}$.\n",
    "\n",
    "In a **time series** setting, we would think of columns $j$ as indexing different __times__ at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "In a **cross section** setting, we would think of columns $j$ as indexing different __individuals__ for  which random variables are observed, while rows index different **attributes**.\n",
    "\n",
    "The number of positive singular values equals the rank of  matrix $X$.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $\\Sigma$ of into a vector $\\sigma_R$.\n",
    "\n",
    "Set all other entries of $\\Sigma$ to zero.\n",
    "\n",
    "## Relationship of PCA to SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $X$, first construct  the  SVD of the data matrix $X$:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^\\top  = \\sigma_1 U_1 V_1^\\top  + \\sigma_2 U_2 V_2^\\top  + \\cdots + \\sigma_p U_p V_p^\\top \n",
    "$$ (eq:PCA1)\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^\\top  = \\begin{bmatrix}V_1^\\top \\\\V_2^\\top \\\\\\ldots\\\\V_n^\\top \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In equation {eq}`eq:PCA1`, each of the $m \\times n$ matrices $U_{j}V_{j}^\\top $ is evidently\n",
    "of rank $1$. \n",
    "\n",
    "Thus, we have \n",
    "\n",
    "$$\n",
    "X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^\\top \\\\U_{21}V_{1}^\\top \\\\\\cdots\\\\U_{m1}V_{1}^\\top \\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^\\top \\\\U_{22}V_{2}^\\top \\\\\\cdots\\\\U_{m2}V_{2}^\\top \\\\\\end{pmatrix}+\\ldots + \\sigma_p\\begin{pmatrix}U_{1p}V_{p}^\\top \\\\U_{2p}V_{p}^\\top \\\\\\cdots\\\\U_{mp}V_{p}^\\top \\\\\\end{pmatrix}\n",
    "$$ (eq:PCA2)\n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation {eq}`eq:PCA2` in \n",
    "a time series context:\n",
    "\n",
    "* $  \\textrm{for each} \\   k=1, \\ldots, n $, the object $\\lbrace V_{kj} \\rbrace_{j=1}^n$ is a time series   for the $k$th **principal component**\n",
    "\n",
    "* $U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m$\n",
    "is a vector of **loadings** of variables $X_i$ on the $k$th principal component,  $i=1, \\ldots, m$\n",
    "\n",
    "* $\\sigma_k $ for each $k=1, \\ldots, p$ is the strength of $k$th **principal component**, where strength means contribution to the overall covariance of $X$.\n",
    "\n",
    "## PCA with Eigenvalues and Eigenvectors\n",
    "\n",
    "We now  use an eigen decomposition of a sample covariance matrix to do PCA.\n",
    "\n",
    "Let $X_{m \\times n}$ be our $m \\times n$ data matrix.\n",
    "\n",
    "Let's assume that sample means of all variables are zero.\n",
    "\n",
    "We can assure  this  by **pre-processing** the data by subtracting sample means.\n",
    "\n",
    "Define a sample covariance matrix $\\Omega$ as \n",
    "\n",
    "$$\n",
    "\\Omega = XX^\\top \n",
    "$$\n",
    "\n",
    "Then use an eigen decomposition to represent $\\Omega$ as follows:\n",
    "\n",
    "$$\n",
    "\\Omega =P\\Lambda P^\\top \n",
    "$$\n",
    "\n",
    "Here \n",
    "\n",
    "* $P$ is $m√óm$ matrix of eigenvectors of $\\Omega$\n",
    "\n",
    "* $\\Lambda$ is a diagonal matrix of eigenvalues of $\\Omega$\n",
    "\n",
    "We can then represent $X$ as\n",
    "\n",
    "$$\n",
    "X=P\\epsilon\n",
    "$$          \n",
    "\n",
    "where \n",
    "\n",
    "$$ \n",
    "\\epsilon = P^{-1} X\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\epsilon\\epsilon^\\top =\\Lambda .\n",
    "$$ \n",
    "\n",
    "We can verify that\n",
    "\n",
    "$$\n",
    "XX^\\top =P\\Lambda P^\\top  .\n",
    "$$ (eq:XXo)\n",
    "\n",
    "It follows that we can represent the data matrix $X$  as \n",
    "\n",
    "\\begin{equation*}\n",
    "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we had obtained earlier through the SVD, we first note that $\\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j$.\n",
    "\n",
    "Now define  $\\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}}$, \n",
    "which  implies that $\\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^\\top =1$.\n",
    "\n",
    "Therefore \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m} ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which  agrees with \n",
    "\n",
    "$$\n",
    "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}\n",
    "$$\n",
    "\n",
    "provided that  we set \n",
    "\n",
    "* $U_j=P_j$ (a vector of  loadings of variables on principal component $j$) \n",
    "\n",
    "* ${V_k}^{T}=\\tilde{\\epsilon_k}$ (the $k$th principal component)\n",
    "\n",
    "Because  there are alternative algorithms for  computing  $P$ and $U$ for  given a data matrix $X$, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We can resolve such ambiguities about  $U$ and $P$ by\n",
    "\n",
    "1. sorting eigenvalues and singular values in descending order\n",
    "2. imposing positive diagonals on $P$ and $U$ and adjusting signs in $V^\\top $ accordingly\n",
    "\n",
    "## Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider an  SVD of an $m \\times n$ matrix:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^\\top \n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "XX^\\top &=U\\Sigma V^\\top V\\Sigma^\\top  U^\\top \\cr\n",
    "&\\equiv U\\Sigma\\Sigma^\\top U^\\top \\cr\n",
    "&\\equiv U\\Lambda U^\\top \n",
    "\\end{aligned}\n",
    "$$  (eq:XXcompare)\n",
    "\n",
    "Compare representation {eq}`eq:XXcompare` with equation {eq}`eq:XXo` above.\n",
    "\n",
    "Evidently, $U$ in the SVD is the matrix $P$  of\n",
    "eigenvectors of $XX^\\top $ and $\\Sigma \\Sigma^\\top $ is the matrix $\\Lambda$ of eigenvalues.\n",
    "\n",
    "Second, let's compute\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^\\top X &=V\\Sigma^\\top  U^\\top U\\Sigma V^\\top \\\\\n",
    "&=V\\Sigma^\\top {\\Sigma}V^\\top \n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "\n",
    "\n",
    "Thus, the matrix $V$ in the SVD is the matrix of eigenvectors of $X^\\top X$\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$\n",
    "X X^\\top  = P \\Lambda P^\\top \n",
    "$$\n",
    "\n",
    "where $P$ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $X$, we know that\n",
    "\n",
    "$$\n",
    "X X^\\top  = U \\Sigma \\Sigma^\\top  U^\\top \n",
    "$$\n",
    "\n",
    "where $U$ is an orthonal matrix.  \n",
    "\n",
    "Thus, $P = U$ and we have the representation of $X$\n",
    "\n",
    "$$\n",
    "X = P \\epsilon = U \\Sigma V^\\top \n",
    "$$\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$\n",
    "U^\\top  X = \\Sigma V^\\top  = \\epsilon\n",
    "$$\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$\n",
    "\\epsilon \\epsilon^\\top  = \\Sigma V^\\top  V \\Sigma^\\top  = \\Sigma \\Sigma^\\top  = \\Lambda ,\n",
    "$$\n",
    "\n",
    "so that everything fits together.\n",
    "\n",
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd634f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Œ© = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        ùúÜ, P = LA.eigh(self.Œ©)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(ùúÜ.size), key=lambda x: ùúÜ[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.ùúÜ = ùúÜ[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Œõ = np.diag(self.ùúÜ)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.ùúÜ) / self.ùúÜ.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.ùúñ = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        ùúñ = self.ùúñ[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ùúñ\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, ùúé, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(ùúé.size), key=lambda x: ùúé[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.ùúé = ùúé[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Œ£ = np.zeros((self.m, self.n))\n",
    "        self.Œ£[:d, :d] = np.diag(self.ùúé)\n",
    "\n",
    "        ùúé_sq = self.ùúé ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(ùúé_sq) / ùúé_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Œ£ = self.Œ£[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Œ£ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        ùúñ = self.ùúñ[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ùúñ\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Œ£ = self.Œ£[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Œ£ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17702021",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1754d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'Œª = {da.Œª}\\n')\n",
    "    print(f'œÉ^2 = {da.œÉ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.Œµ.T)\n",
    "    axs[0].set_title('Œµ')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.Œª))\n",
    "    axs[1].set_title('$V^\\top *\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ca000",
   "metadata": {},
   "source": [
    "For an example  PCA applied to analyzing the structure of intelligence tests see this lecture {doc}`Multivariable Normal Distribution <multivariate_normal>`.\n",
    "\n",
    "Look at  parts of that lecture that describe and illustrate the classic factor analysis model.\n",
    "\n",
    "As mentioned earlier, in a sequel to this lecture about  {doc}`Dynamic Mode Decompositions <var_dmd>`, we'll describe how SVD's provide ways rapidly to compute reduced-order approximations to first-order Vector Autoregressions (VARs)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "source_map": [
   12,
   295,
   300,
   307,
   311,
   344,
   454,
   463,
   468,
   471,
   481,
   489,
   494,
   516,
   525,
   528,
   531,
   534,
   798,
   895,
   900,
   935
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}