

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>21. Statistical Divergence Measures &#8212; Intermediate Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/quantecon-book-theme.css?digest=bd0785fbb14d8d2bd4d9ae501d79ed8d3bc089ec" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>


    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/scripts/quantecon-book-theme.js?digest=d6d86bce9979111653c4c495e33499e1796e172a"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-J0SMYR4SG3"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-J0SMYR4SG3');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'divergence_measures';</script>
    <link rel="canonical" href="https://python.quantecon.org/divergence_measures.html" />
    <link rel="shortcut icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="22. Likelihood Ratio Processes" href="likelihood_ratio_process.html" />
    <link rel="prev" title="20. Forecasting an AR(1) Process" href="ar1_turningpts.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Statistical Divergence Measures"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Statistical Divergence Measures" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python.quantecon.org/divergence_measures.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Intermediate Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=divergence_measures>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">21.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#primer-on-entropy-cross-entropy-kl-divergence">21.2. Primer on entropy, cross-entropy, KL divergence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-beta-distributions-running-example">21.3. Two Beta distributions: running example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kullbackleibler-divergence">21.4. Kullback–Leibler divergence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jensen-shannon-divergence">21.5. Jensen-Shannon divergence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chernoff-entropy">21.6. Chernoff entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-divergence-measures">21.7. Comparing divergence measures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kl-divergence-and-maximum-likelihood-estimation">21.8. KL divergence and maximum-likelihood estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#related-lectures">21.9. Related lectures</a></li>
</ul>
                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo logo-img" alt="logo"></a>
                                    
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Intermediate Quantitative Economics with Python</a></p>

                        <p class="qe-page__header-subheading">Statistical Divergence Measures</p>

                    </div>
                    <!-- length 2, since its a string and empty dict has length 2 - {} -->
                        <p class="qe-page__header-authors" font-size="18">
                            
                                
                                    <a href="http://www.tomsargent.com/" target="_blank"><span>Thomas J. Sargent</span></a>
                                
                            
                                
                                    and <a href="https://johnstachurski.net/" target="_blank"><span>John Stachurski</span></a>
                                
                            
                        </p>


                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <section class="tex2jax_ignore mathjax_ignore" id="statistical-divergence-measures">
<h1><a class="toc-backref" href="#id3"><span class="section-number">21. </span>Statistical Divergence Measures</a><a class="headerlink" href="#statistical-divergence-measures" title="Permalink to this heading">#</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#statistical-divergence-measures" id="id3">Statistical Divergence Measures</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id4">Overview</a></p></li>
<li><p><a class="reference internal" href="#primer-on-entropy-cross-entropy-kl-divergence" id="id5">Primer on entropy, cross-entropy, KL divergence</a></p></li>
<li><p><a class="reference internal" href="#two-beta-distributions-running-example" id="id6">Two Beta distributions: running example</a></p></li>
<li><p><a class="reference internal" href="#kullbackleibler-divergence" id="id7">Kullback–Leibler divergence</a></p></li>
<li><p><a class="reference internal" href="#jensen-shannon-divergence" id="id8">Jensen-Shannon divergence</a></p></li>
<li><p><a class="reference internal" href="#chernoff-entropy" id="id9">Chernoff entropy</a></p></li>
<li><p><a class="reference internal" href="#comparing-divergence-measures" id="id10">Comparing divergence measures</a></p></li>
<li><p><a class="reference internal" href="#kl-divergence-and-maximum-likelihood-estimation" id="id11">KL divergence and maximum-likelihood estimation</a></p></li>
<li><p><a class="reference internal" href="#related-lectures" id="id12">Related lectures</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="overview">
<h2><a class="toc-backref" href="#id4"><span class="section-number">21.1. </span>Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h2>
<p>A statistical divergence quantifies discrepancies between two distinct
probability distributions that can be   challenging to distinguish for the following reason:</p>
<ul class="simple">
<li><p>every event that has positive probability  under one of the distributions also has positive probability under the other distribution</p></li>
<li><p>this means that  there is no “smoking gun” event whose occurrence  tells  a statistician that one of the probability distributions surely governs the data</p></li>
</ul>
<p>A statistical divergence is a <strong>function</strong> that maps two  probability distributions into a nonnegative real number.</p>
<p>Statistical divergence functions  play important roles in statistics, information theory, and what many people now call “machine learning”.</p>
<p>This lecture describes  three divergence measures:</p>
<ul class="simple">
<li><p><strong>Kullback–Leibler (KL) divergence</strong></p></li>
<li><p><strong>Jensen–Shannon (JS) divergence</strong></p></li>
<li><p><strong>Chernoff entropy</strong></p></li>
</ul>
<p>These will appear in several quantecon lectures.</p>
<p>Let’s start by importing the necessary Python tools.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">vectorize</span><span class="p">,</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">gamma</span>
<span class="kn">from</span> <span class="nn">scipy.integrate</span> <span class="kn">import</span> <span class="n">quad</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize_scalar</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Math</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="primer-on-entropy-cross-entropy-kl-divergence">
<h2><a class="toc-backref" href="#id5"><span class="section-number">21.2. </span>Primer on entropy, cross-entropy, KL divergence</a><a class="headerlink" href="#primer-on-entropy-cross-entropy-kl-divergence" title="Permalink to this heading">#</a></h2>
<p>Before diving in, we’ll introduce some useful concepts in a simple setting.</p>
<p>We’ll temporarily assume that <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are two probability mass functions for discrete random variables
on state space <span class="math notranslate nohighlight">\(I = \{1, 2, \ldots, n\}\)</span>  that satisfy <span class="math notranslate nohighlight">\(f_i \geq 0, \sum_{i} f_i =1, g_i \geq 0, \sum_{i} g_i =1\)</span>.</p>
<p>We follow some  statisticians and information theorists who  define the <strong>surprise</strong> or <strong>surprisal</strong>
associated with having  observed a single draw <span class="math notranslate nohighlight">\(x = i\)</span> from distribution <span class="math notranslate nohighlight">\(f\)</span>  as</p>
<div class="math notranslate nohighlight">
\[
\log\left(\frac{1}{f_i}\right)
\]</div>
<p>They then define the <strong>information</strong> that you can   anticipate  to gather from observing a single realization
as the expected surprisal</p>
<div class="math notranslate nohighlight">
\[
H(f) = \sum_i f_i \log\left(\frac{1}{f_i}\right).  
\]</div>
<p>Claude Shannon <span id="id1">[<a class="reference internal" href="zreferences.html#id3" title="Claude E Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.">Shannon, 1948</a>]</span> called <span class="math notranslate nohighlight">\(H(f)\)</span> the <strong>entropy</strong> of distribution <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By maximizing <span class="math notranslate nohighlight">\(H(f)\)</span> with respect to <span class="math notranslate nohighlight">\(\{f_1, f_2, \ldots, f_n\}\)</span> subject to <span class="math notranslate nohighlight">\(\sum_i f_i = 1\)</span>, we can verify that the distribution
that maximizes entropy is the uniform distribution
<span class="math notranslate nohighlight">\(
f_i = \frac{1}{n} .
\)</span>
Entropy <span class="math notranslate nohighlight">\(H(f)\)</span> for the uniform distribution evidently equals <span class="math notranslate nohighlight">\(- \log(n)\)</span>.</p>
</div>
<p>Kullback and Leibler <span id="id2">[<a class="reference internal" href="zreferences.html#id4" title="Solomon Kullback and Richard A Leibler. On information and sufficiency. The Annals of Mathematical Statistics, 22(1):79–86, 1951.">Kullback and Leibler, 1951</a>]</span> define the amount of information that a single draw of <span class="math notranslate nohighlight">\(x\)</span> provides for distinguishing <span class="math notranslate nohighlight">\(f\)</span> from <span class="math notranslate nohighlight">\(g\)</span>  as the log likelihood ratio</p>
<div class="math notranslate nohighlight">
\[
\log \frac{f(x)}{g(x)}
\]</div>
<p>The following two  concepts are widely used to compare two distributions <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span>.</p>
<p><strong>Cross-Entropy:</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-2c58179e-85fb-4b8e-bc56-29fd5c713681">
<span class="eqno">(21.1)<a class="headerlink" href="#equation-2c58179e-85fb-4b8e-bc56-29fd5c713681" title="Permalink to this equation">#</a></span>\[\begin{equation}
H(f,g) = -\sum_{i} f_i \log g_i
\end{equation}\]</div>
<p><strong>Kullback-Leibler (KL) Divergence:</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-bbe3f52f-0e7e-46d8-b0bf-03a4e15e5068">
<span class="eqno">(21.2)<a class="headerlink" href="#equation-bbe3f52f-0e7e-46d8-b0bf-03a4e15e5068" title="Permalink to this equation">#</a></span>\[\begin{equation}
D_{KL}(f \parallel g) = \sum_{i} f_i \log\left[\frac{f_i}{g_i}\right]
\end{equation}\]</div>
<p>These concepts are related by the following equality.</p>
<div class="math notranslate nohighlight" id="equation-eq-klcross">
<span class="eqno">(21.3)<a class="headerlink" href="#equation-eq-klcross" title="Permalink to this equation">#</a></span>\[
D_{KL}(f \parallel g) = H(f,g) - H(f)
\]</div>
<p>To prove <a class="reference internal" href="#equation-eq-klcross">(21.3)</a>, note that</p>
<div class="amsmath math notranslate nohighlight" id="equation-fd6f3a47-4fe1-4b99-b05d-332f3807fe46">
<span class="eqno">(21.4)<a class="headerlink" href="#equation-fd6f3a47-4fe1-4b99-b05d-332f3807fe46" title="Permalink to this equation">#</a></span>\[\begin{align}
D_{KL}(f \parallel g) &amp;= \sum_{i} f_i \log\left[\frac{f_i}{g_i}\right] \\
&amp;= \sum_{i} f_i \left[\log f_i - \log g_i\right] \\
&amp;= \sum_{i} f_i \log f_i - \sum_{i} f_i \log g_i \\
&amp;= -H(f) + H(f,g) \\
&amp;= H(f,g) - H(f)
\end{align}\]</div>
<p>Remember that <span class="math notranslate nohighlight">\(H(f)\)</span> is the anticipated surprisal from drawing <span class="math notranslate nohighlight">\(x\)</span> from <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>Then the above equation tells us that  the KL divergence is an anticipated “excess surprise” that comes from anticipating that <span class="math notranslate nohighlight">\(x\)</span> is drawn from <span class="math notranslate nohighlight">\(f\)</span> when it is
actually drawn from <span class="math notranslate nohighlight">\(g\)</span>.</p>
</section>
<section id="two-beta-distributions-running-example">
<h2><a class="toc-backref" href="#id6"><span class="section-number">21.3. </span>Two Beta distributions: running example</a><a class="headerlink" href="#two-beta-distributions-running-example" title="Permalink to this heading">#</a></h2>
<p>We’ll use Beta distributions extensively to illustrate concepts.</p>
<p>The Beta distribution is particularly convenient as it’s defined on <span class="math notranslate nohighlight">\([0,1]\)</span> and exhibits diverse shapes by appropriately choosing its  two parameters.</p>
<p>The density of a Beta distribution with parameters <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
f(z; a, b) = \frac{\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\Gamma(a) \Gamma(b)}
\quad \text{where} \quad
\Gamma(p) := \int_{0}^{\infty} x^{p-1} e^{-x} dx
\]</div>
<p>Let’s define parameters and density functions in Python</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters in the two Beta distributions</span>
<span class="n">F_a</span><span class="p">,</span> <span class="n">F_b</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">G_a</span><span class="p">,</span> <span class="n">G_b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span>

<span class="nd">@vectorize</span>
<span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">r</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># The two density functions</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F_a</span><span class="p">,</span> <span class="n">F_b</span><span class="p">))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">G_a</span><span class="p">,</span> <span class="n">G_b</span><span class="p">))</span>

<span class="c1"># Plot the distributions</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">f_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_range</span><span class="p">]</span>
<span class="n">g_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_range</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">f_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) \sim \text</span><span class="si">{Beta}</span><span class="s1">(1,1)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">g_vals</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$g(x) \sim \text</span><span class="si">{Beta}</span><span class="s1">(3,1.2)$&#39;</span><span class="p">)</span>

<span class="c1"># Fill overlap region</span>
<span class="n">overlap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">f_vals</span><span class="p">,</span> <span class="n">g_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">overlap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;overlap&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f602cca419752251ba96378c6dfebdb0797aa79419fbd8daab54f3c38a033fc1.png" src="_images/f602cca419752251ba96378c6dfebdb0797aa79419fbd8daab54f3c38a033fc1.png" />
</div>
</div>
</section>
<section id="kullbackleibler-divergence">
<span id="rel-entropy"></span><h2><a class="toc-backref" href="#id7"><span class="section-number">21.4. </span>Kullback–Leibler divergence</a><a class="headerlink" href="#kullbackleibler-divergence" title="Permalink to this heading">#</a></h2>
<p>Our  first divergence function is the <strong>Kullback–Leibler (KL) divergence</strong>.</p>
<p>For probability densities (or pmfs) <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> it is defined by</p>
<div class="math notranslate nohighlight">
\[
D_{KL}(f\|g) = KL(f, g) = \int f(x) \log \frac{f(x)}{g(x)} \, dx.
\]</div>
<p>We can interpret <span class="math notranslate nohighlight">\(D_{KL}(f\|g)\)</span> as the expected excess log loss (expected excess surprisal) incurred when we use <span class="math notranslate nohighlight">\(g\)</span> while the data are generated by <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>It has several important properties:</p>
<ul class="simple">
<li><p>Non-negativity (Gibbs’ inequality): <span class="math notranslate nohighlight">\(D_{KL}(f\|g) \ge 0\)</span> with equality if and only if <span class="math notranslate nohighlight">\(f=g\)</span> almost everywhere.</p></li>
<li><p>Asymmetry: <span class="math notranslate nohighlight">\(D_{KL}(f\|g) \neq D_{KL}(g\|f)\)</span> in general (hence it is not a metric)</p></li>
<li><p>Information decomposition:
<span class="math notranslate nohighlight">\(D_{KL}(f\|g) = H(f,g) - H(f)\)</span>, where <span class="math notranslate nohighlight">\(H(f,g)\)</span> is the cross entropy and <span class="math notranslate nohighlight">\(H(f)\)</span> is the Shannon entropy of <span class="math notranslate nohighlight">\(f\)</span>.</p></li>
<li><p>Chain rule: For joint distributions <span class="math notranslate nohighlight">\(f(x, y)\)</span> and <span class="math notranslate nohighlight">\(g(x, y)\)</span>,
<span class="math notranslate nohighlight">\(D_{KL}(f(x,y)\|g(x,y)) = D_{KL}(f(x)\|g(x)) + E_{f}\left[D_{KL}(f(y|x)\|g(y|x))\right]\)</span></p></li>
</ul>
<p>KL divergence plays a central role in statistical inference, including model selection and hypothesis testing.</p>
<p><a class="reference internal" href="likelihood_ratio_process.html"><span class="doc">Likelihood Ratio Processes</span></a> describes a link between KL divergence and the expected log likelihood ratio,
and the lecture <a class="reference internal" href="wald_friedman.html"><span class="doc">A Problem that Stumped Milton Friedman</span></a> connects it to the test performance of the sequential probability ratio test.</p>
<p>Let’s compute the KL divergence between our example distributions <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_KL</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute KL divergence KL(f, g) via numerical integration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">integrand</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">fw</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">gw</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fw</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fw</span> <span class="o">/</span> <span class="n">gw</span><span class="p">)</span>
    <span class="n">val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="n">integrand</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val</span>

<span class="c1"># Compute KL divergences between our example distributions</span>
<span class="n">kl_fg</span> <span class="o">=</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
<span class="n">kl_gf</span> <span class="o">=</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KL(f, g) = </span><span class="si">{</span><span class="n">kl_fg</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KL(g, f) = </span><span class="si">{</span><span class="n">kl_gf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KL(f, g) = 0.7590
KL(g, f) = 0.3436
</pre></div>
</div>
</div>
</div>
<p>The asymmetry of KL divergence has important practical implications.</p>
<p><span class="math notranslate nohighlight">\(D_{KL}(f\|g)\)</span> penalizes regions where <span class="math notranslate nohighlight">\(f &gt; 0\)</span> but <span class="math notranslate nohighlight">\(g\)</span> is close to zero, reflecting the cost of using <span class="math notranslate nohighlight">\(g\)</span> to model <span class="math notranslate nohighlight">\(f\)</span> and vice versa.</p>
</section>
<section id="jensen-shannon-divergence">
<h2><a class="toc-backref" href="#id8"><span class="section-number">21.5. </span>Jensen-Shannon divergence</a><a class="headerlink" href="#jensen-shannon-divergence" title="Permalink to this heading">#</a></h2>
<p>Sometimes we want a symmetric measure of divergence that captures the difference between two distributions without favoring one over the other.</p>
<p>This often arises in applications like clustering, where we want to compare distributions without assuming one is the true model.</p>
<p>The <strong>Jensen-Shannon (JS) divergence</strong> symmetrizes KL divergence by comparing both distributions to their mixture:</p>
<div class="math notranslate nohighlight">
\[
JS(f,g) = \frac{1}{2} D_{KL}(f\|m) + \frac{1}{2} D_{KL}(g\|m), \quad m = \frac{1}{2}(f+g).
\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is a mixture distribution that averages <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span></p>
<p>Let’s also visualize the mixture distribution <span class="math notranslate nohighlight">\(m\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">m</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">m_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_range</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">f_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">g_vals</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$g(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">m_vals</span><span class="p">,</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$m(x) = \frac</span><span class="si">{1}{2}</span><span class="s1">(f(x) + g(x))$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/96cadc632fddc7643641efc582b7994d3173215524bab21901d63ac2fc691950.png" src="_images/96cadc632fddc7643641efc582b7994d3173215524bab21901d63ac2fc691950.png" />
</div>
</div>
<p>The JS divergence has several useful properties:</p>
<ul class="simple">
<li><p>Symmetry: <span class="math notranslate nohighlight">\(JS(f,g)=JS(g,f)\)</span>.</p></li>
<li><p>Boundedness: <span class="math notranslate nohighlight">\(0 \le JS(f,g) \le \log 2\)</span>.</p></li>
<li><p>Its square root <span class="math notranslate nohighlight">\(\sqrt{JS}\)</span> is a metric (Jensen–Shannon distance) on the space of probability distributions.</p></li>
<li><p>JS divergence equals the mutual information between a binary random variable <span class="math notranslate nohighlight">\(Z \sim \text{Bernoulli}(1/2)\)</span> indicating the source and a sample <span class="math notranslate nohighlight">\(X\)</span> drawn from <span class="math notranslate nohighlight">\(f\)</span> if <span class="math notranslate nohighlight">\(Z=0\)</span> or from <span class="math notranslate nohighlight">\(g\)</span> if <span class="math notranslate nohighlight">\(Z=1\)</span>.</p></li>
</ul>
<p>The Jensen–Shannon divergence plays a key role in the optimization of certain
generative models, as it is bounded, symmetric, and smoother than KL divergence,
often providing more stable gradients for training.</p>
<p>Let’s compute the JS divergence between our example distributions <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_JS</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute Jensen-Shannon divergence.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">m</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">g</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="n">js_div</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">js_div</span>

<span class="n">js_div</span> <span class="o">=</span> <span class="n">compute_JS</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jensen-Shannon divergence JS(f,g) = </span><span class="si">{</span><span class="n">js_div</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jensen-Shannon divergence JS(f,g) = 0.0984
</pre></div>
</div>
</div>
</div>
<p>We can easily generalize to more than two distributions using the generalized Jensen-Shannon divergence with weights <span class="math notranslate nohighlight">\(\alpha = (\alpha_i)_{i=1}^{n}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
JS_\alpha(f_1, \ldots, f_n) = 
H\left(\sum_{i=1}^n \alpha_i f_i\right) - \sum_{i=1}^n \alpha_i H(f_i)
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha_i \geq 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n \alpha_i = 1\)</span>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(H(f) = -\int f(x) \log f(x) dx\)</span> is the <strong>Shannon entropy</strong> of distribution <span class="math notranslate nohighlight">\(f\)</span></p></li>
</ul>
</section>
<section id="chernoff-entropy">
<h2><a class="toc-backref" href="#id9"><span class="section-number">21.6. </span>Chernoff entropy</a><a class="headerlink" href="#chernoff-entropy" title="Permalink to this heading">#</a></h2>
<p>Chernoff entropy originates from early applications of the <a class="reference external" href="https://en.wikipedia.org/wiki/Large_deviations_theory">theory of large deviations</a>, which refines central limit approximations by providing exponential decay rates for rare events.</p>
<p>For densities <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> the Chernoff entropy is</p>
<div class="math notranslate nohighlight">
\[
C(f,g) = - \log \min_{\phi \in (0,1)} \int f^{\phi}(x) g^{1-\phi}(x) \, dx.
\]</div>
<p>Remarks:</p>
<ul class="simple">
<li><p>The inner integral is the <strong>Chernoff coefficient</strong>.</p></li>
<li><p>At <span class="math notranslate nohighlight">\(\phi=1/2\)</span> it becomes the <strong>Bhattacharyya coefficient</strong> <span class="math notranslate nohighlight">\(\int \sqrt{f g}\)</span>.</p></li>
<li><p>In binary hypothesis testing with <span class="math notranslate nohighlight">\(T\)</span> iid observations, the optimal error probability decays as <span class="math notranslate nohighlight">\(e^{-C(f,g) T}\)</span>.</p></li>
</ul>
<p>We will see an example of the third point in the lecture <a class="reference internal" href="likelihood_ratio_process.html"><span class="doc">Likelihood Ratio Processes</span></a>,
where we study the Chernoff entropy in the context of model selection.</p>
<p>Let’s compute the Chernoff entropy between our example distributions <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">chernoff_integrand</span><span class="p">(</span><span class="n">ϕ</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Integral entering Chernoff entropy for a given ϕ.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">integrand</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">**</span><span class="n">ϕ</span> <span class="o">*</span> <span class="n">g</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">ϕ</span><span class="p">)</span>
    <span class="n">result</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="n">integrand</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">compute_chernoff_entropy</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute Chernoff entropy C(f,g).&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">ϕ</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">chernoff_integrand</span><span class="p">(</span><span class="n">ϕ</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize_scalar</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-5</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;bounded&#39;</span><span class="p">)</span>
    <span class="n">min_value</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">fun</span>
    <span class="n">ϕ_optimal</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
    <span class="n">chernoff_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">min_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chernoff_entropy</span><span class="p">,</span> <span class="n">ϕ_optimal</span>

<span class="n">C_fg</span><span class="p">,</span> <span class="n">ϕ_optimal</span> <span class="o">=</span> <span class="n">compute_chernoff_entropy</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chernoff entropy C(f,g) = </span><span class="si">{</span><span class="n">C_fg</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal ϕ = </span><span class="si">{</span><span class="n">ϕ_optimal</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Chernoff entropy C(f,g) = 0.1212
Optimal ϕ = 0.5969
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparing-divergence-measures">
<h2><a class="toc-backref" href="#id10"><span class="section-number">21.7. </span>Comparing divergence measures</a><a class="headerlink" href="#comparing-divergence-measures" title="Permalink to this heading">#</a></h2>
<p>We now compare these measures across several pairs of Beta distributions</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distribution_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># (f_params, g_params)</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">]</span>

<span class="c1"># Create comparison table</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">f_a</span><span class="p">,</span> <span class="n">f_b</span><span class="p">),</span> <span class="p">(</span><span class="n">g_a</span><span class="p">,</span> <span class="n">g_b</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">distribution_pairs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">f_a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">f_b</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">g_a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">g_b</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="n">kl_fg</span> <span class="o">=</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">kl_gf</span> <span class="o">=</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="n">js_div</span> <span class="o">=</span> <span class="n">compute_JS</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">chernoff_ent</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">compute_chernoff_entropy</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s1">&#39;Pair (f, g)&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">text</span><span class="se">{{</span><span class="s2">Beta</span><span class="se">}}</span><span class="s2">(</span><span class="si">{</span><span class="n">f_a</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">f_b</span><span class="si">}</span><span class="s2">), </span><span class="se">\\</span><span class="s2">text</span><span class="se">{{</span><span class="s2">Beta</span><span class="se">}}</span><span class="s2">(</span><span class="si">{</span><span class="n">g_a</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">g_b</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span>
        <span class="s1">&#39;KL(f, g)&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">kl_fg</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s1">&#39;KL(g, f)&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">kl_gf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s1">&#39;JS&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">js_div</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">chernoff_ent</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">})</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="c1"># Sort by JS divergence</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;JS_numeric&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;JS&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;JS_numeric&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;JS_numeric&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="s1">&#39; &amp; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">text</span><span class="se">{{</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="se">}}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
<span class="n">rows</span> <span class="o">=</span> <span class="s1">&#39; </span><span class="se">\\\\\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="p">[</span><span class="s1">&#39; &amp; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">row</span><span class="p">])</span> 
     <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>

<span class="n">latex_code</span> <span class="o">=</span> <span class="sa">rf</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">\begin</span><span class="se">{{</span><span class="s2">array</span><span class="se">}}{{</span><span class="s2">lcccc</span><span class="se">}}</span>
<span class="si">{</span><span class="n">columns</span><span class="si">}</span><span class="s2"> \\</span>
<span class="s2">\hline</span>
<span class="si">{</span><span class="n">rows</span><span class="si">}</span>
<span class="s2">\end</span><span class="se">{{</span><span class="s2">array</span><span class="se">}}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">display</span><span class="p">(</span><span class="n">Math</span><span class="p">(</span><span class="n">latex_code</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_latex math notranslate nohighlight">
\[\begin{split}\displaystyle 
\begin{array}{lcccc}
\text{Pair (f, g)} &amp; \text{KL(f, g)} &amp; \text{KL(g, f)} &amp; \text{JS} &amp; \text{C} \\
\hline
\text{Beta}(1,1), \text{Beta}(1.1,1.05) &amp; 0.0028 &amp; 0.0026 &amp; 0.0007 &amp; 0.0007 \\
\text{Beta}(1,1), \text{Beta}(1.2,1.1) &amp; 0.0105 &amp; 0.0092 &amp; 0.0024 &amp; 0.0025 \\
\text{Beta}(1,1), \text{Beta}(0.9,0.8) &amp; 0.0143 &amp; 0.0166 &amp; 0.0038 &amp; 0.0039 \\
\text{Beta}(1,1), \text{Beta}(1.5,1.2) &amp; 0.0589 &amp; 0.0437 &amp; 0.0121 &amp; 0.0126 \\
\text{Beta}(1,1), \text{Beta}(0.7,0.6) &amp; 0.0673 &amp; 0.0924 &amp; 0.0186 &amp; 0.0201 \\
\text{Beta}(1,1), \text{Beta}(2,1.5) &amp; 0.1781 &amp; 0.1081 &amp; 0.0309 &amp; 0.0339 \\
\text{Beta}(1,1), \text{Beta}(0.5,0.5) &amp; 0.1448 &amp; 0.2190 &amp; 0.0400 &amp; 0.0461 \\
\text{Beta}(1,1), \text{Beta}(2.5,1.8) &amp; 0.3323 &amp; 0.1731 &amp; 0.0502 &amp; 0.0577 \\
\text{Beta}(1,1), \text{Beta}(0.3,0.4) &amp; 0.3317 &amp; 0.5572 &amp; 0.0869 &amp; 0.1203 \\
\text{Beta}(1,1), \text{Beta}(3,1.2) &amp; 0.7590 &amp; 0.3436 &amp; 0.0984 &amp; 0.1212 \\
\text{Beta}(1,1), \text{Beta}(0.3,0.3) &amp; 0.3935 &amp; 0.6516 &amp; 0.1008 &amp; 0.1456 \\
\text{Beta}(1,1), \text{Beta}(4,1) &amp; 1.6134 &amp; 0.6362 &amp; 0.1733 &amp; 0.2341 \\
\text{Beta}(1,1), \text{Beta}(0.1,0.2) &amp; 0.9811 &amp; 1.0036 &amp; 0.1783 &amp; 0.4556 \\
\text{Beta}(1,1), \text{Beta}(5,1) &amp; 2.3901 &amp; 0.8094 &amp; 0.2162 &amp; 0.3128
\end{array}
\end{split}\]</div>
</div>
</div>
<p>We can clearly see co-movement across the divergence measures as we vary the parameters of the Beta distributions.</p>
<p>Next we visualize relationships among KL, JS, and Chernoff entropy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kl_fg_values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;KL(f, g)&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">js_values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;JS&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">chernoff_values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">kl_fg_values</span><span class="p">,</span> <span class="n">js_values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;KL divergence KL(f, g)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;JS divergence&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;JS divergence vs KL divergence&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">js_values</span><span class="p">,</span> <span class="n">chernoff_values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;JS divergence&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Chernoff entropy&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Chernoff entropy vs JS divergence&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7dc375ae145116b37789f5add9d5f18a9e537b7558da14037653abd9a0e09e5d.png" src="_images/7dc375ae145116b37789f5add9d5f18a9e537b7558da14037653abd9a0e09e5d.png" />
</div>
</div>
<p>We now generate plots illustrating how overlap visually diminishes as divergence measures increase.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>   
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)),</span>  
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)),</span>  
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">))</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_dist_diff</span><span class="p">(</span><span class="n">para_grid</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot overlap of selected Beta distribution pairs.&quot;&quot;&quot;</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    <span class="n">divergence_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">f_a</span><span class="p">,</span> <span class="n">f_b</span><span class="p">),</span> <span class="p">(</span><span class="n">g_a</span><span class="p">,</span> <span class="n">g_b</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_grid</span><span class="p">):</span>
        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">f_a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">f_b</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">g_a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">g_b</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
        <span class="n">kl_fg</span> <span class="o">=</span> <span class="n">compute_KL</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">js_div</span> <span class="o">=</span> <span class="n">compute_JS</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">chernoff_ent</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">compute_chernoff_entropy</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
        <span class="n">divergence_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s1">&#39;f_params&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">f_a</span><span class="p">,</span> <span class="n">f_b</span><span class="p">),</span>
            <span class="s1">&#39;g_params&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">g_a</span><span class="p">,</span> <span class="n">g_b</span><span class="p">),</span>
            <span class="s1">&#39;kl_fg&#39;</span><span class="p">:</span> <span class="n">kl_fg</span><span class="p">,</span>
            <span class="s1">&#39;js_div&#39;</span><span class="p">:</span> <span class="n">js_div</span><span class="p">,</span>
            <span class="s1">&#39;chernoff&#39;</span><span class="p">:</span> <span class="n">chernoff_ent</span>
        <span class="p">})</span>
        <span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
        <span class="n">f_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_range</span><span class="p">]</span>
        <span class="n">g_vals</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_range</span><span class="p">]</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">f_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> 
                        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;f ~ Beta(</span><span class="si">{</span><span class="n">f_a</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">f_b</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">g_vals</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> 
                        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;g ~ Beta(</span><span class="si">{</span><span class="n">g_a</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">g_b</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="n">overlap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">f_vals</span><span class="p">,</span> <span class="n">g_vals</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                        <span class="n">overlap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;overlap&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;KL(f,g)=</span><span class="si">{</span><span class="n">kl_fg</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, JS=</span><span class="si">{</span><span class="n">js_div</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, C=</span><span class="si">{</span><span class="n">chernoff_ent</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> 
            <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">divergence_data</span>

<span class="n">divergence_data</span> <span class="o">=</span> <span class="n">plot_dist_diff</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/8e062f4371d8ddafee30e4b5f95a7659230cea6d087ef9885ae09e7c088781ad.png" src="_images/8e062f4371d8ddafee30e4b5f95a7659230cea6d087ef9885ae09e7c088781ad.png" />
</div>
</div>
</section>
<section id="kl-divergence-and-maximum-likelihood-estimation">
<h2><a class="toc-backref" href="#id11"><span class="section-number">21.8. </span>KL divergence and maximum-likelihood estimation</a><a class="headerlink" href="#kl-divergence-and-maximum-likelihood-estimation" title="Permalink to this heading">#</a></h2>
<p>Given a sample of <span class="math notranslate nohighlight">\(n\)</span> observations <span class="math notranslate nohighlight">\(X = \{x_1, x_2, \ldots, x_n\}\)</span>, the <strong>empirical distribution</strong> is</p>
<div class="math notranslate nohighlight">
\[p_e(x) = \frac{1}{n} \sum_{i=1}^n \delta(x - x_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta(x - x_i)\)</span> is the Dirac delta function centered at <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\delta(x - x_i) = \begin{cases}
+\infty &amp; \text{if } x = x_i \\
0 &amp; \text{if } x \neq x_i
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p><strong>Discrete probability measure</strong>: Assigns probability <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span> to each observed data point</p></li>
<li><p><strong>Empirical expectation</strong>: <span class="math notranslate nohighlight">\(\langle X \rangle_{p_e} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{\mu}\)</span></p></li>
<li><p><strong>Support</strong>: Only on the observed data points <span class="math notranslate nohighlight">\(\{x_1, x_2, \ldots, x_n\}\)</span></p></li>
</ul>
<p>The KL divergence from the empirical distribution <span class="math notranslate nohighlight">\(p_e\)</span> to a parametric model <span class="math notranslate nohighlight">\(p_\theta(x)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[D_{KL}(p_e \parallel p_\theta) = \int p_e(x) \log \frac{p_e(x)}{p_\theta(x)} dx\]</div>
<p>Using the mathematics of the Dirac delta function, it follows that</p>
<div class="math notranslate nohighlight">
\[D_{KL}(p_e \parallel p_\theta) = \sum_{i=1}^n \frac{1}{n} \log \frac{\left(\frac{1}{n}\right)}{p_\theta(x_i)}\]</div>
<div class="math notranslate nohighlight">
\[= \frac{1}{n} \sum_{i=1}^n \log \frac{1}{n} - \frac{1}{n} \sum_{i=1}^n \log p_\theta(x_i)\]</div>
<div class="math notranslate nohighlight">
\[= -\log n - \frac{1}{n} \sum_{i=1}^n \log p_\theta(x_i)\]</div>
<p>Since the log-likelihood function for parameter <span class="math notranslate nohighlight">\(\theta\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta; X) = \sum_{i=1}^n \log p_\theta(x_i) ,
\]</div>
<p>it follows that maximum likelihood chooses parameters to minimize</p>
<div class="math notranslate nohighlight">
\[ D_{KL}(p_e \parallel p_\theta) \]</div>
<p>Thus, MLE is equivalent to minimizing the KL divergence from the empirical distribution to the statistical model <span class="math notranslate nohighlight">\(p_\theta\)</span>.</p>
</section>
<section id="related-lectures">
<h2><a class="toc-backref" href="#id12"><span class="section-number">21.9. </span>Related lectures</a><a class="headerlink" href="#related-lectures" title="Permalink to this heading">#</a></h2>
<p>This lecture has introduced tools  that we’ll encounter elsewhere.</p>
<ul class="simple">
<li><p>Other quantecon lectures  that apply  connections between divergence measures and statistical inference include  <a class="reference internal" href="likelihood_ratio_process.html"><span class="doc">Likelihood Ratio Processes</span></a>, <a class="reference internal" href="wald_friedman.html"><span class="doc">A Problem that Stumped Milton Friedman</span></a>, and <a class="reference internal" href="mix_model.html"><span class="doc">Incorrect Models</span></a>.</p></li>
<li><p>Statistical divergence functions also take center stage in  <a class="reference internal" href="likelihood_ratio_process_2.html"><span class="doc">Heterogeneous Beliefs and Financial Markets</span></a> that studies Lawrence Blume and David Easley’s model of  heterogeneous beliefs and financial markets.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="sir_model.html">
   1. Modeling COVID 19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   2. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="qr_decomp.html">
   3. QR Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eig_circulant.html">
   4. Circulant Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svd_intro.html">
   5. Singular Value Decomposition (SVD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="var_dmd.html">
   6. VARs and DMDs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newton_method.html">
   7. Using Newton’s Method to Solve Economic Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementary Statistics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="prob_matrix.html">
   8. Elementary Probability with Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stats_examples.html">
   9. Some Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   10. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prob_meaning.html">
   11. Two Meanings of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multi_hyper.html">
   12. Multivariate Hypergeometric Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_normal.html">
   13. Multivariate Normal Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hoist_failure.html">
   14. Fault Tree Uncertainties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="back_prop.html">
   15. Introduction to Artificial Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rand_resp.html">
   16. Randomized Response Surveys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="util_rand_resp.html">
   17. Expected Utilities of Random Responses
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayes Law
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="bayes_nonconj.html">
   18. Non-Conjugate Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_bayes.html">
   19. Posterior Distributions for  AR(1) Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_turningpts.html">
   20. Forecasting  an AR(1) Process
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics and Information
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   21. Statistical Divergence Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process.html">
   22. Likelihood Ratio Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process_2.html">
   23. Heterogeneous Beliefs and Financial Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_var.html">
   24. Likelihood Processes for VAR Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="imp_sample.html">
   25. Mean of a Likelihood Ratio Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman.html">
   26. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman_2.html">
   27. A Bayesian Formulation of Friedman and Wald’s Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exchangeable.html">
   28. Exchangeability and Bayesian Updating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_bayes.html">
   29. Likelihood Ratio Processes and Bayesian Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mix_model.html">
   30. Incorrect Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="navy_captain.html">
   31. Bayesian versus Frequentist Decision Rules
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Programming
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="opt_transport.html">
   32. Optimal Transport
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="von_neumann_model.html">
   33. Von Neumann Growth Model (and a Generalization)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="finite_markov.html">
   34. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inventory_dynamics.html">
   35. Inventory Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_models.html">
   36. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="samuelson.html">
   37. Samuelson Multiplier-Accelerator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kesten_processes.html">
   38. Kesten Processes and Firm Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wealth_dynamics.html">
   39. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman.html">
   40. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman_2.html">
   41. Another Look at the Kalman Filter
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Search
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model.html">
   42. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_separation.html">
   43. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_fitted_vfi.html">
   44. Job Search III: Fitted Value Function Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_correlated.html">
   45. Job Search IV: Correlated Wage Offers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="career.html">
   46. Job Search V: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jv.html">
   47. Job Search VI: On-the-Job Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_q.html">
   48. Job Search VII: A McCall Worker Q-Learns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="odu.html">
   49. Job Search VII: Search with Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Consumption, Savings and Capital
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_1.html">
   50. Cass-Koopmans Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_2.html">
   51. Cass-Koopmans Competitive Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_fiscal.html">
   52. Cass-Koopmans Model with Distorting Taxes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_fiscal_2.html">
   53. Two-Country Model with Distorting Taxes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ak2.html">
   54. Transitions in an Overlapping Generations Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_problem.html">
   55. Cake Eating I: Introduction to Optimal Saving
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_numerical.html">
   56. Cake Eating II: Numerical Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth.html">
   57. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth_fast.html">
   58. Optimal Growth II: Accelerating the Code with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coleman_policy_iter.html">
   59. Optimal Growth III: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="egm_policy_iter.html">
   60. Optimal Growth IV: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp.html">
   61. The Income Fluctuation Problem I: Basic Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp_advanced.html">
   62. The Income Fluctuation Problem II: Stochastic Returns on Assets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lqcontrol.html">
   63. LQ Control: Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lagrangian_lqdp.html">
   64. Lagrangian for LQ Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cross_product_trick.html">
   65. Eliminating Cross Products
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income.html">
   66. The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income_cons.html">
   67. Permanent Income II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lq_inventories.html">
   68. Production Smoothing via Inventories
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lake_model.html">
   69. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rational_expectations.html">
   70. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="re_with_feedback.html">
   71. Stability in Linear Rational Expectations Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_perf.html">
   72. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncertainty_traps.html">
   73. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aiyagari.html">
   74. The Aiyagari Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markov_asset.html">
   75. Asset Pricing: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ge_arrow.html">
   76. Competitive Equilibria with Arrow Securities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrison_kreps.html">
   77. Heterogeneous Beliefs and Bubbles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data and Empirics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_panel.html">
   78. Pandas for Panel Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ols.html">
   79. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mle.html">
   80. Maximum Likelihood Estimation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Auctions
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="two_auctions.html">
   81. First-Price and Second-Price Auctions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="house_auction.html">
   82. Multiple Good Allocation Mechanisms
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   83. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   84. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   85. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                </ul>

                <ul class="qe-toolbar__links">
                    <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search..." aria-label="Search..." autocomplete="off" accesskey="k">
                            <i data-feather="search" id="search-icon"></i>
                        </form>
                    </li>
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/divergence_measures.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <!--
                    # Enable if looking for link to specific document hosted on GitHub
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst/blob/main/lectures/divergence_measures.md" download><i data-feather="github"></i></a></li>
                    -->
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="/_pdf/quantecon-python.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/main/divergence_measures.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python.notebooks" data-urlpath="tree/lecture-python.notebooks/divergence_measures.ipynb" data-branch=main>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/main/divergence_measures.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "divergence_measures";
                const repoURL = "https://github.com/QuantEcon/lecture-python.notebooks";
                const urlPath = "tree/lecture-python.notebooks/divergence_measures.ipynb";
                const branch = "main"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
  </body>
</html>