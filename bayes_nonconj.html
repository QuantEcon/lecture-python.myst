
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>18. Non-Conjugate Priors &#8212; Intermediate Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" href="_static/styles/quantecon-book-theme.css?digest=10af625e9e6eb695015491be7f888e42a03bc430" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css?v=982b99e0" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=4c010e0d" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>


    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/scripts/quantecon-book-theme.js?digest=ceefa0362b71a389a50c4058a117a949b54259bd"></script>
    <script src="_static/scripts/jquery.js?v=5d32c60e"></script>
    <script src="_static/scripts/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-J0SMYR4SG3"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-J0SMYR4SG3');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-J0SMYR4SG3');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'bayes_nonconj';</script>
    <link rel="canonical" href="https://python.quantecon.org/bayes_nonconj.html" />
    <link rel="icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="19. Posterior Distributions for AR(1) Parameters" href="ar1_bayes.html" />
    <link rel="prev" title="17. Expected Utilities of Random Responses" href="util_rand_resp.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Non-Conjugate Priors"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Non-Conjugate Priors" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python.quantecon.org/bayes_nonconj.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Intermediate Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>

<!-- Override QuantEcon theme colors -->

    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=bayes_nonconj>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unleashing-mcmc-on-a-binomial-likelihood">18.1. Unleashing MCMC on a binomial likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analytical-posterior">18.1.1. Analytical posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#two-ways-to-approximate-posteriors">18.1.2. Two ways to approximate posteriors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distributions">18.2. Prior distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference">18.2.1. Variational inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">18.3. Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-prior-distributions">18.4. Alternative prior distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posteriors-via-mcmc-and-vi">18.5. Posteriors via MCMC and VI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-prior-and-posteriors">18.5.1. Beta prior and posteriors:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-conjugate-prior-distributions">18.6. Non-conjugate prior distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo">18.6.1. Markov chain Monte Carlo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">18.6.2. Variational inference</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vi-with-a-truncated-normal-guide">18.6.2.1. VI with a truncated normal guide</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference-with-a-beta-guide-distribution">18.6.2.2. Variational inference with a Beta guide distribution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo logo-img" alt="logo"></a>
                                    
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/en/stable/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Intermediate Quantitative Economics with Python</a></p>

                    </div>

                    <!-- Authors section -->
                        <p class="qe-page__header-authors" font-size="18">
                            
                                
                                    <a href="http://www.tomsargent.com/" target="_blank"><span>Thomas J. Sargent</span></a>
                                
                            
                                
                                    and <a href="https://johnstachurski.net/" target="_blank"><span>John Stachurski</span></a>
                                
                            
                        </p>

                    <!-- Last modified / Changelog dropdown -->
                        <button class="qe-page__header-changed" id="changelog-toggle" aria-expanded="false">
                            Last changed: Sep 11, 2025
                            <span class="changelog-icon">▼</span>
                        </button>

                    <!-- Changelog dropdown content -->
                    <div class="qe-page__header-changelog" id="changelog-content" aria-hidden="true">
                        <h4>Changelog (<a href="https://github.com/QuantEcon/lecture-python.myst/commits/main/lectures/bayes_nonconj.md">full history</a>)</h4>
                        <ul class="changelog-list">
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/a7648c019" class="changelog-hash">a7648c019</a>
                                
                                <span class="changelog-author">kp992</span>
                                <span class="changelog-time">2 months ago</span>
                                <span class="changelog-message">[bayes_nonconj] Update lecture (#545)</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/ec279702b" class="changelog-hash">ec279702b</a>
                                
                                <span class="changelog-author">Matt McKay</span>
                                <span class="changelog-time">6 months ago</span>
                                <span class="changelog-message">MAINT: remove jax, pyro, torch, and gpu related software installs + GPU admonition (#453)</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/290c5290e" class="changelog-hash">290c5290e</a>
                                
                                <span class="changelog-author">thomassargent30</span>
                                <span class="changelog-time">7 months ago</span>
                                <span class="changelog-message">Tom's March 30 edits of two intermediate lectures</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/1ead0a6c5" class="changelog-hash">1ead0a6c5</a>
                                
                                <span class="changelog-author">kp992</span>
                                <span class="changelog-time">1 year ago</span>
                                <span class="changelog-message">Use float dtype for numpyro (#418)</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/dd9fd0199" class="changelog-hash">dd9fd0199</a>
                                
                                <span class="changelog-author">Smit Lunagariya</span>
                                <span class="changelog-time">1 year ago</span>
                                <span class="changelog-message">MAINT: Fix small typos (#362)</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/dbec34949" class="changelog-hash">dbec34949</a>
                                
                                <span class="changelog-author">Thomas Sargent</span>
                                <span class="changelog-time">2 years ago</span>
                                <span class="changelog-message">Tom's Dec 20 edits of two lectures</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/c2953babb" class="changelog-hash">c2953babb</a>
                                
                                <span class="changelog-author">thomassargent30</span>
                                <span class="changelog-time">3 years ago</span>
                                <span class="changelog-message">Tom's June 30 edits of bayes_nonconj lecture</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/00b0760a3" class="changelog-hash">00b0760a3</a>
                                
                                <span class="changelog-author">mmcky</span>
                                <span class="changelog-time">3 years ago</span>
                                <span class="changelog-message">FIX: Adjustments for main branch to compile (#237)</span>
                            </li>
                            
                            <li class="changelog-entry">
                                
                                <a href="https://github.com/QuantEcon/lecture-python.myst/commit/a6b14916c" class="changelog-hash">a6b14916c</a>
                                
                                <span class="changelog-author">thomassargent30</span>
                                <span class="changelog-time">3 years ago</span>
                                <span class="changelog-message">Tom's bringing four new lectures into repo, June 27</span>
                            </li>
                            
                        </ul>
                    </div>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <section class="tex2jax_ignore mathjax_ignore" id="non-conjugate-priors">
<h1><span class="section-number">18. </span>Non-Conjugate Priors<a class="headerlink" href="#non-conjugate-priors" title="Link to this heading">#</a></h1>
<div class="warning admonition">
<p class="admonition-title">GPU</p>
<p>This lecture was built using a machine with the latest CUDA and CUDANN frameworks installed with access to a GPU.</p>
<p>To run this lecture on <a class="reference external" href="https://colab.research.google.com/">Google Colab</a>, click on the “play” icon top right, select Colab, and set the runtime environment to include a GPU.</p>
<p>To run this lecture on your own machine, you need to install the software listed following this notice.</p>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>numpyro<span class="w"> </span>jax
</pre></div>
</div>
</div>
<details class="admonition hide below-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell output</p>
<p class="expanded admonition-title">Hide code cell output</p>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: numpyro in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (0.19.0)
Requirement already satisfied: jax in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (0.8.1)
Requirement already satisfied: jaxlib&gt;=0.4.25 in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from numpyro) (0.8.1)
Requirement already satisfied: multipledispatch in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from numpyro) (0.6.0)
Requirement already satisfied: numpy in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from numpyro) (2.1.3)
Requirement already satisfied: tqdm in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from numpyro) (4.67.1)
Requirement already satisfied: ml_dtypes&gt;=0.5.0 in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from jax) (0.5.4)
Requirement already satisfied: opt_einsum in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from jax) (3.4.0)
Requirement already satisfied: scipy&gt;=1.13 in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from jax) (1.15.3)
Requirement already satisfied: six in /home/runner/miniconda3/envs/quantecon/lib/python3.13/site-packages (from multipledispatch-&gt;numpyro) (1.17.0)
</pre></div>
</div>
</div>
</details>
</div>
<p>This lecture is a sequel to the <a class="reference internal" href="prob_meaning.html"><span class="doc">Two Meanings of Probability</span></a>.</p>
<p>That lecture offers a Bayesian interpretation of probability in a setting in which the likelihood function and the prior distribution
over parameters just happened to form a <strong>conjugate</strong> pair in which</p>
<ul class="simple">
<li><p>application of Bayes’ Law produces a posterior distribution that has the same functional form as the prior</p></li>
</ul>
<p>Having a likelihood and prior that are conjugate can simplify calculation of a posterior, facilitating analytical or nearly analytical calculations.</p>
<p>But in many situations the likelihood and prior need not form a conjugate pair.</p>
<ul class="simple">
<li><p>after all, a person’s prior is his or her own business and would take a form conjugate to a likelihood only by remote coincidence</p></li>
</ul>
<p>In these situations, computing a posterior can become very challenging.</p>
<p>In this lecture, we illustrate how modern Bayesians confront non-conjugate priors by using Monte Carlo techniques that involve</p>
<ul class="simple">
<li><p>first cleverly forming a Markov chain whose invariant distribution is the posterior distribution we want</p></li>
<li><p>simulating the Markov chain until it has converged and then sampling from the invariant distribution to approximate the posterior</p></li>
</ul>
<p>We shall illustrate the approach by deploying a powerful Python library, <a class="reference external" href="https://num.pyro.ai/en/stable/getting_started.html">NumPyro</a> that implements this approach.</p>
<p>As usual, we begin by importing some Python code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">st</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">random</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpyro</span><span class="w"> </span><span class="kn">import</span> <span class="n">distributions</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpyro.distributions.constraints</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">constraints</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpyro.infer</span><span class="w"> </span><span class="kn">import</span> <span class="n">MCMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">,</span> <span class="n">SVI</span><span class="p">,</span> <span class="n">Trace_ELBO</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpyro.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>
</pre></div>
</div>
</div>
</div>
<section id="unleashing-mcmc-on-a-binomial-likelihood">
<h2><span class="section-number">18.1. </span>Unleashing MCMC on a binomial likelihood<a class="headerlink" href="#unleashing-mcmc-on-a-binomial-likelihood" title="Link to this heading">#</a></h2>
<p>This lecture begins with the binomial example in the <a class="reference internal" href="prob_meaning.html"><span class="doc">Two Meanings of Probability</span></a>.</p>
<p>That lecture computed a posterior</p>
<ul class="simple">
<li><p>analytically via choosing the conjugate priors,</p></li>
</ul>
<p>This lecture instead computes posteriors</p>
<ul class="simple">
<li><p>numerically by sampling from the posterior distribution through MCMC methods, and</p></li>
<li><p>using a variational inference (VI) approximation.</p></li>
</ul>
<p>We use <code class="docutils literal notranslate"><span class="pre">numpyro</span></code> with assistance from <code class="docutils literal notranslate"><span class="pre">jax</span></code> to approximate a posterior distribution.</p>
<p>We use several alternative prior distributions.</p>
<p>We compare computed posteriors with ones associated with a conjugate prior as described in <a class="reference internal" href="prob_meaning.html"><span class="doc">Two Meanings of Probability</span></a>.</p>
<section id="analytical-posterior">
<h3><span class="section-number">18.1.1. </span>Analytical posterior<a class="headerlink" href="#analytical-posterior" title="Link to this heading">#</a></h3>
<p>Assume that the random variable <span class="math notranslate nohighlight">\(X\sim Binom\left(n,\theta\right)\)</span>.</p>
<p>This defines a likelihood function</p>
<div class="math notranslate nohighlight">
\[
L\left(Y\vert\theta\right) = \textrm{Prob}(X =  k | \theta) =
\left(\frac{n!}{k! (n-k)!} \right) \theta^k (1-\theta)^{n-k}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y=k\)</span> is an observed data point.</p>
<p>We view  <span class="math notranslate nohighlight">\(\theta\)</span> as a random variable for which we assign a prior distribution having density <span class="math notranslate nohighlight">\(f(\theta)\)</span>.</p>
<p>We will try alternative priors later, but for now, suppose the prior is distributed as <span class="math notranslate nohighlight">\(\theta\sim Beta\left(\alpha,\beta\right)\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
f(\theta) = \textrm{Prob}(\theta) = \frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)}
\]</div>
<p>We choose this as our prior for now because we know that a conjugate prior for the binomial likelihood function is a beta distribution.</p>
<p>After observing <span class="math notranslate nohighlight">\(k\)</span> successes among <span class="math notranslate nohighlight">\(N\)</span> sample observations, the posterior probability distribution of <span class="math notranslate nohighlight">\( \theta \)</span> is</p>
<div class="math notranslate nohighlight">
\[
\textrm{Prob}(\theta|k) = \frac{\textrm{Prob}(\theta,k)}{\textrm{Prob}(k)}=\frac{\textrm{Prob}(k|\theta)\textrm{Prob}(\theta)}{\textrm{Prob}(k)}=\frac{\textrm{Prob}(k|\theta) \textrm{Prob}(\theta)}{\int_0^1 \textrm{Prob}(k|\theta)\textrm{Prob}(\theta) d\theta}
\]</div>
<div class="math notranslate nohighlight">
\[
=\frac{{N \choose k} (1 - \theta)^{N-k} \theta^k \frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)}}{\int_0^1 {N \choose k} (1 - \theta)^{N-k} \theta^k\frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)} d\theta}
\]</div>
<div class="math notranslate nohighlight">
\[
=\frac{(1 -\theta)^{\beta+N-k-1} \theta^{\alpha+k-1}}{\int_0^1 (1 - \theta)^{\beta+N-k-1} \theta^{\alpha+k-1} d\theta} .
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\textrm{Prob}(\theta|k) \sim {Beta}(\alpha + k, \beta+N-k)
\]</div>
<p>The analytical posterior for a given conjugate beta prior is coded in the following</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">simulate_draw</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Draws a Bernoulli sample of size n with probability P(Y=1) = θ&quot;&quot;&quot;</span>
    <span class="n">rand_draw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">draw</span> <span class="o">=</span> <span class="p">(</span><span class="n">rand_draw</span> <span class="o">&lt;</span> <span class="n">θ</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">draw</span>


<span class="k">def</span><span class="w"> </span><span class="nf">analytical_beta_posterior</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">α0</span><span class="p">,</span> <span class="n">β0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes analytically the posterior distribution </span>
<span class="sd">    with beta prior parametrized by (α, β)</span>
<span class="sd">    given # num observations</span>

<span class="sd">    Parameters</span>
<span class="sd">    ---------</span>
<span class="sd">    num : int.</span>
<span class="sd">        the number of observations after which we calculate the posterior</span>
<span class="sd">    α0, β0 : float.</span>
<span class="sd">        the parameters for the beta distribution as a prior</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    The posterior beta distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">up_num</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">down_num</span> <span class="o">=</span> <span class="n">num</span> <span class="o">-</span> <span class="n">up_num</span>
    <span class="k">return</span> <span class="n">st</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">α0</span> <span class="o">+</span> <span class="n">up_num</span><span class="p">,</span> <span class="n">β0</span> <span class="o">+</span> <span class="n">down_num</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="two-ways-to-approximate-posteriors">
<h3><span class="section-number">18.1.2. </span>Two ways to approximate posteriors<a class="headerlink" href="#two-ways-to-approximate-posteriors" title="Link to this heading">#</a></h3>
<p>Suppose that we don’t have a conjugate prior.</p>
<p>Then we can’t compute posteriors analytically.</p>
<p>Instead, we use computational tools to approximate the posterior distribution for a set of alternative prior distributions using <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>.</p>
<p>We first use the <strong>Markov Chain Monte Carlo</strong> (MCMC) algorithm.</p>
<p>We implement the NUTS sampler to sample from the posterior.</p>
<p>In that way we construct a sampling distribution that approximates the posterior.</p>
<p>After doing that we deploy another procedure called <strong>Variational Inference</strong> (VI).</p>
<p>In particular, we implement Stochastic Variational Inference (SVI) machinery in <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>.</p>
<p>The MCMC algorithm supposedly generates a more accurate approximation since in principle it directly samples from the posterior distribution.</p>
<p>But it can be computationally expensive, especially when dimension is large.</p>
<p>A VI approach can be cheaper, but it is likely to produce an inferior approximation to the posterior, for the simple reason that it requires guessing a parametric <strong>guide functional form</strong> that we use to approximate a posterior.</p>
<p>This guide function is likely at best to be an imperfect approximation.</p>
<p>By paying the cost of restricting the putative posterior to have a restricted functional form,
the problem of approximating a posterior is transformed to a well-posed optimization problem that seeks parameters of the putative posterior that minimize
a Kullback-Leibler (KL) divergence between true posterior and the putative posterior distribution.</p>
<ul class="simple">
<li><p>minimizing the KL divergence is equivalent to maximizing a criterion called the <strong>Evidence Lower Bound</strong> (ELBO), as we shall verify soon.</p></li>
</ul>
</section>
</section>
<section id="prior-distributions">
<h2><span class="section-number">18.2. </span>Prior distributions<a class="headerlink" href="#prior-distributions" title="Link to this heading">#</a></h2>
<p>In order to be able to apply MCMC sampling or VI, <code class="docutils literal notranslate"><span class="pre">numpyro</span></code> requires that a prior distribution satisfy special properties:</p>
<ul class="simple">
<li><p>we must be able to sample from it;</p></li>
<li><p>we must be able to compute the log pdf pointwise;</p></li>
<li><p>the pdf must be differentiable with respect to the parameters.</p></li>
</ul>
<p>We’ll want to define a distribution <code class="docutils literal notranslate"><span class="pre">class</span></code>.</p>
<p>We will use the following priors:</p>
<ul class="simple">
<li><p>a uniform distribution on <span class="math notranslate nohighlight">\([\underline \theta, \overline \theta]\)</span>, where <span class="math notranslate nohighlight">\(0 \leq \underline \theta &lt; \overline \theta \leq 1\)</span>.</p></li>
<li><p>a truncated log-normal distribution with support on <span class="math notranslate nohighlight">\([0,1]\)</span> with parameters <span class="math notranslate nohighlight">\((\mu,\sigma)\)</span>.</p>
<ul>
<li><p>To implement this, let <span class="math notranslate nohighlight">\(Z\sim N(\mu,\sigma)\)</span> and <span class="math notranslate nohighlight">\(\tilde{Z}\)</span> be truncated normal with support <span class="math notranslate nohighlight">\([-\infty,\log(1)]\)</span>, then <span class="math notranslate nohighlight">\(\exp(Z)\)</span> has a log normal distribution with bounded support <span class="math notranslate nohighlight">\([0,1]\)</span>. This can be easily coded since <code class="docutils literal notranslate"><span class="pre">numpyro</span></code> has a built-in truncated normal distribution, and <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>’s <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> class that includes an exponential transformation.</p></li>
</ul>
</li>
<li><p>a shifted von Mises distribution that has support confined to <span class="math notranslate nohighlight">\([0,1]\)</span> with parameter <span class="math notranslate nohighlight">\((\mu,\kappa)\)</span>.</p>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(X\sim vonMises(0,\kappa)\)</span>. We know that <span class="math notranslate nohighlight">\(X\)</span> has bounded support <span class="math notranslate nohighlight">\([-\pi, \pi]\)</span>. We can define a shifted von Mises random variable <span class="math notranslate nohighlight">\(\tilde{X}=a+bX\)</span> where <span class="math notranslate nohighlight">\(a=0.5, b=1/(2 \pi)\)</span> so that <span class="math notranslate nohighlight">\(\tilde{X}\)</span> is supported on <span class="math notranslate nohighlight">\([0,1]\)</span>.</p></li>
<li><p>This can be implemented using <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>’s <code class="docutils literal notranslate"><span class="pre">TransformedDistribution</span></code> class with its <code class="docutils literal notranslate"><span class="pre">AffineTransform</span></code> method.</p></li>
</ul>
</li>
<li><p>a truncated Laplace distribution.</p>
<ul>
<li><p>We also considered a truncated Laplace distribution because its density comes in a piece-wise non-smooth form and has a distinctive spiked shape.</p></li>
<li><p>The truncated Laplace can be created using <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>’s <code class="docutils literal notranslate"><span class="pre">TruncatedDistribution</span></code> class.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">truncated_log_normal_trans</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtains the truncated log normal distribution </span>
<span class="sd">    using numpyro&#39;s TruncatedNormal and ExpTransform</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">base_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">(</span>
        <span class="n">low</span><span class="o">=-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span>
        <span class="n">base_dist</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ExpTransform</span><span class="p">()</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">shifted_von_mises</span><span class="p">(</span><span class="n">κ</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Obtains the shifted von Mises distribution using AffineTransform&quot;&quot;&quot;</span>
    <span class="n">base_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">VonMises</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">κ</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span>
        <span class="n">base_dist</span><span class="p">,</span> 
        <span class="n">dist</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">AffineTransform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">truncated_laplace</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Obtains the truncated Laplace distribution on [0,1]&quot;&quot;&quot;</span>
    <span class="n">base_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Laplace</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedDistribution</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="variational-inference">
<h3><span class="section-number">18.2.1. </span>Variational inference<a class="headerlink" href="#variational-inference" title="Link to this heading">#</a></h3>
<p>Instead of directly sampling from the posterior, the <strong>variational inference</strong> method approximates an unknown posterior distribution with a family of tractable distributions/densities.</p>
<p>It then seeks to minimize a measure of statistical discrepancy between the approximating and true posteriors.</p>
<p>Thus variational inference (VI) approximates a posterior by solving a minimization problem.</p>
<p>Let the latent parameter/variable that we want to infer be <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Let the prior be <span class="math notranslate nohighlight">\(p(\theta)\)</span> and the likelihood be <span class="math notranslate nohighlight">\(p\left(Y\vert\theta\right)\)</span>.</p>
<p>We want <span class="math notranslate nohighlight">\(p\left(\theta\vert Y\right)\)</span>.</p>
<p>Bayes’ rule implies</p>
<div class="math notranslate nohighlight">
\[
p\left(\theta\vert Y\right)=\frac{p\left(Y,\theta\right)}{p\left(Y\right)}=\frac{p\left(Y\vert\theta\right)p\left(\theta\right)}{p\left(Y\right)}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-intchallenge">
<span class="eqno">(18.1)<a class="headerlink" href="#equation-eq-intchallenge" title="Link to this equation">#</a></span>\[
p\left(Y\right)=\int p\left(Y\mid\theta\right)p\left(\theta\right) d\theta.
\]</div>
<p>The integral on the right side of <a class="reference internal" href="#equation-eq-intchallenge">(18.1)</a> is typically difficult to compute.</p>
<p>Consider a <strong>guide distribution</strong> <span class="math notranslate nohighlight">\(q_{\phi}(\theta)\)</span> parameterized by <span class="math notranslate nohighlight">\(\phi\)</span> that we’ll use to approximate the posterior.</p>
<p>We choose parameters <span class="math notranslate nohighlight">\(\phi\)</span> of the guide distribution to minimize a Kullback-Leibler (KL) divergence between the approximate posterior <span class="math notranslate nohighlight">\(q_{\phi}(\theta)\)</span> and the posterior:</p>
<div class="math notranslate nohighlight">
\[
 D_{KL}(q(\theta;\phi)\;\|\;p(\theta\mid Y)) \equiv -\int q(\theta;\phi)\log\frac{p(\theta\mid Y)}{q(\theta;\phi)} d\theta
\]</div>
<p>Thus, we want a <strong>variational distribution</strong> <span class="math notranslate nohighlight">\(q\)</span> that solves</p>
<div class="math notranslate nohighlight">
\[
\min_{\phi}\quad D_{KL}(q(\theta;\phi)\;\|\;p(\theta\mid Y))
\]</div>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}D_{KL}(q(\theta;\phi)\;\|\;p(\theta\mid Y)) &amp; =-\int q(\theta;\phi)\log\frac{P(\theta\mid Y)}{q(\theta;\phi)} d\theta\\
 &amp; =-\int q(\theta)\log\frac{\frac{p(\theta,Y)}{p(Y)}}{q(\theta)} d\theta\\
 &amp; =-\int q(\theta)\log\frac{p(\theta,Y)}{q(\theta)p(Y)} d\theta\\
 &amp; =-\int q(\theta)\left[\log\frac{p(\theta,Y)}{q(\theta)}-\log p(Y)\right] d\theta\\
 &amp; =-\int q(\theta)\log\frac{p(\theta,Y)}{q(\theta)}+\int q(\theta)\log p(Y) d\theta\\
 &amp; =-\int q(\theta)\log\frac{p(\theta,Y)}{q(\theta)} d\theta+\log p(Y)\\
\log p(Y)&amp;=D_{KL}(q(\theta;\phi)\;\|\;p(\theta\mid Y))+\int q_{\phi}(\theta)\log\frac{p(\theta,Y)}{q_{\phi}(\theta)} d\theta
\end{aligned}
\end{split}\]</div>
<p>For observed data <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(p(\theta,Y)\)</span> is a constant, so minimizing KL divergence is equivalent to maximizing</p>
<div class="math notranslate nohighlight" id="equation-eq-elbo">
<span class="eqno">(18.2)<a class="headerlink" href="#equation-eq-elbo" title="Link to this equation">#</a></span>\[
ELBO\equiv\int q_{\phi}(\theta)\log\frac{p(\theta,Y)}{q_{\phi}(\theta)} d\theta=\mathbb{E}_{q_{\phi}(\theta)}\left[\log p(\theta,Y)-\log q_{\phi}(\theta)\right]
\]</div>
<p>Formula <a class="reference internal" href="#equation-eq-elbo">(18.2)</a> is called the evidence lower bound (ELBO).</p>
<p>A standard optimization routine can be used to search for the optimal <span class="math notranslate nohighlight">\(\phi\)</span> in our parametrized distribution <span class="math notranslate nohighlight">\(q_{\phi}(\theta)\)</span>.</p>
<p>The parameterized distribution <span class="math notranslate nohighlight">\(q_{\phi}(\theta)\)</span> is called the <strong>variational distribution</strong>.</p>
<p>We can implement Stochastic Variational Inference (SVI) in numpyro using the <code class="docutils literal notranslate"><span class="pre">Adam</span></code> gradient descent algorithm to approximate the posterior.</p>
<p>We use two sets of variational distributions: Beta and TruncatedNormal with support <span class="math notranslate nohighlight">\([0,1]\)</span></p>
<ul class="simple">
<li><p>Learnable parameters for the Beta distribution are (<span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>), both of which are positive.</p></li>
<li><p>Learnable parameters for the Truncated Normal distribution are (loc, scale).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We restrict the truncated Normal parameter ‘loc’ to be in the interval <span class="math notranslate nohighlight">\([0,1]\)</span></p>
</div>
</section>
</section>
<section id="implementation">
<h2><span class="section-number">18.3. </span>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h2>
<p>We have constructed a Python class <code class="docutils literal notranslate"><span class="pre">BayesianInference</span></code> that requires the following arguments to be initialized:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">param</span></code>: a tuple/scalar of parameters dependent on distribution types</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">name_dist</span></code>: a string that specifies distribution names</p></li>
</ul>
<p>The (<code class="docutils literal notranslate"><span class="pre">param</span></code>, <code class="docutils literal notranslate"><span class="pre">name_dist</span></code>) pair includes:</p>
<ul class="simple">
<li><p>(<span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>, ‘beta’)</p></li>
<li><p>(lower_bound, upper_bound, ‘uniform’)</p></li>
<li><p>(loc, scale, ‘lognormal’)</p>
<ul>
<li><p>Note: This is the truncated log normal.</p></li>
</ul>
</li>
<li><p>(<span class="math notranslate nohighlight">\(\kappa\)</span>, ‘vonMises’), where <span class="math notranslate nohighlight">\(\kappa\)</span> denotes concentration parameter, and center location is set to <span class="math notranslate nohighlight">\(0.5\)</span>. Using <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>, this is the <strong>shifted</strong> distribution.</p></li>
<li><p>(loc, scale, ‘laplace’)</p>
<ul>
<li><p>Note: This is the truncated Laplace</p></li>
</ul>
</li>
</ul>
<p>The class <code class="docutils literal notranslate"><span class="pre">BayesianInference</span></code> has several key methods :</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sample_prior</span></code>:</p>
<ul>
<li><p>This can be used to draw a single sample from the given prior distribution.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">show_prior</span></code>:</p>
<ul>
<li><p>Plots the approximate prior distribution by repeatedly drawing samples and fitting a kernel density curve.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">mcmc_sampling</span></code>:</p>
<ul>
<li><p>INPUT: (data, num_samples, num_warmup=1000)</p></li>
<li><p>Takes a <code class="docutils literal notranslate"><span class="pre">jnp.array</span></code> data and generates MCMC sampling of posterior of size <code class="docutils literal notranslate"><span class="pre">num_samples</span></code>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">svi_run</span></code>:</p>
<ul>
<li><p>INPUT: (data, guide_dist, n_steps=10000)</p></li>
<li><p>guide_dist = ‘normal’ - use a <strong>truncated</strong> normal distribution as the parametrized guide</p></li>
<li><p>guide_dist = ‘beta’ - use a beta distribution as the parametrized guide</p></li>
<li><p>RETURN: (params, losses) - the learned parameters in a <code class="docutils literal notranslate"><span class="pre">dict</span></code> and the vector of loss at each step.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BayesianInference</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parameters</span>
<span class="sd">    ---------</span>
<span class="sd">    param : tuple.</span>
<span class="sd">        a tuple object that contains all relevant parameters for the distribution</span>
<span class="sd">    name_dist : str.</span>
<span class="sd">        name of the distribution - &#39;beta&#39;, &#39;uniform&#39;, &#39;lognormal&#39;, &#39;vonMises&#39;, &#39;laplace&#39;</span>
<span class="sd">    rng_key : jax.random.PRNGKey</span>
<span class="sd">        PRNG key for random number generation. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">param</span><span class="p">:</span> <span class="nb">tuple</span>
    <span class="n">name_dist</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">rng_key</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_bayesian_inference</span><span class="p">(</span>
    <span class="n">param</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> 
    <span class="n">name_dist</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BayesianInference</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Factory function to create a BayesianInference instance&quot;&quot;&quot;</span>

    <span class="n">rng_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">BayesianInference</span><span class="p">(</span>
        <span class="n">param</span><span class="o">=</span><span class="n">param</span><span class="p">,</span>
        <span class="n">name_dist</span><span class="o">=</span><span class="n">name_dist</span><span class="p">,</span>
        <span class="n">rng_key</span><span class="o">=</span><span class="n">rng_key</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">sample_prior</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Define the prior distribution to sample from in numpyro models.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">name_dist</span> <span class="o">==</span> <span class="s2">&quot;beta&quot;</span><span class="p">:</span>
        <span class="c1"># unpack parameters</span>
        <span class="n">α0</span><span class="p">,</span> <span class="n">β0</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">param</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="n">α0</span><span class="p">,</span> <span class="n">β0</span><span class="p">),</span> <span class="n">rng_key</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">rng_key</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">name_dist</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="c1"># unpack parameters</span>
        <span class="n">lb</span><span class="p">,</span> <span class="n">ub</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">param</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">lb</span><span class="p">,</span> <span class="n">ub</span><span class="p">),</span> <span class="n">rng_key</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">rng_key</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">name_dist</span> <span class="o">==</span> <span class="s2">&quot;lognormal&quot;</span><span class="p">:</span>
        <span class="c1"># unpack parameters</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">param</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="s2">&quot;theta&quot;</span><span class="p">,</span> 
            <span class="n">truncated_log_normal_trans</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span> 
            <span class="n">rng_key</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">rng_key</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">name_dist</span> <span class="o">==</span> <span class="s2">&quot;vonMises&quot;</span><span class="p">:</span>
        <span class="c1"># unpack parameters</span>
        <span class="n">κ</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">param</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">shifted_von_mises</span><span class="p">(</span><span class="n">κ</span><span class="p">),</span> <span class="n">rng_key</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">rng_key</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">name_dist</span> <span class="o">==</span> <span class="s2">&quot;laplace&quot;</span><span class="p">:</span>
        <span class="c1"># unpack parameters</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">param</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">truncated_laplace</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span> <span class="n">rng_key</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">rng_key</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">sample</span>


<span class="k">def</span><span class="w"> </span><span class="nf">show_prior</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mf">1e5</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">disp_plot</span><span class="o">=</span><span class="mi">1</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Visualizes prior distribution by sampling from prior </span>
<span class="sd">    and plots the approximated sampling distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s2">&quot;show_prior&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">sample_prior</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="c1"># to JAX array</span>
    <span class="n">sample_array</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

    <span class="c1"># plot histogram and kernel density</span>
    <span class="k">if</span> <span class="n">disp_plot</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">displot</span><span class="p">(</span>
            <span class="n">sample_array</span><span class="p">,</span> 
            <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> 
            <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> 
            <span class="n">height</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
            <span class="n">aspect</span><span class="o">=</span><span class="mf">1.5</span>
        <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">sample_array</span>


<span class="k">def</span><span class="w"> </span><span class="nf">set_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define the probabilistic model by specifying prior, </span>
<span class="sd">    conditional likelihood, and data conditioning</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">sample_prior</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
        <span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">theta</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">mcmc_sampling</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes numerically the posterior distribution </span>
<span class="sd">    with beta prior parametrized by (α0, β0)</span>
<span class="sd">    given data using MCMC</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">nuts_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">set_model</span><span class="p">)</span>
    <span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span>
        <span class="n">nuts_kernel</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">num_warmup</span><span class="o">=</span><span class="n">num_warmup</span><span class="p">,</span>
        <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()[</span><span class="s2">&quot;theta&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">samples</span>


<span class="c1"># arguments in this function are used to align with the arguments in set_model()</span>
<span class="c1"># this is required by svi.run()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">beta_guide</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines the candidate parametrized variational distribution </span>
<span class="sd">    that we train to approximate posterior with numpyro</span>
<span class="sd">    Here we use parameterized beta</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">α_q</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;alpha_q&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">β_q</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;beta_q&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>

    <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="n">α_q</span><span class="p">,</span> <span class="n">β_q</span><span class="p">))</span>


<span class="c1"># similar with beta_guide()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">truncnormal_guide</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines the candidate parametrized variational distribution </span>
<span class="sd">    that we train to approximate posterior with numpyro</span>
<span class="sd">    Here we use truncated normal on [0,1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
        <span class="s2">&quot;theta&quot;</span><span class="p">,</span> 
        <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">svi_init</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">guide_dist</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initiate SVI training mode with Adam optimizer&quot;&quot;&quot;</span>
    <span class="n">adam_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">}</span>

    <span class="k">if</span> <span class="n">guide_dist</span> <span class="o">==</span> <span class="s2">&quot;beta&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span>
            <span class="n">set_model</span><span class="p">,</span> <span class="n">beta_guide</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">guide_dist</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">svi</span> <span class="o">=</span> <span class="n">SVI</span><span class="p">(</span>
            <span class="n">set_model</span><span class="p">,</span> <span class="n">truncnormal_guide</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">Trace_ELBO</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;WARNING: Please input either &#39;beta&#39; or &#39;normal&#39;&quot;</span><span class="p">)</span>
        <span class="n">svi</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">svi</span>


<span class="k">def</span><span class="w"> </span><span class="nf">svi_run</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">guide_dist</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs SVI and returns optimized parameters and losses</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    params : the learned parameters for guide</span>
<span class="sd">    losses : a vector of loss at each step</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># initiate SVI</span>
    <span class="n">svi</span> <span class="o">=</span> <span class="n">svi_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide_dist</span><span class="p">)</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">svi</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">))</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">losses</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="alternative-prior-distributions">
<h2><span class="section-number">18.4. </span>Alternative prior distributions<a class="headerlink" href="#alternative-prior-distributions" title="Link to this heading">#</a></h2>
<p>Let’s see how well our sampling algorithm does in approximating</p>
<ul class="simple">
<li><p>a log normal distribution</p></li>
<li><p>a uniform distribution</p></li>
</ul>
<p>To examine our alternative prior distributions, we’ll plot approximate prior distributions below by calling the <code class="docutils literal notranslate"><span class="pre">show_prior</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># truncated log normal</span>
<span class="n">example_ln</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;lognormal&quot;</span><span class="p">)</span>
<span class="n">show_prior</span><span class="p">(</span><span class="n">example_ln</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1124 03:48:15.648743    3798 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn&#39;t support extracting fabric info or NVLink is not used by the device.
W1124 03:48:15.652328    3733 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn&#39;t support extracting fabric info or NVLink is not used by the device.
</pre></div>
</div>
<figure class="align-default" id="fig-lognormal-dist">
<img alt="_images/1ccf1957b65d8ff95c13dfb63a952d8d2e58d7c601b570dde87cc0bd384ee71f.png" src="_images/1ccf1957b65d8ff95c13dfb63a952d8d2e58d7c601b570dde87cc0bd384ee71f.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.1 </span><span class="caption-text">Truncated log normal distribution</span><a class="headerlink" href="#fig-lognormal-dist" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># truncated uniform</span>
<span class="n">example_un</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
<span class="n">show_prior</span><span class="p">(</span><span class="n">example_un</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="fig-uniform-dist">
<img alt="_images/bd942f72b41c33ed2f8332ef97f4e3088381088c5d1893f1bbfd431a56f7dfec.png" src="_images/bd942f72b41c33ed2f8332ef97f4e3088381088c5d1893f1bbfd431a56f7dfec.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.2 </span><span class="caption-text">Truncated uniform distribution</span><a class="headerlink" href="#fig-uniform-dist" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>The above graphs show that sampling seems to work well with both distributions.</p>
<p>Now let’s see how well things work with von Mises distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># shifted von Mises</span>
<span class="n">example_vm</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;vonMises&quot;</span><span class="p">)</span>
<span class="n">show_prior</span><span class="p">(</span><span class="n">example_vm</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="fig-vonmises-dist">
<img alt="_images/e425414ec0b38c2063425b2563153131176c7328e58bddd1c16666993c69c7e0.png" src="_images/e425414ec0b38c2063425b2563153131176c7328e58bddd1c16666993c69c7e0.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.3 </span><span class="caption-text">Shifted von Mises distribution</span><a class="headerlink" href="#fig-vonmises-dist" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>The graphs look good too.</p>
<p>Now let’s try with a Laplace distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># truncated Laplace</span>
<span class="n">example_lp</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;laplace&quot;</span><span class="p">)</span>
<span class="n">show_prior</span><span class="p">(</span><span class="n">example_lp</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="fig-laplace-dist">
<img alt="_images/9067561b3a70cfacddbff71465c580a63902477a0a296e9e8c2058974df8ce95.png" src="_images/9067561b3a70cfacddbff71465c580a63902477a0a296e9e8c2058974df8ce95.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.4 </span><span class="caption-text">Truncated Laplace distribution</span><a class="headerlink" href="#fig-laplace-dist" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>Having assured ourselves that our sampler seems to do a good job, let’s put it to work in using MCMC to compute posterior probabilities.</p>
</section>
<section id="posteriors-via-mcmc-and-vi">
<h2><span class="section-number">18.5. </span>Posteriors via MCMC and VI<a class="headerlink" href="#posteriors-via-mcmc-and-vi" title="Link to this heading">#</a></h2>
<p>We construct a class <code class="docutils literal notranslate"><span class="pre">BayesianInferencePlot</span></code> to implement MCMC or VI algorithms and plot multiple posteriors for different updating data sizes and different possible priors.</p>
<p>This class takes as inputs the true data generating parameter <code class="docutils literal notranslate"><span class="pre">θ</span></code>, a list of updating data sizes for multiple posterior plotting, and a defined and parametrized <code class="docutils literal notranslate"><span class="pre">BayesianInference</span></code> class.</p>
<p>It has two key methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BayesianInferencePlot.mcmc_plot()</span></code> takes desired MCMC sample size as input and plots the output posteriors together with the prior defined in <code class="docutils literal notranslate"><span class="pre">BayesianInference</span></code> class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BayesianInferencePlot.svi_plot()</span></code> takes desired VI distribution class (‘beta’ or ‘normal’) as input and plots the posteriors together with the prior.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BayesianInferencePlot</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Easily implement the MCMC and VI inference for a given instance of </span>
<span class="sd">    BayesianInference class and plot the prior together with multiple posteriors</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    θ : float.</span>
<span class="sd">        the true DGP parameter</span>
<span class="sd">    N_list : list.</span>
<span class="sd">        a list of sample size</span>
<span class="sd">    bayesian_model : BayesianInference.</span>
<span class="sd">        a class initiated using create_bayesian_inference()</span>
<span class="sd">    binwidth : float.</span>
<span class="sd">        plotting parameter for histogram bin width</span>
<span class="sd">    linewidth : float.</span>
<span class="sd">        plotting parameter for line width</span>
<span class="sd">    colorlist : list.</span>
<span class="sd">        list of colors for plotting</span>
<span class="sd">    N_max : int.</span>
<span class="sd">        maximum sample size</span>
<span class="sd">    data : np.ndarray.</span>
<span class="sd">        generated data array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">θ</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">N_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">bayesian_model</span><span class="p">:</span> <span class="n">BayesianInference</span>
    <span class="n">binwidth</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">linewidth</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">colorlist</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">N_max</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_bayesian_inference_plot</span><span class="p">(</span>
    <span class="n">θ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">N_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">bayesian_model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">binwidth</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BayesianInferencePlot</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Factory function to create a BayesianInferencePlot instance&quot;&quot;&quot;</span>

    <span class="n">colorlist</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="n">n_colors</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">N_list</span><span class="p">))</span>
    <span class="n">N_max</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">N_list</span><span class="p">))</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">simulate_draw</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">N_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">BayesianInferencePlot</span><span class="p">(</span>
        <span class="n">θ</span><span class="o">=</span><span class="n">θ</span><span class="p">,</span>
        <span class="n">N_list</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">N_list</span><span class="p">)),</span>
        <span class="n">bayesian_model</span><span class="o">=</span><span class="n">bayesian_model</span><span class="p">,</span>
        <span class="n">binwidth</span><span class="o">=</span><span class="n">binwidth</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">,</span>
        <span class="n">colorlist</span><span class="o">=</span><span class="n">colorlist</span><span class="p">,</span>
        <span class="n">N_max</span><span class="o">=</span><span class="n">N_max</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">mcmc_plot</span><span class="p">(</span>
    <span class="n">plot_model</span><span class="p">:</span> <span class="n">BayesianInferencePlot</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="c1"># plot prior</span>
    <span class="n">prior_sample</span> <span class="o">=</span> <span class="n">show_prior</span><span class="p">(</span>
        <span class="n">plot_model</span><span class="o">.</span><span class="n">bayesian_model</span><span class="p">,</span> <span class="n">disp_plot</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">prior_sample</span><span class="p">,</span>
        <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span>
        <span class="n">binwidth</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">binwidth</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#4C4E52&quot;</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">linewidth</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># plot posteriors</span>
    <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">plot_model</span><span class="o">.</span><span class="n">N_list</span><span class="p">):</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">mcmc_sampling</span><span class="p">(</span>
            <span class="n">plot_model</span><span class="o">.</span><span class="n">bayesian_model</span><span class="p">,</span> 
            <span class="n">plot_model</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> 
            <span class="n">num_samples</span><span class="p">,</span> 
            <span class="n">num_warmup</span>
        <span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span>
            <span class="n">samples</span><span class="p">,</span>
            <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span>
            <span class="n">binwidth</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">binwidth</span><span class="p">,</span>
            <span class="n">linewidth</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">linewidth</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">colorlist</span><span class="p">[</span><span class="nb">id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Posterior with $n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">svi_fitting</span><span class="p">(</span><span class="n">guide_dist</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit the beta/truncnormal curve using parameters trained by SVI.&quot;&quot;&quot;</span>
    <span class="c1"># create x axis</span>
    <span class="n">xaxis</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">guide_dist</span> <span class="o">==</span> <span class="s2">&quot;beta&quot;</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xaxis</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;alpha_q&quot;</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;beta_q&quot;</span><span class="p">])</span>

    <span class="k">elif</span> <span class="n">guide_dist</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
        <span class="c1"># rescale upper/lower bound. See Scipy&#39;s truncnorm doc</span>
        <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">lower</span> <span class="o">-</span> <span class="n">loc</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span><span class="p">,</span> <span class="p">(</span><span class="n">upper</span> <span class="o">-</span> <span class="n">loc</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">truncnorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
            <span class="n">xaxis</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">xaxis</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">svi_plot</span><span class="p">(</span>
    <span class="n">plot_model</span><span class="p">:</span> <span class="n">BayesianInferencePlot</span><span class="p">,</span> <span class="n">guide_dist</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">2000</span>
<span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="c1"># plot prior</span>
    <span class="n">prior_sample</span> <span class="o">=</span> <span class="n">show_prior</span><span class="p">(</span><span class="n">plot_model</span><span class="o">.</span><span class="n">bayesian_model</span><span class="p">,</span> <span class="n">disp_plot</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">prior_sample</span><span class="p">,</span>
        <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span>
        <span class="n">binwidth</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">binwidth</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#4C4E52&quot;</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">linewidth</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># plot posteriors</span>
    <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">plot_model</span><span class="o">.</span><span class="n">N_list</span><span class="p">):</span>
        <span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">losses</span><span class="p">)</span> <span class="o">=</span> <span class="n">svi_run</span><span class="p">(</span>
            <span class="n">plot_model</span><span class="o">.</span><span class="n">bayesian_model</span><span class="p">,</span> <span class="n">plot_model</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">guide_dist</span><span class="p">,</span> <span class="n">n_steps</span>
        <span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">svi_fitting</span><span class="p">(</span><span class="n">guide_dist</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">plot_model</span><span class="o">.</span><span class="n">colorlist</span><span class="p">[</span><span class="nb">id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Posterior with $n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s set some parameters that we’ll use in all of the examples below.</p>
<p>To save computer time at first, notice that we’ll set <code class="docutils literal notranslate"><span class="pre">mcmc_num_samples</span> <span class="pre">=</span> <span class="pre">2000</span></code> and <code class="docutils literal notranslate"><span class="pre">svi_num_steps</span> <span class="pre">=</span> <span class="pre">5000</span></code>.</p>
<p>(Later, to increase accuracy of approximations, we’ll want to increase these.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
<span class="n">mcmc_num_samples</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">svi_num_steps</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="c1"># θ is the data generating process</span>
<span class="n">true_θ</span> <span class="o">=</span> <span class="mf">0.8</span>
</pre></div>
</div>
</div>
</div>
<section id="beta-prior-and-posteriors">
<h3><span class="section-number">18.5.1. </span>Beta prior and posteriors:<a class="headerlink" href="#beta-prior-and-posteriors" title="Link to this heading">#</a></h3>
<p>Let’s compare outcomes when we use a Beta prior.</p>
<p>For the same Beta prior, we shall</p>
<ul class="simple">
<li><p>compute posteriors analytically</p></li>
<li><p>compute posteriors using MCMC using <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>.</p></li>
<li><p>compute posteriors using VI using <code class="docutils literal notranslate"><span class="pre">numpyro</span></code>.</p></li>
</ul>
<p>Let’s start with the analytical method that we described in this <a class="reference internal" href="prob_meaning.html"><span class="doc">Two Meanings of Probability</span></a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first examine Beta prior</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">)</span>

<span class="n">beta_plot</span> <span class="o">=</span> <span class="n">create_bayesian_inference_plot</span><span class="p">(</span><span class="n">true_θ</span><span class="p">,</span> <span class="n">num_list</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># plot analytical Beta prior and posteriors</span>
<span class="n">xaxis</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_prior</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xaxis</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="c1"># plot analytical beta prior</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xaxis</span><span class="p">,</span> <span class="n">y_prior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Analytical Beta prior&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#4C4E52&quot;</span><span class="p">)</span>

<span class="n">data</span><span class="p">,</span> <span class="n">colorlist</span><span class="p">,</span> <span class="n">N_list</span> <span class="o">=</span> <span class="n">beta_plot</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">beta_plot</span><span class="o">.</span><span class="n">colorlist</span><span class="p">,</span> <span class="n">beta_plot</span><span class="o">.</span><span class="n">N_list</span>

<span class="c1"># Plot analytical beta posteriors</span>
<span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N_list</span><span class="p">):</span>
    <span class="n">func</span> <span class="o">=</span> <span class="n">analytical_beta_posterior</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">α0</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">β0</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y_posterior</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xaxis</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">xaxis</span><span class="p">,</span>
        <span class="n">y_posterior</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">colorlist</span><span class="p">[</span><span class="nb">id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Analytical Beta posterior with $n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="fig-analytical">
<img alt="_images/f17c6bc6dfeca32941db1fbdac09670ff8e76076959e41941a7bf423a30926a8.png" src="_images/f17c6bc6dfeca32941db1fbdac09670ff8e76076959e41941a7bf423a30926a8.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.5 </span><span class="caption-text">Analytical density (Beta prior)</span><a class="headerlink" href="#fig-analytical" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>Now let’s use MCMC while still using a beta prior.</p>
<p>We’ll do this for both MCMC and VI.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mcmc_plot</span><span class="p">(</span>
    <span class="n">beta_plot</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">mcmc_num_samples</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="fig-mcmc-beta">
<img alt="_images/2ffda6270a32c6dc012d119c75e54626305b58fe2ff214afa7eb0486486388a0.png" src="_images/2ffda6270a32c6dc012d119c75e54626305b58fe2ff214afa7eb0486486388a0.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.6 </span><span class="caption-text">MCMC density (Beta prior)</span><a class="headerlink" href="#fig-mcmc-beta" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svi_plot</span><span class="p">(</span>
    <span class="n">beta_plot</span><span class="p">,</span> <span class="n">guide_dist</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<figure class="align-default" id="fig-svi-beta-beta">
<img alt="_images/1d11969a72b66aee59de70f5d3910fa9c04bbcc96344d66d24362c188420b067.png" src="_images/1d11969a72b66aee59de70f5d3910fa9c04bbcc96344d66d24362c188420b067.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.7 </span><span class="caption-text">SVI density (Beta prior, Beta guide)</span><a class="headerlink" href="#fig-svi-beta-beta" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>Here the MCMC approximation looks good.</p>
<p>But the VI approximation doesn’t look so good.</p>
<ul class="simple">
<li><p>even though we use the beta distribution as our guide, the VI approximated posterior distributions do not closely resemble the posteriors that we had just computed analytically.</p></li>
</ul>
<p>(Here, our initial parameter for Beta guide is (0.5, 0.5).)</p>
<p>But if we increase the number of steps from 5000 to 100000 in VI as we now shall do, we’ll get VI-approximated posteriors
that will be more accurate, as we shall see next.</p>
<p>(Increasing the step size increases computational time though).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svi_plot</span><span class="p">(</span>
    <span class="n">beta_plot</span><span class="p">,</span> <span class="n">guide_dist</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">100000</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cad955a1aa7319a5f721c9c3e355ac6810825e6fb24d633589c51978d6af579c.png" src="_images/cad955a1aa7319a5f721c9c3e355ac6810825e6fb24d633589c51978d6af579c.png" />
</div>
</div>
</section>
</section>
<section id="non-conjugate-prior-distributions">
<h2><span class="section-number">18.6. </span>Non-conjugate prior distributions<a class="headerlink" href="#non-conjugate-prior-distributions" title="Link to this heading">#</a></h2>
<p>Having assured ourselves that our MCMC and VI methods can work well when we have a conjugate prior and so can also compute analytically, we
next proceed to situations in which our prior is not a beta distribution, so we don’t have a conjugate prior.</p>
<p>So we will have non-conjugate priors and are cast into situations in which we can’t calculate posteriors analytically.</p>
<section id="markov-chain-monte-carlo">
<h3><span class="section-number">18.6.1. </span>Markov chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Link to this heading">#</a></h3>
<p>First, we implement and display MCMC.</p>
<p>We first initialize the <code class="docutils literal notranslate"><span class="pre">BayesianInference</span></code> classes and then can directly call <code class="docutils literal notranslate"><span class="pre">BayesianInferencePlot</span></code> to plot both MCMC and SVI approximating posteriors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize BayesianInference classes</span>
<span class="c1"># Try uniform</span>
<span class="n">std_uniform</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">)</span>
<span class="n">uniform</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">)</span>

<span class="c1"># Try truncated log normal</span>
<span class="n">lognormal</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;lognormal&quot;</span><span class="p">)</span>

<span class="c1"># Try Von Mises</span>
<span class="n">vonmises</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;vonMises&quot;</span><span class="p">)</span>

<span class="c1"># Try Laplace</span>
<span class="n">laplace</span> <span class="o">=</span> <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;laplace&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To conduct our experiments more concisely, here we define two experiment functions that will print the model information and plot the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_mcmc_experiment</span><span class="p">(</span>
    <span class="n">bayesian_model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">num_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_warmup</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to run and plot MCMC experiments for a given Bayesian model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;=======INFO=======</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Parameters: </span><span class="si">{</span><span class="n">bayesian_model</span><span class="o">.</span><span class="n">param</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Prior Dist: </span><span class="si">{</span><span class="n">bayesian_model</span><span class="o">.</span><span class="n">name_dist</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">description</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">description</span><span class="p">)</span>
    
    <span class="n">plot_model</span> <span class="o">=</span> <span class="n">create_bayesian_inference_plot</span><span class="p">(</span>
        <span class="n">true_θ</span><span class="p">,</span> <span class="n">num_list</span><span class="p">,</span> <span class="n">bayesian_model</span>
    <span class="p">)</span>
    <span class="n">mcmc_plot</span><span class="p">(</span><span class="n">plot_model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="n">num_warmup</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">bayesian_model</span><span class="p">:</span> <span class="n">BayesianInference</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">num_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">guide_dist</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to run and plot SVI experiments for a given Bayesian model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;=======INFO=======</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Parameters: </span><span class="si">{</span><span class="n">bayesian_model</span><span class="o">.</span><span class="n">param</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Prior Dist: </span><span class="si">{</span><span class="n">bayesian_model</span><span class="o">.</span><span class="n">name_dist</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">description</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">description</span><span class="p">)</span>

    <span class="n">plot_model</span> <span class="o">=</span> <span class="n">create_bayesian_inference_plot</span><span class="p">(</span>
        <span class="n">true_θ</span><span class="p">,</span> <span class="n">num_list</span><span class="p">,</span> <span class="n">bayesian_model</span>
    <span class="p">)</span>
    <span class="n">svi_plot</span><span class="p">(</span><span class="n">plot_model</span><span class="p">,</span> <span class="n">guide_dist</span><span class="o">=</span><span class="n">guide_dist</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uniform</span>
<span class="n">plot_mcmc_experiment</span><span class="p">(</span>
    <span class="n">std_uniform</span><span class="p">,</span> 
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="n">mcmc_num_samples</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0, 1)
Prior Dist: uniform
</pre></div>
</div>
<figure class="align-default" id="fig-mcmc-stduniform">
<img alt="_images/b911f850f1ad05e310e4de90d243c9b24629ec95c77cd2466dbc4d38d5a10c2c.png" src="_images/b911f850f1ad05e310e4de90d243c9b24629ec95c77cd2466dbc4d38d5a10c2c.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.8 </span><span class="caption-text">MCMC density (uniform prior)</span><a class="headerlink" href="#fig-mcmc-stduniform" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_mcmc_experiment</span><span class="p">(</span>
    <span class="n">uniform</span><span class="p">,</span> 
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="n">mcmc_num_samples</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0.2, 0.7)
Prior Dist: uniform
</pre></div>
</div>
<figure class="align-default" id="fig-mcmc-uniform">
<img alt="_images/76d07021897ae509ef6aa921e9701e9e0de57ba71b22db59182d7fb3aaf2b274.png" src="_images/76d07021897ae509ef6aa921e9701e9e0de57ba71b22db59182d7fb3aaf2b274.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.9 </span><span class="caption-text">MCMC density (uniform prior)</span><a class="headerlink" href="#fig-mcmc-uniform" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>In the situation depicted above, we have assumed a <span class="math notranslate nohighlight">\(Uniform(\underline{\theta}, \overline{\theta})\)</span> prior that puts zero probability outside a bounded support that excludes the true value.</p>
<p>Consequently, the posterior cannot put positive probability above <span class="math notranslate nohighlight">\(\overline{\theta}\)</span> or below <span class="math notranslate nohighlight">\(\underline{\theta}\)</span>.</p>
<p>Note how when the true data-generating <span class="math notranslate nohighlight">\(\theta\)</span> is located at <span class="math notranslate nohighlight">\(0.8\)</span> as it is here, when <span class="math notranslate nohighlight">\(n\)</span> gets large, the posterior concentrates on the upper bound of the support of the prior, <span class="math notranslate nohighlight">\(0.7\)</span> here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># log normal</span>
<span class="n">plot_mcmc_experiment</span><span class="p">(</span>
    <span class="n">lognormal</span><span class="p">,</span> 
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="n">mcmc_num_samples</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0, 2)
Prior Dist: lognormal
</pre></div>
</div>
<figure class="align-default" id="fig-mcmc-lognormal">
<img alt="_images/219ee1a9f81865a719df929a3ca35957515f97fd260ea01cd517631a6ddc5d9a.png" src="_images/219ee1a9f81865a719df929a3ca35957515f97fd260ea01cd517631a6ddc5d9a.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.10 </span><span class="caption-text">MCMC density (log normal prior)</span><a class="headerlink" href="#fig-mcmc-lognormal" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># von Mises</span>
<span class="n">plot_mcmc_experiment</span><span class="p">(</span>
    <span class="n">vonmises</span><span class="p">,</span> 
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="n">mcmc_num_samples</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NOTE: Shifted von Mises&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: 10
Prior Dist: vonMises

NOTE: Shifted von Mises
</pre></div>
</div>
<figure class="align-default" id="fig-mcmc-vonmises">
<img alt="_images/2bf42193444dc96082e26ab47e9e1c0898e945daa2df1bea1ce51a5dd35aa53d.png" src="_images/2bf42193444dc96082e26ab47e9e1c0898e945daa2df1bea1ce51a5dd35aa53d.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.11 </span><span class="caption-text">MCMC density (von Mises prior)</span><a class="headerlink" href="#fig-mcmc-vonmises" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Laplace</span>
<span class="n">plot_mcmc_experiment</span><span class="p">(</span>
    <span class="n">laplace</span><span class="p">,</span> 
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="n">mcmc_num_samples</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0.5, 0.07)
Prior Dist: laplace
</pre></div>
</div>
<figure class="align-default" id="fig-mcmc-laplace">
<img alt="_images/1101248bc0a1e3ed8c6dc96749d42a93316a4ed00566357edbcac3d1b94f234c.png" src="_images/1101248bc0a1e3ed8c6dc96749d42a93316a4ed00566357edbcac3d1b94f234c.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.12 </span><span class="caption-text">MCMC density (Laplace prior)</span><a class="headerlink" href="#fig-mcmc-laplace" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</section>
<section id="id1">
<h3><span class="section-number">18.6.2. </span>Variational inference<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>To get more accuracy we will now increase the number of steps for Variational Inference (VI)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svi_num_steps</span> <span class="o">=</span> <span class="mi">50000</span>
</pre></div>
</div>
</div>
</div>
<section id="vi-with-a-truncated-normal-guide">
<h4><span class="section-number">18.6.2.1. </span>VI with a truncated normal guide<a class="headerlink" href="#vi-with-a-truncated-normal-guide" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uniform</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">create_bayesian_inference</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name_dist</span><span class="o">=</span><span class="s2">&quot;uniform&quot;</span><span class="p">),</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;normal&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0, 1)
Prior Dist: uniform
</pre></div>
</div>
<figure class="align-default" id="fig-svi-uniform-normal">
<img alt="_images/280b08b4881865249fabe1e0c941b7135edeec6030391791622215b70290e652.png" src="_images/280b08b4881865249fabe1e0c941b7135edeec6030391791622215b70290e652.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.13 </span><span class="caption-text">SVI density (uniform prior, normal guide)</span><a class="headerlink" href="#fig-svi-uniform-normal" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># log normal</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">lognormal</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;normal&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0, 2)
Prior Dist: lognormal
</pre></div>
</div>
<figure class="align-default" id="fig-svi-lognormal-normal">
<img alt="_images/1763d1db8caa6d74d8d967af3a1f53d76778eb5cb9249c60c8b8f7d19a5737e4.png" src="_images/1763d1db8caa6d74d8d967af3a1f53d76778eb5cb9249c60c8b8f7d19a5737e4.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.14 </span><span class="caption-text">SVI density (log normal prior, normal guide)</span><a class="headerlink" href="#fig-svi-lognormal-normal" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Laplace</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">laplace</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;normal&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0.5, 0.07)
Prior Dist: laplace
</pre></div>
</div>
<figure class="align-default" id="fig-svi-laplace-normal">
<img alt="_images/cc7134b8fff7b8aaaf93609a93d79f9ae4a996b7714b6e141525684ff9e061ef.png" src="_images/cc7134b8fff7b8aaaf93609a93d79f9ae4a996b7714b6e141525684ff9e061ef.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.15 </span><span class="caption-text">SVI density (Laplace prior, normal guide)</span><a class="headerlink" href="#fig-svi-laplace-normal" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</section>
<section id="variational-inference-with-a-beta-guide-distribution">
<h4><span class="section-number">18.6.2.2. </span>Variational inference with a Beta guide distribution<a class="headerlink" href="#variational-inference-with-a-beta-guide-distribution" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># uniform</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">std_uniform</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;beta&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0, 1)
Prior Dist: uniform
</pre></div>
</div>
<figure class="align-default" id="fig-svi-uniform-beta">
<img alt="_images/e860202fc1c8652d59522989fdb6b0afc00017bf15cb69ccc4c0632cfb1e76e2.png" src="_images/e860202fc1c8652d59522989fdb6b0afc00017bf15cb69ccc4c0632cfb1e76e2.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.16 </span><span class="caption-text">SVI density (uniform prior, Beta guide)</span><a class="headerlink" href="#fig-svi-uniform-beta" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># log normal</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">lognormal</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;beta&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0, 2)
Prior Dist: lognormal
</pre></div>
</div>
<figure class="align-default" id="fig-svi-lognormal-beta">
<img alt="_images/434e7f9943011e6e3561e2ab96ada3a5c08bf8e31339877f75f55849638e4f5c.png" src="_images/434e7f9943011e6e3561e2ab96ada3a5c08bf8e31339877f75f55849638e4f5c.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.17 </span><span class="caption-text">SVI density (log normal prior, Beta guide)</span><a class="headerlink" href="#fig-svi-lognormal-beta" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># von Mises</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">vonmises</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;beta&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Shifted von Mises&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: 10
Prior Dist: vonMises
Shifted von Mises
</pre></div>
</div>
<figure class="align-default" id="fig-svi-vonmises-beta">
<img alt="_images/c10c70313273c1af936dcfd03b5056689c1f55f48b4933c2cbc4b4723c55341e.png" src="_images/c10c70313273c1af936dcfd03b5056689c1f55f48b4933c2cbc4b4723c55341e.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.18 </span><span class="caption-text">SVI density (von Mises prior, Beta guide)</span><a class="headerlink" href="#fig-svi-vonmises-beta" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Laplace</span>
<span class="n">plot_svi_experiment</span><span class="p">(</span>
    <span class="n">laplace</span><span class="p">,</span>
    <span class="n">true_θ</span><span class="p">,</span> 
    <span class="n">num_list</span><span class="p">,</span> 
    <span class="s2">&quot;beta&quot;</span><span class="p">,</span> 
    <span class="n">svi_num_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=======INFO=======
Parameters: (0.5, 0.07)
Prior Dist: laplace
</pre></div>
</div>
<figure class="align-default" id="fig-svi-laplace-beta">
<img alt="_images/28159c9aa4a5c08900b7e9322d24a540f57324f0944b5b368cbb20f7ce5da756.png" src="_images/28159c9aa4a5c08900b7e9322d24a540f57324f0944b5b368cbb20f7ce5da756.png" />
<figcaption>
<p><span class="caption-number">Fig. 18.19 </span><span class="caption-text">SVI density (Laplace prior, Beta guide)</span><a class="headerlink" href="#fig-svi-laplace-beta" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                    <p>A theme by <a href="https://quantecon.org">QuantEcon</a></p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="sir_model.html">
   1. Modeling COVID 19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   2. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="qr_decomp.html">
   3. QR Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eig_circulant.html">
   4. Circulant Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svd_intro.html">
   5. Singular Value Decomposition (SVD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="var_dmd.html">
   6. VARs and DMDs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newton_method.html">
   7. Using Newton’s Method to Solve Economic Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementary Statistics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="prob_matrix.html">
   8. Elementary Probability with Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stats_examples.html">
   9. Some Probability Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   10. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prob_meaning.html">
   11. Two Meanings of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multi_hyper.html">
   12. Multivariate Hypergeometric Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_normal.html">
   13. Multivariate Normal Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hoist_failure.html">
   14. Fault Tree Uncertainties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="back_prop.html">
   15. Introduction to Artificial Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rand_resp.html">
   16. Randomized Response Surveys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="util_rand_resp.html">
   17. Expected Utilities of Random Responses
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayes Law
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   18. Non-Conjugate Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_bayes.html">
   19. Posterior Distributions for  AR(1) Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_turningpts.html">
   20. Forecasting  an AR(1) Process
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics and Information
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="divergence_measures.html">
   21. Statistical Divergence Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process.html">
   22. Likelihood Ratio Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process_2.html">
   23. Heterogeneous Beliefs and Financial Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_var.html">
   24. Likelihood Processes For VAR Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="imp_sample.html">
   25. Mean of a Likelihood Ratio Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman.html">
   26. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman_2.html">
   27. A Bayesian Formulation of Friedman and Wald’s Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exchangeable.html">
   28. Exchangeability and Bayesian Updating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_bayes.html">
   29. Likelihood Ratio Processes and Bayesian Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mix_model.html">
   30. Incorrect Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="navy_captain.html">
   31. Bayesian versus Frequentist Decision Rules
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Programming
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="opt_transport.html">
   32. Optimal Transport
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="von_neumann_model.html">
   33. Von Neumann Growth Model (and a Generalization)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="finite_markov.html">
   34. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inventory_dynamics.html">
   35. Inventory Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_models.html">
   36. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="samuelson.html">
   37. Samuelson Multiplier-Accelerator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kesten_processes.html">
   38. Kesten Processes and Firm Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wealth_dynamics.html">
   39. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman.html">
   40. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman_2.html">
   41. Another Look at the Kalman Filter
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Search
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model.html">
   42. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_separation.html">
   43. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_sep_markov.html">
   44. Job Search III: Search with Separation and Markov Wages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_fitted_vfi.html">
   45. Job Search IV: Fitted Value Function Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_persist_trans.html">
   46. Job Search V: Persistent and Transitory Wage Shocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="career.html">
   47. Job Search VI: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jv.html">
   48. Job Search VII: On-the-Job Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="odu.html">
   49. Job Search VIII: Search with Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_q.html">
   50. Job Search IX: Search with Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Household Problems
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating.html">
   51. Cake Eating I: Introduction to Optimal Saving
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_numerical.html">
   52. Cake Eating II: Numerical Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_stochastic.html">
   53. Cake Eating III: Stochastic Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_time_iter.html">
   54. Cake Eating IV: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_egm.html">
   55. Cake Eating V: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_egm_jax.html">
   56. Cake Eating VI: EGM with JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp.html">
   57. The Income Fluctuation Problem I: Basic Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp_advanced.html">
   58. The Income Fluctuation Problem II: Stochastic Returns on Assets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lqcontrol.html">
   59. LQ Control: Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lagrangian_lqdp.html">
   60. Lagrangian for LQ Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cross_product_trick.html">
   61. Eliminating Cross Products
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income.html">
   62. The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income_cons.html">
   63. Permanent Income II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lq_inventories.html">
   64. Production Smoothing via Inventories
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Optimal Growth
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_1.html">
   65. Cass-Koopmans Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_2.html">
   66. Cass-Koopmans Competitive Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_fiscal.html">
   67. Cass-Koopmans Model with Distorting Taxes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_fiscal_2.html">
   68. Two-Country Model with Distorting Taxes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ak2.html">
   69. Transitions in an Overlapping Generations Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lake_model.html">
   70. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="endogenous_lake.html">
   71. Lake Model with an Endogenous Job Finding Rate
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rational_expectations.html">
   72. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="re_with_feedback.html">
   73. Stability in Linear Rational Expectations Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_perf.html">
   74. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncertainty_traps.html">
   75. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aiyagari.html">
   76. The Aiyagari Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ak_aiyagari.html">
   77. A Long-Lived, Heterogeneous Agent, Overlapping Generations Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markov_asset.html">
   78. Asset Pricing: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ge_arrow.html">
   79. Competitive Equilibria with Arrow Securities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrison_kreps.html">
   80. Heterogeneous Beliefs and Bubbles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="morris_learn.html">
   81. Speculative Behavior with Bayesian Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data and Empirics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_panel.html">
   82. Pandas for Panel Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ols.html">
   83. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mle.html">
   84. Maximum Likelihood Estimation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Auctions
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="two_auctions.html">
   85. First-Price and Second-Price Auctions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="house_auction.html">
   86. Multiple Good Allocation Mechanisms
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   87. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   88. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   89. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                </ul>

                <ul class="qe-toolbar__links">
                    <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search..." aria-label="Search..." autocomplete="off" accesskey="k">
                            <i data-feather="search" id="search-icon"></i>
                        </form>
                    </li>
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/bayes_nonconj.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <!--
                    # Enable if looking for link to specific document hosted on GitHub
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst/blob/main/lectures/bayes_nonconj.md" download><i data-feather="github"></i></a></li>
                    -->
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="/_pdf/quantecon-python.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/main/lectures/bayes_nonconj.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python.notebooks" data-urlpath="tree/lecture-python.notebooks/lectures/bayes_nonconj.ipynb" data-branch=main>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/main/lectures/bayes_nonconj.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "bayes_nonconj";
                const repoURL = "https://github.com/QuantEcon/lecture-python.notebooks";
                const urlPath = "tree/lecture-python.notebooks/lectures/bayes_nonconj.ipynb";
                const branch = "main"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
  </body>
</html>