
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>38. A Problem that Stumped Milton Friedman &#8212; Quantitative Economics with Python</title>
    <link rel="stylesheet" href="_static/quantecon-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/quantecon-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/quantecon-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://python.quantecon.org/wald_friedman.html" />
    <link rel="shortcut icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="39. Exchangeability and Bayesian Updating" href="exchangeable.html" />
    <link rel="prev" title="37. Likelihood Ratio Processes" href="likelihood_ratio_process.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="A Problem that Stumped Milton Friedman"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="A Problem that Stumped Milton Friedman" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python.quantecon.org/wald_friedman.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Quantitative Economics with Python" />

<meta name="theme-color" content="#ffffff" />


  </head>
<body>


    <span id="top"></span>

    <div class="wrapper">

        <div class="main">

            <div class="page">

                <div class="page__toc">

                    <div class="inner">

                        
                        <div class="page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="page__toc-nav">

                            <ul class="nav section-nav flex-column">
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#overview" class="nav-link">Overview</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#origin-of-the-problem" class="nav-link">Origin of the Problem</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#a-dynamic-programming-approach" class="nav-link">A Dynamic Programming Approach</a><ul class="nav section-nav flex-column">
                                        
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#losses-and-costs" class="nav-link">Losses and Costs</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#digression-on-type-i-and-type-ii-errors" class="nav-link">Digression on Type I and Type II Errors</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#intuition" class="nav-link">Intuition</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#a-bellman-equation" class="nav-link">A Bellman Equation</a>
                                </li>
                                
                                    </ul>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#implementation" class="nav-link">Implementation</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#analysis" class="nav-link">Analysis</a><ul class="nav section-nav flex-column">
                                        
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#value-function" class="nav-link">Value Function</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#simulations" class="nav-link">Simulations</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#comparative-statics" class="nav-link">Comparative Statics</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h3">
                                    <a href="#a-notebook-implementation" class="nav-link">A Notebook Implementation</a>
                                </li>
                                
                                    </ul>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#comparison-with-neyman-pearson-formulation" class="nav-link">Comparison with Neyman-Pearson Formulation</a>
                                </li>
                                
                                <li class="nav-item toc-entry toc-h2">
                                    <a href="#sequels" class="nav-link">Sequels</a>
                                </li>
                                
                            </ul>

                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo" alt="logo"></a>
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="page__header">

                    <div class="page__header-copy">

                        <p class="page__header-heading"><a href="intro.html">Quantitative Economics with Python</a></p>

                        <p class="page__header-subheading">A Problem that Stumped Milton Friedman</p>

                    </div>

                    <p class="page__header-authors">Thomas J. Sargent & John Stachurski</p>

                </div> <!-- .page__header -->



                
                <main class="page__content" role="main">
                    
                    <div>
                        
  <div id="qe-notebook-header" align="right" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" width="250px" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div><div class="section" id="a-problem-that-stumped-milton-friedman">
<h1><a class="toc-backref" href="#id9"><span class="section-number">38. </span><span class="target" id="index-0"></span>A Problem that Stumped Milton Friedman</a><a class="headerlink" href="#a-problem-that-stumped-milton-friedman" title="Permalink to this headline">¶</a></h1>
<p>(and that Abraham Wald solved by inventing sequential analysis)</p>
<div class="contents topic" id="contents">
<span id="index-1"></span><p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#a-problem-that-stumped-milton-friedman" id="id9">A Problem that Stumped Milton Friedman</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id10">Overview</a></p></li>
<li><p><a class="reference internal" href="#origin-of-the-problem" id="id11">Origin of the Problem</a></p></li>
<li><p><a class="reference internal" href="#a-dynamic-programming-approach" id="id12">A Dynamic Programming Approach</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id13">Implementation</a></p></li>
<li><p><a class="reference internal" href="#analysis" id="id14">Analysis</a></p></li>
<li><p><a class="reference internal" href="#comparison-with-neyman-pearson-formulation" id="id15">Comparison with Neyman-Pearson Formulation</a></p></li>
<li><p><a class="reference internal" href="#sequels" id="id16">Sequels</a></p></li>
</ul>
</li>
</ul>
</div>
<p>In addition to what’s in Anaconda, this lecture will need the following libraries:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>conda install -y quantecon
<span class="o">!</span>pip install interpolation
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting package metadata (current_repodata.json): - 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>done
Solving environment: \ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>\ 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>done
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># All requested packages already installed.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: interpolation in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (2.2.1)
Requirement already satisfied: tempita&gt;=0.5.2 in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (from interpolation) (0.5.2)
Requirement already satisfied: numba&gt;=0.47 in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (from interpolation) (0.51.2)
Requirement already satisfied: scipy&gt;=1.4.1 in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (from interpolation) (1.5.2)
Requirement already satisfied: numpy&gt;=1.18.1 in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (from interpolation) (1.19.2)
Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (from numba&gt;=0.47-&gt;interpolation) (0.34.0)
Requirement already satisfied: setuptools in /usr/share/miniconda3/envs/lecture-python/lib/python3.8/site-packages (from numba&gt;=0.47-&gt;interpolation) (50.3.1.post20201107)
</pre></div>
</div>
</div>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id10"><span class="section-number">38.1. </span>Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This lecture describes a statistical decision problem encountered  by Milton
Friedman and W. Allen Wallis during World War II when they were analysts at
the U.S. Government’s  Statistical Research Group at Columbia University.</p>
<p>This problem led Abraham Wald <span id="id1">[<a class="reference internal" href="zreferences.html#id69"><span>Wal47</span></a>]</span> to formulate <strong>sequential analysis</strong>,
an approach to statistical decision problems intimately related to dynamic programming.</p>
<p>In this lecture, we apply dynamic programming algorithms to Friedman and Wallis and Wald’s problem.</p>
<p>Key ideas in play will be:</p>
<ul class="simple">
<li><p>Bayes’ Law</p></li>
<li><p>Dynamic programming</p></li>
<li><p>Type I and type II statistical errors</p>
<ul>
<li><p>a type I error occurs when you reject a null hypothesis that is true</p></li>
<li><p>a type II error is when you accept a null hypothesis that is false</p></li>
</ul>
</li>
<li><p>Abraham Wald’s <strong>sequential probability ratio test</strong></p></li>
<li><p>The <strong>power</strong> of a statistical test</p></li>
<li><p>The <strong>critical region</strong> of a statistical test</p></li>
<li><p>A <strong>uniformly most powerful test</strong></p></li>
</ul>
<p>We’ll begin with some imports:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">prange</span><span class="p">,</span> <span class="n">float64</span><span class="p">,</span> <span class="n">int64</span>
<span class="kn">from</span> <span class="nn">numba.experimental</span> <span class="kn">import</span> <span class="n">jitclass</span>
<span class="kn">from</span> <span class="nn">interpolation</span> <span class="kn">import</span> <span class="n">interp</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">gamma</span>
</pre></div>
</div>
</div>
</div>
<p>This lecture uses ideas studied in <a class="reference internal" href="likelihood_ratio_process.html"><span class="doc">this lecture</span></a>, <a class="reference internal" href="likelihood_bayes.html"><span class="doc">this lecture</span></a>.
and <a class="reference internal" href="exchangeable.html"><span class="doc">this lecture</span></a>.</p>
</div>
<div class="section" id="origin-of-the-problem">
<h2><a class="toc-backref" href="#id11"><span class="section-number">38.2. </span>Origin of the Problem</a><a class="headerlink" href="#origin-of-the-problem" title="Permalink to this headline">¶</a></h2>
<p>On pages 137-139 of his 1998 book <em>Two Lucky People</em> with Rose Friedman <span id="id2">[<a class="reference internal" href="zreferences.html#id64"><span>FF98</span></a>]</span>,
Milton Friedman described a problem presented to him and Allen Wallis
during World War II, when they worked at the US Government’s
Statistical Research Group at Columbia University.</p>
<p>Let’s listen to Milton Friedman tell us what happened</p>
<blockquote>
<div><p>In order to understand the story, it is necessary to have an idea of a
simple statistical problem, and of the standard procedure for dealing
with it. The actual problem out of which sequential analysis grew will
serve. The Navy has two alternative designs (say A and B) for a
projectile. It wants to determine which is superior. To do so it
undertakes a series of paired firings. On each round, it assigns the
value 1 or 0 to A accordingly as its performance is superior or inferior
to that of B and conversely 0 or 1 to B. The Navy asks the statistician
how to conduct the test and how to analyze the results.</p>
</div></blockquote>
<blockquote>
<div><p>The standard statistical answer was to specify a number of firings (say
1,000) and a pair of percentages (e.g., 53% and 47%) and tell the client
that if A receives a 1 in more than 53% of the firings, it can be
regarded as superior; if it receives a 1 in fewer than 47%, B can be
regarded as superior; if the percentage is between 47% and 53%, neither
can be so regarded.</p>
</div></blockquote>
<blockquote>
<div><p>When Allen Wallis was discussing such a problem with (Navy) Captain
Garret L. Schyler, the captain objected that such a test, to quote from
Allen’s account, may prove wasteful. If a wise and seasoned ordnance
officer like Schyler were on the premises, he would see after the first
few thousand or even few hundred [rounds] that the experiment need not
be completed either because the new method is obviously inferior or
because it is obviously superior beyond what was hoped for
<span class="math notranslate nohighlight">\(\ldots\)</span>.</p>
</div></blockquote>
<p>Friedman and Wallis struggled with the problem but, after realizing that
they were not able to solve it,  described the problem to  Abraham Wald.</p>
<p>That started Wald on the path that led him  to <em>Sequential Analysis</em> <span id="id3">[<a class="reference internal" href="zreferences.html#id69"><span>Wal47</span></a>]</span>.</p>
<p>We’ll formulate the problem using dynamic programming.</p>
</div>
<div class="section" id="a-dynamic-programming-approach">
<h2><a class="toc-backref" href="#id12"><span class="section-number">38.3. </span>A Dynamic Programming Approach</a><a class="headerlink" href="#a-dynamic-programming-approach" title="Permalink to this headline">¶</a></h2>
<p>The following presentation of the problem closely follows Dmitri
Berskekas’s treatment in <strong>Dynamic Programming and Stochastic Control</strong> <span id="id4">[<a class="reference internal" href="zreferences.html#id68"><span>Ber75</span></a>]</span>.</p>
<p>A decision-maker observes a sequence of draws of a random variable <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>He (or she) wants to know which of two probability distributions <span class="math notranslate nohighlight">\(f_0\)</span> or <span class="math notranslate nohighlight">\(f_1\)</span> governs <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>Conditional on knowing that successive observations are drawn from distribution <span class="math notranslate nohighlight">\(f_0\)</span>, the sequence of
random variables is independently and identically distributed (IID).</p>
<p>Conditional on knowing that successive observations are drawn from distribution <span class="math notranslate nohighlight">\(f_1\)</span>, the sequence of
random variables is also independently and identically distributed (IID).</p>
<p>But the observer does not know which of the two distributions generated the sequence.</p>
<p>For reasons explained in  <a class="reference external" href="https://python.quantecon.org/exchangeable.html">Exchangeability and Bayesian Updating</a>, this means that the sequence is not
IID and that the observer has something to learn, even though he knows both <span class="math notranslate nohighlight">\(f_0\)</span> and <span class="math notranslate nohighlight">\(f_1\)</span>.</p>
<p>The decision maker   chooses a number of draws (i.e., random samples from the unknown distribution) and uses them to decide
which of the  two distributions is generating outcomes.</p>
<p>He starts with prior</p>
<div class="math notranslate nohighlight">
\[
\pi_{-1} =
\mathbb P \{ f = f_0 \mid \textrm{ no observations} \} \in (0, 1)
\]</div>
<p>After observing <span class="math notranslate nohighlight">\(k+1\)</span> observations <span class="math notranslate nohighlight">\(z_k, z_{k-1}, \ldots, z_0\)</span>, he updates this value to</p>
<div class="math notranslate nohighlight">
\[
\pi_k = \mathbb P \{ f = f_0 \mid z_k, z_{k-1}, \ldots, z_0 \}
\]</div>
<p>which is calculated recursively by applying Bayes’ law:</p>
<div class="math notranslate nohighlight">
\[
\pi_{k+1} = \frac{ \pi_k f_0(z_{k+1})}{ \pi_k f_0(z_{k+1}) + (1-\pi_k) f_1 (z_{k+1}) },
\quad k = -1, 0, 1, \ldots
\]</div>
<p>After observing <span class="math notranslate nohighlight">\(z_k, z_{k-1}, \ldots, z_0\)</span>, the decision-maker believes
that <span class="math notranslate nohighlight">\(z_{k+1}\)</span> has probability distribution</p>
<div class="math notranslate nohighlight">
\[
f_{{\pi}_k} (v) = \pi_k f_0(v) + (1-\pi_k) f_1 (v) ,
\]</div>
<p>which  is a mixture of distributions <span class="math notranslate nohighlight">\(f_0\)</span> and <span class="math notranslate nohighlight">\(f_1\)</span>, with the weight
on <span class="math notranslate nohighlight">\(f_0\)</span> being the posterior probability that <span class="math notranslate nohighlight">\(f = f_0\)</span> <a class="footnote-reference brackets" href="#f1" id="id5">1</a>.</p>
<p>To  illustrate such a distribution, let’s inspect some mixtures of beta distributions.</p>
<p>The density of a beta probability distribution with parameters <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> is</p>
<div class="math notranslate nohighlight">
\[
f(z; a, b) = \frac{\Gamma(a+b) z^{a-1} (1-z)^{b-1}}{\Gamma(a) \Gamma(b)}
\quad \text{where} \quad
\Gamma(t) := \int_{0}^{\infty} x^{t-1} e^{-x} dx
\]</div>
<p>The next figure shows two beta distributions in the top panel.</p>
<p>The bottom panel presents mixtures of these distributions, with various mixing probabilities <span class="math notranslate nohighlight">\(\pi_k\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">r</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">f0</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">f1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original Distributions&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">f0</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f_0$&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">f1</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f_1$&quot;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Mixtures&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">π</span> <span class="ow">in</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">π</span> <span class="o">*</span> <span class="n">f0</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π</span><span class="p">)</span> <span class="o">*</span> <span class="n">f1</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$\pi_k$ = </span><span class="si">{</span><span class="n">π</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;$z$ values&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;probability of $z_k$&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/wald_friedman_5_0.png" src="_images/wald_friedman_5_0.png" />
</div>
</div>
<div class="section" id="losses-and-costs">
<h3><span class="section-number">38.3.1. </span>Losses and Costs<a class="headerlink" href="#losses-and-costs" title="Permalink to this headline">¶</a></h3>
<p>After observing <span class="math notranslate nohighlight">\(z_k, z_{k-1}, \ldots, z_0\)</span>, the decision-maker
chooses among three distinct actions:</p>
<ul class="simple">
<li><p>He decides that <span class="math notranslate nohighlight">\(f = f_0\)</span> and draws no more <span class="math notranslate nohighlight">\(z\)</span>’s</p></li>
<li><p>He decides that <span class="math notranslate nohighlight">\(f = f_1\)</span> and draws no more <span class="math notranslate nohighlight">\(z\)</span>’s</p></li>
<li><p>He postpones deciding now and instead chooses to draw a
<span class="math notranslate nohighlight">\(z_{k+1}\)</span></p></li>
</ul>
<p>Associated with these three actions, the decision-maker can suffer three
kinds of losses:</p>
<ul class="simple">
<li><p>A loss <span class="math notranslate nohighlight">\(L_0\)</span> if he decides <span class="math notranslate nohighlight">\(f = f_0\)</span> when actually
<span class="math notranslate nohighlight">\(f=f_1\)</span></p></li>
<li><p>A loss <span class="math notranslate nohighlight">\(L_1\)</span> if he decides <span class="math notranslate nohighlight">\(f = f_1\)</span> when actually
<span class="math notranslate nohighlight">\(f=f_0\)</span></p></li>
<li><p>A cost <span class="math notranslate nohighlight">\(c\)</span> if he postpones deciding and chooses instead to draw
another <span class="math notranslate nohighlight">\(z\)</span></p></li>
</ul>
</div>
<div class="section" id="digression-on-type-i-and-type-ii-errors">
<h3><span class="section-number">38.3.2. </span>Digression on Type I and Type II Errors<a class="headerlink" href="#digression-on-type-i-and-type-ii-errors" title="Permalink to this headline">¶</a></h3>
<p>If we regard  <span class="math notranslate nohighlight">\(f=f_0\)</span> as a null hypothesis and <span class="math notranslate nohighlight">\(f=f_1\)</span> as an alternative hypothesis,
then <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_0\)</span> are losses associated with two types of statistical errors</p>
<ul class="simple">
<li><p>a type I error is an incorrect rejection of a true null hypothesis (a “false positive”)</p></li>
<li><p>a type II error is a failure to reject a false null hypothesis (a “false negative”)</p></li>
</ul>
<p>So when we treat <span class="math notranslate nohighlight">\(f=f_0\)</span> as the null hypothesis</p>
<ul class="simple">
<li><p>We can think of <span class="math notranslate nohighlight">\(L_1\)</span> as the loss associated with a type I
error.</p></li>
<li><p>We can think of <span class="math notranslate nohighlight">\(L_0\)</span> as the loss associated with a type II
error.</p></li>
</ul>
</div>
<div class="section" id="intuition">
<h3><span class="section-number">38.3.3. </span>Intuition<a class="headerlink" href="#intuition" title="Permalink to this headline">¶</a></h3>
<p>Let’s try to guess what an optimal decision rule might look like before we go further.</p>
<p>Suppose at some given point in time that <span class="math notranslate nohighlight">\(\pi\)</span> is close to 1.</p>
<p>Then our prior beliefs and the evidence so far point strongly to <span class="math notranslate nohighlight">\(f = f_0\)</span>.</p>
<p>If, on the other hand, <span class="math notranslate nohighlight">\(\pi\)</span> is close to 0, then <span class="math notranslate nohighlight">\(f = f_1\)</span> is strongly favored.</p>
<p>Finally, if <span class="math notranslate nohighlight">\(\pi\)</span> is in the middle of the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>, then we have little information in either direction.</p>
<p>This reasoning suggests a decision rule such as the one shown in the figure</p>
<div class="figure align-default">
<img alt="_images/wald_dec_rule.png" src="_images/wald_dec_rule.png" />
</div>
<p>As we’ll see, this is indeed the correct form of the decision rule.</p>
<p>The key problem is to determine the threshold values <span class="math notranslate nohighlight">\(\alpha, \beta\)</span>,
which will depend on the parameters listed above.</p>
<p>You might like to pause at this point and try to predict the impact of a
parameter such as <span class="math notranslate nohighlight">\(c\)</span> or <span class="math notranslate nohighlight">\(L_0\)</span> on <span class="math notranslate nohighlight">\(\alpha\)</span> or <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
</div>
<div class="section" id="a-bellman-equation">
<h3><span class="section-number">38.3.4. </span>A Bellman Equation<a class="headerlink" href="#a-bellman-equation" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(J(\pi)\)</span> be the total loss for a decision-maker with current belief <span class="math notranslate nohighlight">\(\pi\)</span> who chooses optimally.</p>
<p>With some thought, you will agree that <span class="math notranslate nohighlight">\(J\)</span> should satisfy the Bellman equation</p>
<div class="math notranslate nohighlight" id="equation-new1">
<span class="eqno">(38.1)<a class="headerlink" href="#equation-new1" title="Permalink to this equation">¶</a></span>\[J(\pi) =
    \min
    \left\{
        (1-\pi) L_0, \; \pi L_1, \;
        c + \mathbb E [ J (\pi') ]
    \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi'\)</span> is the random variable defined by Bayes’ Law</p>
<div class="math notranslate nohighlight">
\[
\pi' = \kappa(z', \pi) = \frac{ \pi f_0(z')}{ \pi f_0(z') + (1-\pi) f_1 (z') }
\]</div>
<p>when <span class="math notranslate nohighlight">\(\pi\)</span> is fixed and <span class="math notranslate nohighlight">\(z'\)</span> is drawn from the current best guess, which is the distribution <span class="math notranslate nohighlight">\(f\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[
f_{\pi}(v) = \pi f_0(v) + (1-\pi) f_1 (v)
\]</div>
<p>In the Bellman equation, minimization is over three actions:</p>
<ol class="simple">
<li><p>Accept the hypothesis that <span class="math notranslate nohighlight">\(f = f_0\)</span></p></li>
<li><p>Accept the hypothesis that <span class="math notranslate nohighlight">\(f = f_1\)</span></p></li>
<li><p>Postpone deciding and draw again</p></li>
</ol>
<p>We can represent the  Bellman equation as</p>
<div class="math notranslate nohighlight" id="equation-optdec">
<span class="eqno">(38.2)<a class="headerlink" href="#equation-optdec" title="Permalink to this equation">¶</a></span>\[J(\pi) =
\min \left\{ (1-\pi) L_0, \; \pi L_1, \; h(\pi) \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi \in [0,1]\)</span> and</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((1-\pi) L_0\)</span> is the expected loss associated with accepting
<span class="math notranslate nohighlight">\(f_0\)</span> (i.e., the cost of making a type II error).</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi L_1\)</span> is the expected loss associated with accepting
<span class="math notranslate nohighlight">\(f_1\)</span> (i.e., the cost of making a type I error).</p></li>
<li><p><span class="math notranslate nohighlight">\(h(\pi) :=  c + \mathbb E [J(\pi')]\)</span> the continuation value; i.e.,
the expected cost associated with drawing one more <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
</ul>
<p>The optimal decision rule is characterized by two numbers <span class="math notranslate nohighlight">\(\alpha, \beta \in (0,1) \times (0,1)\)</span> that satisfy</p>
<div class="math notranslate nohighlight">
\[
(1- \pi) L_0 &lt; \min \{ \pi L_1, c + \mathbb E [J(\pi')] \}  \textrm { if } \pi \geq \alpha
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\pi L_1 &lt; \min \{ (1-\pi) L_0,  c + \mathbb E [J(\pi')] \} \textrm { if } \pi \leq \beta
\]</div>
<p>The optimal decision rule is then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\textrm { accept } f=f_0 \textrm{ if } \pi \geq \alpha \\
\textrm { accept } f=f_1 \textrm{ if } \pi \leq \beta \\
\textrm { draw another }  z \textrm{ if }  \beta \leq \pi \leq \alpha
\end{aligned}
\end{split}\]</div>
<p>Our aim is to compute the value function <span class="math notranslate nohighlight">\(J\)</span>, and from it the associated cutoffs <span class="math notranslate nohighlight">\(\alpha\)</span>
and <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>To make our computations simpler, using <a class="reference internal" href="#equation-optdec">(38.2)</a>, we can write the continuation value <span class="math notranslate nohighlight">\(h(\pi)\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-optdec2">
<span class="eqno">(38.3)<a class="headerlink" href="#equation-optdec2" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
h(\pi) &amp;= c + \mathbb E [J(\pi')] \\
&amp;= c + \mathbb E_{\pi'} \min \{ (1 - \pi') L_0, \pi' L_1, h(\pi') \} \\
&amp;= c + \int \min \{ (1 - \kappa(z', \pi) ) L_0, \kappa(z', \pi)  L_1, h(\kappa(z', \pi) ) \} f_\pi (z') dz'
\end{aligned}\end{split}\]</div>
<p>The equality</p>
<div class="math notranslate nohighlight" id="equation-funceq">
<span class="eqno">(38.4)<a class="headerlink" href="#equation-funceq" title="Permalink to this equation">¶</a></span>\[h(\pi) =
c + \int \min \{ (1 - \kappa(z', \pi) ) L_0, \kappa(z', \pi)  L_1, h(\kappa(z', \pi) ) \} f_\pi (z') dz'\]</div>
<p>can be understood as a functional equation, where <span class="math notranslate nohighlight">\(h\)</span> is the unknown.</p>
<p>Using the functional equation, <a class="reference internal" href="#equation-funceq">(38.4)</a>, for the continuation value, we can back out
optimal choices using the right side of <a class="reference internal" href="#equation-optdec">(38.2)</a>.</p>
<p>This functional equation can be solved by taking an initial guess and iterating
to find a fixed point.</p>
<p>Thus, we iterate with an operator <span class="math notranslate nohighlight">\(Q\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
Q h(\pi) =
c + \int \min \{ (1 - \kappa(z', \pi) ) L_0, \kappa(z', \pi)  L_1, h(\kappa(z', \pi) ) \} f_\pi (z') dz'
\]</div>
</div>
</div>
<div class="section" id="implementation">
<h2><a class="toc-backref" href="#id13"><span class="section-number">38.4. </span>Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>First, we will construct a <code class="docutils literal notranslate"><span class="pre">jitclass</span></code> to store the parameters of the model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wf_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;a0&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>          <span class="c1"># Parameters of beta distributions</span>
           <span class="p">(</span><span class="s1">&#39;b0&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>
           <span class="p">(</span><span class="s1">&#39;a1&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>
           <span class="p">(</span><span class="s1">&#39;b1&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>
           <span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>           <span class="c1"># Cost of another draw</span>
           <span class="p">(</span><span class="s1">&#39;π_grid_size&#39;</span><span class="p">,</span> <span class="n">int64</span><span class="p">),</span>
           <span class="p">(</span><span class="s1">&#39;L0&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>          <span class="c1"># Cost of selecting f0 when f1 is true</span>
           <span class="p">(</span><span class="s1">&#39;L1&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">),</span>          <span class="c1"># Cost of selecting f1 when f0 is true</span>
           <span class="p">(</span><span class="s1">&#39;π_grid&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">[:]),</span>
           <span class="p">(</span><span class="s1">&#39;mc_size&#39;</span><span class="p">,</span> <span class="n">int64</span><span class="p">),</span>
           <span class="p">(</span><span class="s1">&#39;z0&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">[:]),</span>
           <span class="p">(</span><span class="s1">&#39;z1&#39;</span><span class="p">,</span> <span class="n">float64</span><span class="p">[:])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jitclass</span><span class="p">(</span><span class="n">wf_data</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">WaldFriedman</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">c</span><span class="o">=</span><span class="mf">1.25</span><span class="p">,</span>
                 <span class="n">a0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">b0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">a1</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">b1</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>
                 <span class="n">L0</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                 <span class="n">L1</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                 <span class="n">π_grid_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">mc_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">a0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span> <span class="o">=</span> <span class="n">a0</span><span class="p">,</span> <span class="n">b0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">a1</span><span class="p">,</span> <span class="n">b1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">π_grid_size</span> <span class="o">=</span> <span class="n">c</span><span class="p">,</span> <span class="n">π_grid_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">L0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L1</span> <span class="o">=</span> <span class="n">L0</span><span class="p">,</span> <span class="n">L1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">π_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">π_grid_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mc_size</span> <span class="o">=</span> <span class="n">mc_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">mc_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">mc_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f0</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="k">return</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="k">return</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f0_rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f1_rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">κ</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">π</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates π using Bayes&#39; rule and the current observation z</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">f0</span><span class="p">,</span> <span class="n">f1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">f0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">f1</span>

        <span class="n">π_f0</span><span class="p">,</span> <span class="n">π_f1</span> <span class="o">=</span> <span class="n">π</span> <span class="o">*</span> <span class="n">f0</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π</span><span class="p">)</span> <span class="o">*</span> <span class="n">f1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">π_new</span> <span class="o">=</span> <span class="n">π_f0</span> <span class="o">/</span> <span class="p">(</span><span class="n">π_f0</span> <span class="o">+</span> <span class="n">π_f1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">π_new</span>
</pre></div>
</div>
</div>
</div>
<p>As in the <a class="reference internal" href="optgrowth.html"><span class="doc">optimal growth lecture</span></a>, to approximate a continuous value function</p>
<ul class="simple">
<li><p>We iterate at a finite grid of possible values of <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
<li><p>When we evaluate <span class="math notranslate nohighlight">\(\mathbb E[J(\pi')]\)</span> between grid points, we use linear interpolation.</p></li>
</ul>
<p>We define the operator function <code class="docutils literal notranslate"><span class="pre">Q</span></code> below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">Q</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">wf</span><span class="p">):</span>

    <span class="n">c</span><span class="p">,</span> <span class="n">π_grid</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">c</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span>
    <span class="n">L0</span><span class="p">,</span> <span class="n">L1</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">L0</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">L1</span>
    <span class="n">z0</span><span class="p">,</span> <span class="n">z1</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">z0</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">z1</span>
    <span class="n">mc_size</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">mc_size</span>

    <span class="n">κ</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">κ</span>

    <span class="n">h_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">π_grid</span><span class="p">)</span>
    <span class="n">h_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">interp</span><span class="p">(</span><span class="n">π_grid</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">π_grid</span><span class="p">)):</span>
        <span class="n">π</span> <span class="o">=</span> <span class="n">π_grid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Find the expected value of J by integrating over z</span>
        <span class="n">integral_f0</span><span class="p">,</span> <span class="n">integral_f1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mc_size</span><span class="p">):</span>
            <span class="n">π_0</span> <span class="o">=</span> <span class="n">κ</span><span class="p">(</span><span class="n">z0</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">π</span><span class="p">)</span>  <span class="c1"># Draw z from f0 and update π</span>
            <span class="n">integral_f0</span> <span class="o">+=</span> <span class="nb">min</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π_0</span><span class="p">)</span> <span class="o">*</span> <span class="n">L0</span><span class="p">,</span> <span class="n">π_0</span> <span class="o">*</span> <span class="n">L1</span><span class="p">,</span> <span class="n">h_func</span><span class="p">(</span><span class="n">π_0</span><span class="p">))</span>

            <span class="n">π_1</span> <span class="o">=</span> <span class="n">κ</span><span class="p">(</span><span class="n">z1</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">π</span><span class="p">)</span>  <span class="c1"># Draw z from f1 and update π</span>
            <span class="n">integral_f1</span> <span class="o">+=</span> <span class="nb">min</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">L0</span><span class="p">,</span> <span class="n">π_1</span> <span class="o">*</span> <span class="n">L1</span><span class="p">,</span> <span class="n">h_func</span><span class="p">(</span><span class="n">π_1</span><span class="p">))</span>

        <span class="n">integral</span> <span class="o">=</span> <span class="p">(</span><span class="n">π</span> <span class="o">*</span> <span class="n">integral_f0</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π</span><span class="p">)</span> <span class="o">*</span> <span class="n">integral_f1</span><span class="p">)</span> <span class="o">/</span> <span class="n">mc_size</span>

        <span class="n">h_new</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">integral</span>

    <span class="k">return</span> <span class="n">h_new</span>
</pre></div>
</div>
</div>
</div>
<p>To solve the model, we will iterate using <code class="docutils literal notranslate"><span class="pre">Q</span></code> to find the fixed point</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">solve_model</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the continuation value function</span>

<span class="sd">    * wf is an instance of WaldFriedman</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Set up loop</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">tol</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="n">error</span> <span class="o">&gt;</span> <span class="n">tol</span><span class="p">:</span>
        <span class="n">h_new</span> <span class="o">=</span> <span class="n">Q</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">wf</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">h_new</span><span class="p">))</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h_new</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Failed to converge!&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">h_new</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="analysis">
<h2><a class="toc-backref" href="#id14"><span class="section-number">38.5. </span>Analysis</a><a class="headerlink" href="#analysis" title="Permalink to this headline">¶</a></h2>
<p>Let’s inspect outcomes.</p>
<p>We will be using the default parameterization with distributions like so</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wf</span> <span class="o">=</span> <span class="n">WaldFriedman</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">f0</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f_0$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">f1</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$f_1$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;probability of $z_k$&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;$z_k$&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Distributions&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/wald_friedman_14_0.png" src="_images/wald_friedman_14_0.png" />
</div>
</div>
<div class="section" id="value-function">
<h3><span class="section-number">38.5.1. </span>Value Function<a class="headerlink" href="#value-function" title="Permalink to this headline">¶</a></h3>
<p>To solve the model, we will call our <code class="docutils literal notranslate"><span class="pre">solve_model</span></code> function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h_star</span> <span class="o">=</span> <span class="n">solve_model</span><span class="p">(</span><span class="n">wf</span><span class="p">)</span>    <span class="c1"># Solve the model</span>
</pre></div>
</div>
</div>
</div>
<p>We will also set up a function to compute the cutoffs <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>
and plot these on our value function plot</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">find_cutoff_rule</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function takes a continuation value function and returns the</span>
<span class="sd">    corresponding cutoffs of where you transition between continuing and</span>
<span class="sd">    choosing a specific model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">π_grid</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span>
    <span class="n">L0</span><span class="p">,</span> <span class="n">L1</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">L0</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">L1</span>

    <span class="c1"># Evaluate cost at all points on grid for choosing a model</span>
    <span class="n">payoff_f0</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π_grid</span><span class="p">)</span> <span class="o">*</span> <span class="n">L0</span>
    <span class="n">payoff_f1</span> <span class="o">=</span> <span class="n">π_grid</span> <span class="o">*</span> <span class="n">L1</span>

    <span class="c1"># The cutoff points can be found by differencing these costs with</span>
    <span class="c1"># The Bellman equation (J is always less than or equal to p_c_i)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">π_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span>
                              <span class="n">payoff_f1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">payoff_f0</span><span class="p">),</span>
                              <span class="mf">1e-10</span><span class="p">)</span>
               <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">π_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span>
                              <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">payoff_f1</span><span class="p">)</span> <span class="o">-</span> <span class="n">payoff_f0</span><span class="p">,</span>
                              <span class="mf">1e-10</span><span class="p">)</span>
               <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">α</span><span class="p">)</span>

<span class="n">β</span><span class="p">,</span> <span class="n">α</span> <span class="o">=</span> <span class="n">find_cutoff_rule</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">h_star</span><span class="p">)</span>
<span class="n">cost_L0</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">)</span> <span class="o">*</span> <span class="n">wf</span><span class="o">.</span><span class="n">L0</span>
<span class="n">cost_L1</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span> <span class="o">*</span> <span class="n">wf</span><span class="o">.</span><span class="n">L1</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">,</span> <span class="n">h_star</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;continuation value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">,</span> <span class="n">cost_L1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;choose f1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">,</span> <span class="n">cost_L0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;choose f0&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">h_star</span><span class="p">,</span> <span class="n">cost_L0</span><span class="p">,</span> <span class="n">cost_L1</span><span class="p">]),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;minimum cost&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">β</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">α</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">β</span> <span class="o">*</span> <span class="n">wf</span><span class="o">.</span><span class="n">L0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">α</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">α</span><span class="p">)</span> <span class="o">*</span> <span class="n">wf</span><span class="o">.</span><span class="n">L1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">wf</span><span class="o">.</span><span class="n">L0</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">L1</span><span class="p">)),</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;cost&quot;</span><span class="p">,</span>
       <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;$\pi$&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Value function&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">borderpad</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/wald_friedman_18_0.png" src="_images/wald_friedman_18_0.png" />
</div>
</div>
<p>The value function equals <span class="math notranslate nohighlight">\(\pi L_1\)</span> for <span class="math notranslate nohighlight">\(\pi \leq \beta\)</span>, and <span class="math notranslate nohighlight">\((1-\pi )L_0\)</span> for <span class="math notranslate nohighlight">\(\pi
\geq \alpha\)</span>.</p>
<p>The slopes of the two linear pieces of the value function are determined by <span class="math notranslate nohighlight">\(L_1\)</span>
and <span class="math notranslate nohighlight">\(- L_0\)</span>.</p>
<p>The value function is smooth in the interior region, where the posterior
probability assigned to <span class="math notranslate nohighlight">\(f_0\)</span> is in the indecisive region <span class="math notranslate nohighlight">\(\pi \in (\beta, \alpha)\)</span>.</p>
<p>The decision-maker continues to sample until the probability that he attaches to
model <span class="math notranslate nohighlight">\(f_0\)</span> falls below <span class="math notranslate nohighlight">\(\beta\)</span> or above <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</div>
<div class="section" id="simulations">
<h3><span class="section-number">38.5.2. </span>Simulations<a class="headerlink" href="#simulations" title="Permalink to this headline">¶</a></h3>
<p>The next figure shows the outcomes of 500 simulations of the decision process.</p>
<p>On the left is a histogram of the stopping times, which equal the number of draws of <span class="math notranslate nohighlight">\(z_k\)</span> required to make a decision.</p>
<p>The average number of draws is around 6.6.</p>
<p>On the right is the fraction of correct decisions at the stopping time.</p>
<p>In this case, the decision-maker is correct 80% of the time</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simulate</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">h_star</span><span class="p">,</span> <span class="n">π_0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function takes an initial condition and simulates until it</span>
<span class="sd">    stops (when a decision is made)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">f0</span><span class="p">,</span> <span class="n">f1</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">f0</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">f1</span>
    <span class="n">f0_rvs</span><span class="p">,</span> <span class="n">f1_rvs</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">f0_rvs</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">f1_rvs</span>
    <span class="n">π_grid</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">π_grid</span>
    <span class="n">κ</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">κ</span>

    <span class="k">if</span> <span class="n">true_dist</span> <span class="o">==</span> <span class="s2">&quot;f0&quot;</span><span class="p">:</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">f_rvs</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">f0</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">f0_rvs</span>
    <span class="k">elif</span> <span class="n">true_dist</span> <span class="o">==</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">f_rvs</span> <span class="o">=</span> <span class="n">wf</span><span class="o">.</span><span class="n">f1</span><span class="p">,</span> <span class="n">wf</span><span class="o">.</span><span class="n">f1_rvs</span>

    <span class="c1"># Find cutoffs</span>
    <span class="n">β</span><span class="p">,</span> <span class="n">α</span> <span class="o">=</span> <span class="n">find_cutoff_rule</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">h_star</span><span class="p">)</span>

    <span class="c1"># Initialize a couple of useful variables</span>
    <span class="n">decision_made</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">π_0</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="n">decision_made</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="c1"># Maybe should specify which distribution is correct one so that</span>
        <span class="c1"># the draws come from the &quot;right&quot; distribution</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">f_rvs</span><span class="p">()</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">π</span> <span class="o">=</span> <span class="n">κ</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">π</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">π</span> <span class="o">&lt;</span> <span class="n">β</span><span class="p">:</span>
            <span class="n">decision_made</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">decision</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">π</span> <span class="o">&gt;</span> <span class="n">α</span><span class="p">:</span>
            <span class="n">decision_made</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">decision</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">true_dist</span> <span class="o">==</span> <span class="s2">&quot;f0&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">decision</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">correct</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">correct</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">elif</span> <span class="n">true_dist</span> <span class="o">==</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">decision</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">correct</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">correct</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="n">correct</span><span class="p">,</span> <span class="n">π</span><span class="p">,</span> <span class="n">t</span>

<span class="k">def</span> <span class="nf">stopping_dist</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">h_star</span><span class="p">,</span> <span class="n">ndraws</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">true_dist</span><span class="o">=</span><span class="s2">&quot;f0&quot;</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulates repeatedly to get distributions of time needed to make a</span>
<span class="sd">    decision and how often they are correct</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tdist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="n">cdist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndraws</span><span class="p">):</span>
        <span class="n">correct</span><span class="p">,</span> <span class="n">π</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">true_dist</span><span class="p">,</span> <span class="n">h_star</span><span class="p">)</span>
        <span class="n">tdist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span>
        <span class="n">cdist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">correct</span>

    <span class="k">return</span> <span class="n">cdist</span><span class="p">,</span> <span class="n">tdist</span>

<span class="k">def</span> <span class="nf">simulation_plot</span><span class="p">(</span><span class="n">wf</span><span class="p">):</span>
    <span class="n">h_star</span> <span class="o">=</span> <span class="n">solve_model</span><span class="p">(</span><span class="n">wf</span><span class="p">)</span>
    <span class="n">ndraws</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">cdist</span><span class="p">,</span> <span class="n">tdist</span> <span class="o">=</span> <span class="n">stopping_dist</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span> <span class="n">h_star</span><span class="p">,</span> <span class="n">ndraws</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">tdist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tdist</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stopping times over </span><span class="si">{</span><span class="n">ndraws</span><span class="si">}</span><span class="s2"> replications&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;time&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;number of stops&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tdist</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">tdist</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span>
                   <span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">tdist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">tdist</span><span class="p">))[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">cdist</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correct decisions over </span><span class="si">{</span><span class="n">ndraws</span><span class="si">}</span><span class="s2"> replications&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;% correct = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cdist</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                   <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">ndraws</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">simulation_plot</span><span class="p">(</span><span class="n">wf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/wald_friedman_20_0.png" src="_images/wald_friedman_20_0.png" />
</div>
</div>
</div>
<div class="section" id="comparative-statics">
<h3><span class="section-number">38.5.3. </span>Comparative Statics<a class="headerlink" href="#comparative-statics" title="Permalink to this headline">¶</a></h3>
<p>Now let’s consider the following exercise.</p>
<p>We double the cost of drawing an additional observation.</p>
<p>Before you look, think about what will happen:</p>
<ul class="simple">
<li><p>Will the decision-maker be correct more or less often?</p></li>
<li><p>Will he make decisions sooner or later?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wf</span> <span class="o">=</span> <span class="n">WaldFriedman</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">simulation_plot</span><span class="p">(</span><span class="n">wf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/wald_friedman_22_0.png" src="_images/wald_friedman_22_0.png" />
</div>
</div>
<p>Increased cost per draw has induced the decision-maker to take fewer draws before deciding.</p>
<p>Because he decides with fewer draws, the percentage of time he is correct drops.</p>
<p>This leads to him having a higher expected loss when he puts equal weight on both models.</p>
</div>
<div class="section" id="a-notebook-implementation">
<h3><span class="section-number">38.5.4. </span>A Notebook Implementation<a class="headerlink" href="#a-notebook-implementation" title="Permalink to this headline">¶</a></h3>
<p>To facilitate comparative statics, we provide
a <a class="reference external" href="https://nbviewer.jupyter.org/github/QuantEcon/lecture-python-advanced.notebooks/blob/master/wald_friedman.ipynb">Jupyter notebook</a> that
generates the same plots, but with sliders.</p>
<p>With these sliders, you can adjust parameters and immediately observe</p>
<ul class="simple">
<li><p>effects on the smoothness of the value function in the indecisive middle range
as we increase the number of grid points in the piecewise linear  approximation.</p></li>
<li><p>effects of different settings for the cost parameters <span class="math notranslate nohighlight">\(L_0, L_1, c\)</span>, the
parameters of two beta distributions <span class="math notranslate nohighlight">\(f_0\)</span> and <span class="math notranslate nohighlight">\(f_1\)</span>, and the number
of points and linear functions <span class="math notranslate nohighlight">\(m\)</span> to use in the piece-wise continuous approximation to the value function.</p></li>
<li><p>various simulations from <span class="math notranslate nohighlight">\(f_0\)</span> and associated distributions of waiting times to making a decision.</p></li>
<li><p>associated histograms of correct and incorrect decisions.</p></li>
</ul>
</div>
</div>
<div class="section" id="comparison-with-neyman-pearson-formulation">
<h2><a class="toc-backref" href="#id15"><span class="section-number">38.6. </span>Comparison with Neyman-Pearson Formulation</a><a class="headerlink" href="#comparison-with-neyman-pearson-formulation" title="Permalink to this headline">¶</a></h2>
<p>For several reasons, it is useful to describe the theory underlying the test
that Navy Captain G. S. Schuyler had been told to use and that led him
to approach Milton Friedman and Allan Wallis to convey his conjecture
that superior practical procedures existed.</p>
<p>Evidently, the Navy had told
Captail Schuyler to use what it knew to be a state-of-the-art
Neyman-Pearson test.</p>
<p>We’ll rely on Abraham Wald’s <span id="id6">[<a class="reference internal" href="zreferences.html#id69"><span>Wal47</span></a>]</span> elegant summary of Neyman-Pearson theory.</p>
<p>For our purposes, watch for there features of the setup:</p>
<ul class="simple">
<li><p>the assumption of a <em>fixed</em> sample size <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>the application of laws of large numbers, conditioned on alternative
probability models, to interpret the probabilities <span class="math notranslate nohighlight">\(\alpha\)</span> and
<span class="math notranslate nohighlight">\(\beta\)</span> defined in the Neyman-Pearson theory</p></li>
</ul>
<p>Recall that in the sequential analytic formulation above, that</p>
<ul class="simple">
<li><p>The sample size <span class="math notranslate nohighlight">\(n\)</span> is not fixed but rather an object to be
chosen; technically <span class="math notranslate nohighlight">\(n\)</span> is a random variable.</p></li>
<li><p>The parameters <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\alpha\)</span> characterize cut-off
rules used to determine <span class="math notranslate nohighlight">\(n\)</span> as a random variable.</p></li>
<li><p>Laws of large numbers make no appearances in the sequential
construction.</p></li>
</ul>
<p>In chapter 1 of <strong>Sequential Analysis</strong> <span id="id7">[<a class="reference internal" href="zreferences.html#id69"><span>Wal47</span></a>]</span> Abraham Wald summarizes the
Neyman-Pearson approach to hypothesis testing.</p>
<p>Wald frames the problem as making a decision about a probability
distribution that is partially known.</p>
<p>(You have to assume that <em>something</em> is already known in order to state a well-posed
problem – usually, <em>something</em> means <em>a lot</em>)</p>
<p>By limiting  what is unknown, Wald uses the following simple structure
to illustrate the main ideas:</p>
<ul class="simple">
<li><p>A decision-maker wants to decide which of two distributions
<span class="math notranslate nohighlight">\(f_0\)</span>, <span class="math notranslate nohighlight">\(f_1\)</span> govern an IID random variable <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
<li><p>The null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> is the statement that <span class="math notranslate nohighlight">\(f_0\)</span>
governs the data.</p></li>
<li><p>The alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span> is the statement that
<span class="math notranslate nohighlight">\(f_1\)</span> governs the data.</p></li>
<li><p>The problem is to devise and analyze a test of hypothesis
<span class="math notranslate nohighlight">\(H_0\)</span> against the alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span> on the
basis of a sample of a fixed number <span class="math notranslate nohighlight">\(n\)</span> independent
observations <span class="math notranslate nohighlight">\(z_1, z_2, \ldots, z_n\)</span> of the random variable
<span class="math notranslate nohighlight">\(z\)</span>.</p></li>
</ul>
<p>To quote Abraham Wald,</p>
<blockquote>
<div><p>A test procedure leading to the acceptance or rejection of the [null]
hypothesis in question is simply a rule specifying, for each possible
sample of size <span class="math notranslate nohighlight">\(n\)</span>, whether the [null] hypothesis should be accepted
or rejected on the basis of the sample. This may also be expressed as
follows: A test procedure is simply a subdivision of the totality of
all possible samples of size <span class="math notranslate nohighlight">\(n\)</span> into two mutually exclusive
parts, say part 1 and part 2, together with the application of the
rule that the [null] hypothesis be accepted if the observed sample is
contained in part 2. Part 1 is also called the critical region. Since
part 2 is the totality of all samples of size <span class="math notranslate nohighlight">\(n\)</span> which are not
included in part 1, part 2 is uniquely determined by part 1. Thus,
choosing a test procedure is equivalent to determining a critical
region.</p>
</div></blockquote>
<p>Let’s listen to Wald longer:</p>
<blockquote>
<div><p>As a basis for choosing among critical regions the following
considerations have been advanced by Neyman and Pearson: In accepting
or rejecting <span class="math notranslate nohighlight">\(H_0\)</span> we may commit errors of two kinds. We commit
an error of the first kind if we reject <span class="math notranslate nohighlight">\(H_0\)</span> when it is true;
we commit an error of the second kind if we accept <span class="math notranslate nohighlight">\(H_0\)</span> when
<span class="math notranslate nohighlight">\(H_1\)</span> is true. After a particular critical region <span class="math notranslate nohighlight">\(W\)</span> has
been chosen, the probability of committing an error of the first
kind, as well as the probability of committing an error of the second
kind is uniquely determined. The probability of committing an error
of the first kind is equal to the probability, determined by the
assumption that <span class="math notranslate nohighlight">\(H_0\)</span> is true, that the observed sample will be
included in the critical region <span class="math notranslate nohighlight">\(W\)</span>. The probability of
committing an error of the second kind is equal to the probability,
determined on the assumption that <span class="math notranslate nohighlight">\(H_1\)</span> is true, that the
probability will fall outside the critical region <span class="math notranslate nohighlight">\(W\)</span>. For any
given critical region <span class="math notranslate nohighlight">\(W\)</span> we shall denote the probability of an
error of the first kind by <span class="math notranslate nohighlight">\(\alpha\)</span> and the probability of an
error of the second kind by <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
</div></blockquote>
<p>Let’s listen carefully to how Wald applies law of large numbers to
interpret <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<blockquote>
<div><p>The probabilities <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> have the
following important practical interpretation: Suppose that we draw a
large number of samples of size <span class="math notranslate nohighlight">\(n\)</span>. Let <span class="math notranslate nohighlight">\(M\)</span> be the
number of such samples drawn. Suppose that for each of these
<span class="math notranslate nohighlight">\(M\)</span> samples we reject <span class="math notranslate nohighlight">\(H_0\)</span> if the sample is included in
<span class="math notranslate nohighlight">\(W\)</span> and accept <span class="math notranslate nohighlight">\(H_0\)</span> if the sample lies outside
<span class="math notranslate nohighlight">\(W\)</span>. In this way we make <span class="math notranslate nohighlight">\(M\)</span> statements of rejection or
acceptance. Some of these statements will in general be wrong. If
<span class="math notranslate nohighlight">\(H_0\)</span> is true and if <span class="math notranslate nohighlight">\(M\)</span> is large, the probability is
nearly <span class="math notranslate nohighlight">\(1\)</span> (i.e., it is practically certain) that the
proportion of wrong statements (i.e., the number of wrong statements
divided by <span class="math notranslate nohighlight">\(M\)</span>) will be approximately <span class="math notranslate nohighlight">\(\alpha\)</span>. If
<span class="math notranslate nohighlight">\(H_1\)</span> is true, the probability is nearly <span class="math notranslate nohighlight">\(1\)</span> that the
proportion of wrong statements will be approximately <span class="math notranslate nohighlight">\(\beta\)</span>.
Thus, we can say that in the long run [ here Wald applies law of
large numbers by driving <span class="math notranslate nohighlight">\(M \rightarrow \infty\)</span> (our comment,
not Wald’s) ] the proportion of wrong statements will be
<span class="math notranslate nohighlight">\(\alpha\)</span> if <span class="math notranslate nohighlight">\(H_0\)</span>is true and <span class="math notranslate nohighlight">\(\beta\)</span> if
<span class="math notranslate nohighlight">\(H_1\)</span> is true.</p>
</div></blockquote>
<p>The quantity <span class="math notranslate nohighlight">\(\alpha\)</span> is called the <em>size</em> of the critical region,
and the quantity <span class="math notranslate nohighlight">\(1-\beta\)</span> is called the <em>power</em> of the critical
region.</p>
<p>Wald notes that</p>
<blockquote>
<div><p>one critical region <span class="math notranslate nohighlight">\(W\)</span> is more desirable than another if it
has smaller values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. Although
either <span class="math notranslate nohighlight">\(\alpha\)</span> or <span class="math notranslate nohighlight">\(\beta\)</span> can be made arbitrarily small
by a proper choice of the critical region <span class="math notranslate nohighlight">\(W\)</span>, it is possible
to make both <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> arbitrarily small for a
fixed value of <span class="math notranslate nohighlight">\(n\)</span>, i.e., a fixed sample size.</p>
</div></blockquote>
<p>Wald summarizes Neyman and Pearson’s setup as follows:</p>
<blockquote>
<div><p>Neyman and Pearson show that a region consisting of all samples
<span class="math notranslate nohighlight">\((z_1, z_2, \ldots, z_n)\)</span> which satisfy the inequality</p>
<div class="math notranslate nohighlight">
\[
  \frac{ f_1(z_1) \cdots f_1(z_n)}{f_0(z_1) \cdots f_0(z_n)} \geq k
  \]</div>
<p>is a most powerful critical region for testing the hypothesis
<span class="math notranslate nohighlight">\(H_0\)</span> against the alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span>. The term
<span class="math notranslate nohighlight">\(k\)</span> on the right side is a constant chosen so that the region
will have the required size <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</div></blockquote>
<p>Wald goes on to discuss Neyman and Pearson’s concept of <em>uniformly most
powerful</em> test.</p>
<p>Here is how Wald introduces the notion of a sequential test</p>
<blockquote>
<div><p>A rule is given for making one of the following three decisions at any stage of
the experiment (at the m th trial for each integral value of m ): (1) to
accept the hypothesis H , (2) to reject the hypothesis H , (3) to
continue the experiment by making an additional observation. Thus, such
a test procedure is carried out sequentially. On the basis of the first
observation, one of the aforementioned decision is made. If the first or
second decision is made, the process is terminated. If the third
decision is made, a second trial is performed. Again, on the basis of
the first two observations, one of the three decision is made. If the
third decision is made, a third trial is performed, and so on. The
process is continued until either the first or the second decisions is
made. The number n of observations required by such a test procedure is
a random variable, since the value of n depends on the outcome of the
observations.</p>
</div></blockquote>
</div>
<div class="section" id="sequels">
<h2><a class="toc-backref" href="#id16"><span class="section-number">38.7. </span>Sequels</a><a class="headerlink" href="#sequels" title="Permalink to this headline">¶</a></h2>
<p>We’ll dig deeper into some of the ideas used here in the following lectures:</p>
<ul class="simple">
<li><p><a class="reference internal" href="exchangeable.html"><span class="doc">this lecture</span></a> discusses the key concept of <strong>exchangeability</strong> that rationalizes statistical learning</p></li>
<li><p><a class="reference internal" href="likelihood_ratio_process.html"><span class="doc">this lecture</span></a> describes <strong>likelihood ratio processes</strong> and their role in frequentist and Bayesian statistical theories</p></li>
<li><p><a class="reference internal" href="likelihood_bayes.html"><span class="doc">this lecture</span></a> discusses the role of likelihood ratio processes in <strong>Bayesian learning</strong></p></li>
<li><p><a class="reference internal" href="navy_captain.html"><span class="doc">this lecture</span></a> returns to the subject of this lecture and studies whether the Captain’s hunch that the (frequentist) decision rule
that the Navy had ordered him to use can be expected to be better or worse than the rule sequential rule that Abraham Wald designed</p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="f1"><span class="brackets"><a class="fn-backref" href="#id5">1</a></span></dt>
<dd><p>The decision maker acts as if he believes that the sequence of random variables
<span class="math notranslate nohighlight">\([z_{0}, z_{1}, \ldots]\)</span> is <em>exchangeable</em>.  See <a class="reference external" href="https://python.quantecon.org/exchangeable.html">Exchangeability and Bayesian Updating</a> and
<span id="id8">[<a class="reference internal" href="zreferences.html#id67"><span>Kre88</span></a>]</span> chapter 11, for  discussions of exchangeability.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            
            <div class="sidebar bd-sidebar inactive" id="site-navigation">

                <div class="sidebar__header">


                    Contents

                </div>

                <nav class="sidebar__nav" id="sidebar-nav" aria-label="Main navigation">
                    <p class="caption">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="geom_series.html">
   1. Geometric Series for Elementary Economics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multi_hyper.html">
   2. Multivariate Hypergeometric Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sir_model.html">
   3. Modeling COVID 19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   4. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="complex_and_trig.html">
   5. Complex Numbers and Trigonometry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   6. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heavy_tails.html">
   7. Heavy-Tailed Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_normal.html">
   8. Multivariate Normal Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="time_series_with_matrices.html">
   9. Univariate Time Series with Matrix Algebra
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="scalar_dynam.html">
   10. Dynamics in One Dimension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_processes.html">
   11. AR1 Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="finite_markov.html">
   12. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inventory_dynamics.html">
   13. Inventory Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_models.html">
   14. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="samuelson.html">
   15. Application: The Samuelson Multiplier-Accelerator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kesten_processes.html">
   16. Kesten Processes and Firm Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wealth_dynamics.html">
   17. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman.html">
   18. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="short_path.html">
   19. Shortest Paths
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_1.html">
   20. Cass-Koopmans Planning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_2.html">
   21. Cass-Koopmans Competitive Equilibrium
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Search
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model.html">
   22. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_separation.html">
   23. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_fitted_vfi.html">
   24. Job Search III: Fitted Value Function Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_correlated.html">
   25. Job Search IV: Correlated Wage Offers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="career.html">
   26. Job Search V: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jv.html">
   27. Job Search VI: On-the-Job Search
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Consumption, Savings and Growth
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_problem.html">
   28. Cake Eating I: Introduction to Optimal Saving
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_numerical.html">
   29. Cake Eating II: Numerical Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth.html">
   30. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth_fast.html">
   31. Optimal Growth II: Accelerating the Code with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coleman_policy_iter.html">
   32. Optimal Growth III: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="egm_policy_iter.html">
   33. Optimal Growth IV: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp.html">
   34. The Income Fluctuation Problem I: Basic Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp_advanced.html">
   35. The Income Fluctuation Problem II: Stochastic Returns on Assets
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Information
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="odu.html">
   36. Job Search VII: Search with Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process.html">
   37. Likelihood Ratio Processes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   38. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exchangeable.html">
   39. Exchangeability and Bayesian Updating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_bayes.html">
   40. Likelihood Ratio Processes and Bayesian Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="navy_captain.html">
   41. Bayesian versus Frequentist Decision Rules
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lqcontrol.html">
   42. LQ Control: Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income.html">
   43. The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income_cons.html">
   44. Permanent Income II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lq_inventories.html">
   45. Production Smoothing via Inventories
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="schelling.html">
   46. Schelling’s Segregation Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lake_model.html">
   47. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rational_expectations.html">
   48. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="re_with_feedback.html">
   49. Stability in Linear Rational Expectations Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_perf.html">
   50. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncertainty_traps.html">
   51. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aiyagari.html">
   52. The Aiyagari Model
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markov_asset.html">
   53. Asset Pricing: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrison_kreps.html">
   54. Asset Pricing with Incomplete Markets
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Data and Empirics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_panel.html">
   55. Pandas for Panel Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ols.html">
   56. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mle.html">
   57. Maximum Likelihood Estimation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   58. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   59. References
  </a>
 </li>
</ul>

                </nav>

                <div class="sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="toolbar">

            <div class="toolbar__inner">

                <ul class="toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                    <!-- <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="_notebooks/wald_friedman.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li data-tippy-content="Launch Notebook"><a href="https://mybinder.org/v2/gh/QuantEcon/lecture-python.notebooks/master?urlpath=tree/wald_friedman.ipynb" target="_blank"><i data-feather="play-circle"></i></a></li>
                    <li data-tippy-content="Download PDF" onClick="window.print()"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst/tree/master/lectures/wald_friedman.md" download><i data-feather="github"></i></a></li>
                </ul>

            </div>


        </div> <!-- .toolbar -->

    </div> <!-- .wrapper-->

<script src="_static/plugins.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script src="https://unpkg.com/@popperjs/core@2"></script>
<script src="https://unpkg.com/tippy.js@6"></script>


    <script src=[></script>

    <script src=]></script>

<script src="_static/scripts.js"></script>
<script>
    feather.replace()
    tippy('[data-tippy-content]');
</script>


  </body>
</html>