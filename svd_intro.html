
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7. Singular Value Decomposition (SVD) &#8212; Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/quantecon-book-theme.1ef59f8f4e91ec8319176e8479c6af4e.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">


    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="_static/quantecon-book-theme.15b0c36fffe88f468997fa7b698991d3.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://python.quantecon.org/svd_intro.html" />
    <link rel="shortcut icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="8. Using Newton’s Method to Solve Economic Models" href="newton_method.html" />
    <link rel="prev" title="6. Circulant Matrices" href="eig_circulant.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Singular Value Decomposition (SVD)"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Singular Value Decomposition (SVD)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python.quantecon.org/svd_intro.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=svd_intro>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   7.1. Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-setting">
   7.2. The Setting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#singular-value-decomposition">
   7.3. Singular Value Decomposition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#four-fundamental-subspaces">
   7.4. Four Fundamental Subspaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#full-and-reduced-svds">
   7.5. Full and Reduced SVD’s
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#polar-decomposition">
   7.6. Polar Decomposition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-components-analysis-pca">
   7.7. Principal Components Analysis (PCA)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relationship-of-pca-to-svd">
   7.8. Relationship of PCA to SVD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-with-eigenvalues-and-eigenvectors">
   7.9. PCA with Eigenvalues and Eigenvectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#connections">
   7.10. Connections
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-autoregressions">
   7.11. Vector Autoregressions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-mode-decomposition-dmd">
   7.12. Dynamic Mode Decomposition (DMD)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representation-1">
   7.13. Representation 1
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representation-2">
   7.14. Representation 2
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representation-3">
   7.15. Representation 3
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decoder-of-x-as-a-linear-projection">
     7.15.1. Decoder of
     <span class="math notranslate nohighlight">
      \(X\)
     </span>
     as a linear projection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-useful-approximation">
     7.15.2. A useful approximation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-fewer-modes">
     7.15.3. Using Fewer Modes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#source-for-some-python-code">
   7.16. Source for Some Python Code
  </a>
 </li>
</ul>

                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo" alt="logo"></a>
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Quantitative Economics with Python</a></p>

                        <p class="qe-page__header-subheading">Singular Value Decomposition (SVD)</p>

                    </div>

                    <p class="qe-page__header-authors">Thomas J. Sargent & John Stachurski</p>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <div class="tex2jax_ignore mathjax_ignore section" id="singular-value-decomposition-svd">
<h1><span class="section-number">7. </span>Singular Value Decomposition (SVD)<a class="headerlink" href="#singular-value-decomposition-svd" title="Permalink to this headline">¶</a></h1>
<p>In addition to regular packages contained in Anaconda by default, this lecture also requires:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install quandl
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting quandl
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  Downloading Quandl-3.7.0-py2.py3-none-any.whl (26 kB)
Requirement already satisfied: pandas&gt;=0.14 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (1.4.4)
Requirement already satisfied: inflection&gt;=0.3.1 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (0.5.1)
Requirement already satisfied: python-dateutil in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (2.8.2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting more-itertools
  Downloading more_itertools-9.0.0-py3-none-any.whl (52 kB)
?25l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">0.0/52.8 kB</span> <span class=" -Color -Color-Red">?</span> eta <span class=" -Color -Color-Cyan">-:--:--</span>
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">52.8/52.8 kB</span> <span class=" -Color -Color-Red">10.0 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25h
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: numpy&gt;=1.8 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (1.23.5)
Requirement already satisfied: requests&gt;=2.7.0 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (2.28.1)
Requirement already satisfied: six in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from quandl) (1.16.0)
Requirement already satisfied: pytz&gt;=2020.1 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from pandas&gt;=0.14-&gt;quandl) (2022.7)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests&gt;=2.7.0-&gt;quandl) (1.26.11)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests&gt;=2.7.0-&gt;quandl) (3.3)
Requirement already satisfied: certifi&gt;=2017.4.17 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests&gt;=2.7.0-&gt;quandl) (2022.9.14)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /__w/lecture-python.myst/lecture-python.myst/3/envs/quantecon/lib/python3.9/site-packages (from requests&gt;=2.7.0-&gt;quandl) (2.0.4)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Installing collected packages: more-itertools, quandl
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Successfully installed more-itertools-9.0.0 quandl-3.7.0
<span class=" -Color -Color-Yellow">WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</span>

</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">LA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">quandl</span> <span class="k">as</span> <span class="nn">ql</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="overview">
<h2><span class="section-number">7.1. </span>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The <strong>singular value decomposition</strong> (SVD) is a work-horse in applications of least squares projection that
form  foundations for many statistical and  machine learning methods.</p>
<p>After defining the SVD, we’ll describe how it connects to</p>
<ul class="simple">
<li><p>the <strong>four fundamental spaces</strong> of linear algebra</p></li>
<li><p>underdetermined and over-determined <strong>least squares regressions</strong></p></li>
<li><p><strong>principal components analysis</strong> (PCA)</p></li>
</ul>
<p>We’ll also tell  the essential role that the SVD plays in</p>
<ul class="simple">
<li><p>dynamic mode decomposition (DMD)</p></li>
</ul>
<p>Like principal components analysis (PCA), DMD can be thought of as a data-reduction procedure that  represents salient patterns by projecting data onto a limited set of factors.</p>
</div>
<div class="section" id="the-setting">
<h2><span class="section-number">7.2. </span>The Setting<a class="headerlink" href="#the-setting" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of rank <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Necessarily, <span class="math notranslate nohighlight">\(p \leq \min(m,n)\)</span>.</p>
<p>In  much of this lecture, we’ll think of <span class="math notranslate nohighlight">\(X\)</span> as a matrix of <strong>data</strong> in which</p>
<ul class="simple">
<li><p>each column is an <strong>individual</strong> – a time period or person, depending on the application</p></li>
<li><p>each row is a <strong>random variable</strong> describing an attribute of a time period or a person, depending on the application</p></li>
</ul>
<p>We’ll be interested in  two  situations</p>
<ul class="simple">
<li><p>A <strong>short and fat</strong> case in which <span class="math notranslate nohighlight">\(m &lt;&lt; n\)</span>, so that there are many more columns (individuals) than rows (attributes).</p></li>
<li><p>A  <strong>tall and skinny</strong> case in which <span class="math notranslate nohighlight">\(m &gt;&gt; n\)</span>, so that there are many more rows  (attributes) than columns (individuals).</p></li>
</ul>
<p>We’ll apply a <strong>singular value decomposition</strong> of <span class="math notranslate nohighlight">\(X\)</span> in both situations.</p>
<p>In the <span class="math notranslate nohighlight">\( m &lt; &lt; n\)</span> case  in which there are many more individuals <span class="math notranslate nohighlight">\(n\)</span> than attributes <span class="math notranslate nohighlight">\(m\)</span>, we can calculate sample moments of  a joint distribution  by taking averages  across observations of functions of the observations.</p>
<p>In this <span class="math notranslate nohighlight">\( m &lt; &lt; n\)</span> case,  we’ll look for <strong>patterns</strong> by using a <strong>singular value decomposition</strong> to do a <strong>principal components analysis</strong> (PCA).</p>
<p>In the <span class="math notranslate nohighlight">\(m &gt; &gt; n\)</span>  case in which there are many more attributes <span class="math notranslate nohighlight">\(m\)</span> than individuals <span class="math notranslate nohighlight">\(n\)</span> and when we are in a time-series setting in which <span class="math notranslate nohighlight">\(n\)</span> equals the number of time periods covered in the data set <span class="math notranslate nohighlight">\(X\)</span>, we’ll proceed in a different way.</p>
<p>We’ll again use a <strong>singular value decomposition</strong>,  but now to construct a <strong>dynamic mode decomposition</strong> (DMD)</p>
</div>
<div class="section" id="singular-value-decomposition">
<h2><span class="section-number">7.3. </span>Singular Value Decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">¶</a></h2>
<p>A <strong>singular value decomposition</strong> of an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix <span class="math notranslate nohighlight">\(X\)</span> of rank <span class="math notranslate nohighlight">\(p \leq \min(m,n)\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-svd101">
<span class="eqno">(7.1)<a class="headerlink" href="#equation-eq-svd101" title="Permalink to this equation">¶</a></span>\[
X  = U \Sigma V^T
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
UU^T &amp;  = I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V = I
\end{aligned}
\]</div>
<p>and</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U\)</span> is an <span class="math notranslate nohighlight">\(m \times m\)</span> orthogonal  matrix of <strong>left singular vectors</strong> of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>Columns of <span class="math notranslate nohighlight">\(U\)</span> are eigenvectors of <span class="math notranslate nohighlight">\(X^T X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> orthogonal matrix of <strong>right singular values</strong> of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>Columns of <span class="math notranslate nohighlight">\(V\)</span>  are eigenvectors of <span class="math notranslate nohighlight">\(X X^T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix in which the first <span class="math notranslate nohighlight">\(p\)</span> places on its main diagonal are positive numbers <span class="math notranslate nohighlight">\(\sigma_1, \sigma_2, \ldots, \sigma_p\)</span> called <strong>singular values</strong>; remaining entries of <span class="math notranslate nohighlight">\(\Sigma\)</span> are all zero</p></li>
<li><p>The <span class="math notranslate nohighlight">\(p\)</span> singular values are positive square roots of the eigenvalues of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix  <span class="math notranslate nohighlight">\(X X^T\)</span> and also of the <span class="math notranslate nohighlight">\(n \times n\)</span> matrix <span class="math notranslate nohighlight">\(X^T X\)</span></p></li>
<li><p>We adopt a convention that when <span class="math notranslate nohighlight">\(U\)</span> is a complex valued matrix, <span class="math notranslate nohighlight">\(U^T\)</span> denotes the <strong>conjugate-transpose</strong> or <strong>Hermitian-transpose</strong> of <span class="math notranslate nohighlight">\(U\)</span>, meaning that
<span class="math notranslate nohighlight">\(U_{ij}^T\)</span> is the complex conjugate of <span class="math notranslate nohighlight">\(U_{ji}\)</span>.</p></li>
<li><p>Similarly, when <span class="math notranslate nohighlight">\(V\)</span> is a complex valued matrix, <span class="math notranslate nohighlight">\(V^T\)</span> denotes the <strong>conjugate-transpose</strong> or <strong>Hermitian-transpose</strong> of <span class="math notranslate nohighlight">\(V\)</span></p></li>
</ul>
<p>The matrices <span class="math notranslate nohighlight">\(U,\Sigma,V\)</span> entail linear transformations that reshape in vectors in the following ways:</p>
<ul class="simple">
<li><p>multiplying vectors  by the unitary matrices <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> <strong>rotate</strong> them, but leave <strong>angles between vectors</strong> and <strong>lengths of vectors</strong> unchanged.</p></li>
<li><p>multiplying vectors by the diagonal  matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> leaves <strong>angles between vectors</strong> unchanged but <strong>rescales</strong> vectors.</p></li>
</ul>
<p>Thus, representation <a class="reference internal" href="#equation-eq-svd101">(7.1)</a> asserts that multiplying an <span class="math notranslate nohighlight">\(n \times 1\)</span>  vector <span class="math notranslate nohighlight">\(y\)</span> by the <span class="math notranslate nohighlight">\(m \times n\)</span> matrix <span class="math notranslate nohighlight">\(X\)</span>
amounts to performing the following three multiplcations of <span class="math notranslate nohighlight">\(y\)</span> sequentially:</p>
<ul class="simple">
<li><p><strong>rotating</strong> <span class="math notranslate nohighlight">\(y\)</span> by computing <span class="math notranslate nohighlight">\(V^T y\)</span></p></li>
<li><p><strong>rescaling</strong> <span class="math notranslate nohighlight">\(V^T y\)</span> by multipying it by <span class="math notranslate nohighlight">\(\Sigma\)</span></p></li>
<li><p><strong>rotating</strong> <span class="math notranslate nohighlight">\(\Sigma V^T y\)</span> by multiplying it by <span class="math notranslate nohighlight">\(U\)</span></p></li>
</ul>
<p>This structure of the <span class="math notranslate nohighlight">\(m \times n\)</span> matrix  <span class="math notranslate nohighlight">\(X\)</span> opens the door to constructing systems
of data <strong>encoders</strong> and <strong>decoders</strong>, an idea that we shall  apply later in this lecture.</p>
<p>What we have described above  is called a <strong>full</strong> SVD.</p>
<p>In a <strong>full</strong> SVD, the  shapes of <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> are <span class="math notranslate nohighlight">\(\left(m, m\right)\)</span>, <span class="math notranslate nohighlight">\(\left(m, n\right)\)</span>, <span class="math notranslate nohighlight">\(\left(n, n\right)\)</span>, respectively.</p>
<p>Later we’ll also describe an <strong>economy</strong> or <strong>reduced</strong> SVD.</p>
<p>Before we study a <strong>reduced</strong> SVD we’ll say a little more about properties of a <strong>full</strong> SVD.</p>
</div>
<div class="section" id="four-fundamental-subspaces">
<h2><span class="section-number">7.4. </span>Four Fundamental Subspaces<a class="headerlink" href="#four-fundamental-subspaces" title="Permalink to this headline">¶</a></h2>
<p>Let  <span class="math notranslate nohighlight">\({\mathcal C}\)</span> denote a column space, <span class="math notranslate nohighlight">\({\mathcal N}\)</span> denote a null space, and <span class="math notranslate nohighlight">\({\mathcal R}\)</span> denote a row space.</p>
<p>Let’s start by recalling the four fundamental subspaces of an <span class="math notranslate nohighlight">\(m \times n\)</span>
matrix <span class="math notranslate nohighlight">\(X\)</span> of rank <span class="math notranslate nohighlight">\(p\)</span>.</p>
<ul class="simple">
<li><p>The <strong>column space</strong> of <span class="math notranslate nohighlight">\(X\)</span>, denoted <span class="math notranslate nohighlight">\({\mathcal C}(X)\)</span>, is the span of the  columns of  <span class="math notranslate nohighlight">\(X\)</span>, i.e., all vectors <span class="math notranslate nohighlight">\(y\)</span> that can be written as linear combinations of columns of <span class="math notranslate nohighlight">\(X\)</span>. Its dimension is <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
<li><p>The <strong>null space</strong> of <span class="math notranslate nohighlight">\(X\)</span>, denoted <span class="math notranslate nohighlight">\({\mathcal N}(X)\)</span> consists of all vectors <span class="math notranslate nohighlight">\(y\)</span> that satisfy
<span class="math notranslate nohighlight">\(X y = 0\)</span>. Its dimension is <span class="math notranslate nohighlight">\(m-p\)</span>.</p></li>
<li><p>The <strong>row space</strong> of <span class="math notranslate nohighlight">\(X\)</span>, denoted <span class="math notranslate nohighlight">\({\mathcal R}(X)\)</span> is the column space of <span class="math notranslate nohighlight">\(X^T\)</span>. It consists of all
vectors <span class="math notranslate nohighlight">\(z\)</span> that can be written as  linear combinations of rows of <span class="math notranslate nohighlight">\(X\)</span>. Its dimension is <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
<li><p>The <strong>left null space</strong> of <span class="math notranslate nohighlight">\(X\)</span>, denoted <span class="math notranslate nohighlight">\({\mathcal N}(X^T)\)</span>, consist of all vectors <span class="math notranslate nohighlight">\(z\)</span> such that
<span class="math notranslate nohighlight">\(X^T z =0\)</span>.  Its dimension is <span class="math notranslate nohighlight">\(n-p\)</span>.</p></li>
</ul>
<p>For a  full SVD of a matrix <span class="math notranslate nohighlight">\(X\)</span>, the matrix <span class="math notranslate nohighlight">\(U\)</span> of left singular vectors  and the matrix <span class="math notranslate nohighlight">\(V\)</span> of right singular vectors contain orthogonal bases for all four subspaces.</p>
<p>They form two pairs of orthogonal subspaces
that we’ll describe now.</p>
<p>Let <span class="math notranslate nohighlight">\(u_i, i = 1, \ldots, m\)</span> be the <span class="math notranslate nohighlight">\(m\)</span> column vectors of <span class="math notranslate nohighlight">\(U\)</span> and let
<span class="math notranslate nohighlight">\(v_i, i = 1, \ldots, n\)</span> be the <span class="math notranslate nohighlight">\(n\)</span> column vectors of <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>Let’s write the full SVD of X as</p>
<div class="math notranslate nohighlight" id="equation-eq-fullsvdpartition">
<span class="eqno">(7.2)<a class="headerlink" href="#equation-eq-fullsvdpartition" title="Permalink to this equation">¶</a></span>\[
X = \begin{bmatrix} U_L &amp; U_R \end{bmatrix} \begin{bmatrix} \Sigma_p &amp; 0 \cr 0 &amp; 0 \end{bmatrix}
     \begin{bmatrix} V_L &amp; V_R \end{bmatrix}^T
\]</div>
<p>where  <span class="math notranslate nohighlight">\( \Sigma_p\)</span> is  a <span class="math notranslate nohighlight">\(p \times p\)</span> diagonal matrix with the <span class="math notranslate nohighlight">\(p\)</span> singular values on the diagonal and</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
U_L &amp; = \begin{bmatrix}u_1 &amp; \cdots  &amp; u_p \end{bmatrix},  \quad U_R  = \begin{bmatrix}u_{p+1} &amp; \cdots u_m \end{bmatrix}  \cr
V_L &amp; = \begin{bmatrix}v_1 &amp; \cdots  &amp; v_p \end{bmatrix} , \quad U_R  = \begin{bmatrix}v_{p+1} &amp; \cdots u_n \end{bmatrix} 
\end{aligned}
\]</div>
<p>Representation <a class="reference internal" href="#equation-eq-fullsvdpartition">(7.2)</a> implies that</p>
<div class="math notranslate nohighlight">
\[
X \begin{bmatrix} V_L &amp; V_R \end{bmatrix} = \begin{bmatrix} U_L &amp; U_R \end{bmatrix} \begin{bmatrix} \Sigma_p &amp; 0 \cr 0 &amp; 0 \end{bmatrix}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-xfour1a">
<span class="eqno">(7.3)<a class="headerlink" href="#equation-eq-xfour1a" title="Permalink to this equation">¶</a></span>\[
\begin{aligned}
X V_L &amp; = U_L \Sigma_p \cr 
X V_R &amp; = 0 
\end{aligned}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-orthoortho1">
<span class="eqno">(7.4)<a class="headerlink" href="#equation-eq-orthoortho1" title="Permalink to this equation">¶</a></span>\[
\begin{aligned}
X v_i &amp; = \sigma_i u_i , \quad i = 1, \ldots, p \cr
X v_i &amp; = 0 ,  \quad i = p+1, \ldots, n
\end{aligned}
\]</div>
<p>Equations <a class="reference internal" href="#equation-eq-orthoortho1">(7.4)</a> tell how the transformation <span class="math notranslate nohighlight">\(X\)</span> maps a pair of orthonormal  vectors <span class="math notranslate nohighlight">\(v_i, v_j\)</span> for <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> both less than or equal to the rank <span class="math notranslate nohighlight">\(p\)</span> of <span class="math notranslate nohighlight">\(X\)</span> into a pair of orthonormal vectors <span class="math notranslate nohighlight">\(u_i, u_j\)</span>.</p>
<p>Equations <a class="reference internal" href="#equation-eq-xfour1a">(7.3)</a> assert that</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
{\mathcal C}(X) &amp; = {\mathcal C}(U_L) \cr
{\mathcal N}(X) &amp; = {\mathcal C} (V_R)
\end{aligned}
\]</div>
<p>Taking transposes on both sides of representation <a class="reference internal" href="#equation-eq-fullsvdpartition">(7.2)</a> implies</p>
<div class="math notranslate nohighlight">
\[
X^T \begin{bmatrix} U_L &amp; U_R \end{bmatrix} = \begin{bmatrix} V_L &amp; V_R \end{bmatrix} \begin{bmatrix} \Sigma_p &amp; 0 \cr 0 &amp; 0 \end{bmatrix}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-xfour1b">
<span class="eqno">(7.5)<a class="headerlink" href="#equation-eq-xfour1b" title="Permalink to this equation">¶</a></span>\[
\begin{aligned}
X^T U_L &amp; = V_L \Sigma_p \cr
X^T U_R &amp; = 0 
\end{aligned}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-orthoortho2">
<span class="eqno">(7.6)<a class="headerlink" href="#equation-eq-orthoortho2" title="Permalink to this equation">¶</a></span>\[ 
\begin{aligned}
X^T u_i &amp; = \sigma_i v_i, \quad i=1, \ldots, p \cr
X^T u_i &amp; = 0 \quad i= p+1, \ldots, m 
\end{aligned}
\]</div>
<p>Notice how equations <a class="reference internal" href="#equation-eq-orthoortho2">(7.6)</a> assert that  the transformation <span class="math notranslate nohighlight">\(X^T\)</span> maps a pairsof distinct orthonormal  vectors <span class="math notranslate nohighlight">\(u_i, u_j\)</span>  for <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> both less than or equal to the rank <span class="math notranslate nohighlight">\(p\)</span> of <span class="math notranslate nohighlight">\(X\)</span> into a pair of distinct orthonormal vectors <span class="math notranslate nohighlight">\(v_i, v_j\)</span> .</p>
<p>Equations <a class="reference internal" href="#equation-eq-xfour1b">(7.5)</a> assert that</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
{\mathcal R}(X) &amp; \equiv  {\mathcal C}(X^T) = {\mathcal C} (V_L) \cr
{\mathcal N}(X^T) &amp; = {\mathcal C}(U_R) 
\end{aligned}
\]</div>
<p>Thus, taken together, the systems of quations <a class="reference internal" href="#equation-eq-xfour1a">(7.3)</a> and <a class="reference internal" href="#equation-eq-xfour1b">(7.5)</a>
describe the  four fundamental subspaces of <span class="math notranslate nohighlight">\(X\)</span> in the following ways:</p>
<div class="math notranslate nohighlight" id="equation-eq-fourspacesvd">
<span class="eqno">(7.7)<a class="headerlink" href="#equation-eq-fourspacesvd" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}
\begin{aligned}
{\mathcal C}(X) &amp; = {\mathcal C}(U_L) \cr 
{\mathcal N}(X^T) &amp; = {\mathcal C}(U_R) \cr
{\mathcal R}(X) &amp; \equiv  {\mathcal C}(X^T) = {\mathcal C} (V_L) \cr
{\mathcal N}(X) &amp; = {\mathcal C} (V_R) \cr\\\end{aligned}
\end{aligned}\end{align} \]</div>
<p>Since <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are both orthonormal matrices, collection <a class="reference internal" href="#equation-eq-fourspacesvd">(7.7)</a> asserts that</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U_L\)</span> is an orthonormal basis for the column space of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(U_R\)</span> is an orthonormal basis for the null space of <span class="math notranslate nohighlight">\(X^T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V_L\)</span> is an orthonormal basis for the row space of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V_R\)</span> is an orthonormal basis for the null space of <span class="math notranslate nohighlight">\(X\)</span></p></li>
</ul>
<p>We have verified the four claims in <a class="reference internal" href="#equation-eq-fourspacesvd">(7.7)</a> simply  by performing the multiplications called for by the right side of <a class="reference internal" href="#equation-eq-fullsvdpartition">(7.2)</a> and reading them.</p>
<p>The claims in <a class="reference internal" href="#equation-eq-fourspacesvd">(7.7)</a> and the fact that <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are both unitary (i.e, orthonormal) matrices  imply
that</p>
<ul class="simple">
<li><p>the column space of <span class="math notranslate nohighlight">\(X\)</span> is orthogonal to the null space of of <span class="math notranslate nohighlight">\(X^T\)</span></p></li>
<li><p>the null space of <span class="math notranslate nohighlight">\(X\)</span> is orthogonal to the row space of <span class="math notranslate nohighlight">\(X\)</span></p></li>
</ul>
<p>Sometimes these properties are described with the following two pairs of orthogonal complement subspaces:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({\mathcal C}(X)\)</span> is the orthogonal complement of <span class="math notranslate nohighlight">\( {\mathcal N}(X^T)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\({\mathcal R}(X)\)</span> is the orthogonal complement  <span class="math notranslate nohighlight">\({\mathcal N}(X)\)</span></p></li>
</ul>
<p>Let’s do an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Define the matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="c1"># Compute the SVD of the matrix</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute the rank of the matrix</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># Print the rank of the matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Rank of matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;S: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>

<span class="c1"># Compute the four fundamental subspaces</span>
<span class="n">row_space</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank</span><span class="p">]</span>
<span class="n">col_space</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank</span><span class="p">]</span>
<span class="n">null_space</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="n">rank</span><span class="p">:]</span>
<span class="n">left_null_space</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="n">rank</span><span class="p">:]</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;U:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Column space:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">col_space</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Left null space:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">left_null_space</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;V.T:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Row space:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">row_space</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Right null space:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">null_space</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rank of matrix:
 2
S: 
 [2.69e+01 1.86e+00 7.83e-16 3.27e-16 4.69e-17]
U:
 [[-0.27 -0.73 -0.47  0.06 -0.42]
 [-0.35 -0.42  0.1  -0.18  0.81]
 [-0.43 -0.11  0.75 -0.27 -0.4 ]
 [-0.51  0.19  0.06  0.83  0.05]
 [-0.59  0.5  -0.45 -0.45 -0.04]]
Column space:
 [[-0.27 -0.35]
 [ 0.73  0.42]
 [ 0.02  0.06]
 [ 0.52 -0.83]
 [-0.37 -0.08]]
Left null space:
 [[-0.47  0.06 -0.42]
 [ 0.1  -0.18  0.81]
 [ 0.75 -0.27 -0.4 ]
 [ 0.06  0.83  0.05]
 [-0.45 -0.45 -0.04]]
V.T:
 [[-0.27  0.73  0.02  0.52 -0.37]
 [-0.35  0.42  0.06 -0.83 -0.08]
 [-0.43  0.11  0.29  0.18  0.82]
 [-0.51 -0.19 -0.83  0.06  0.04]
 [-0.59 -0.5   0.46  0.07 -0.42]]
Row space:
 [[-0.27 -0.35 -0.43 -0.51 -0.59]
 [-0.73 -0.42 -0.11  0.19  0.5 ]]
Right null space:
 [[-0.43  0.11  0.29  0.18  0.82]
 [-0.51 -0.19 -0.83  0.06  0.04]
 [-0.59 -0.5   0.46  0.07 -0.42]]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="full-and-reduced-svds">
<h2><span class="section-number">7.5. </span>Full and Reduced SVD’s<a class="headerlink" href="#full-and-reduced-svds" title="Permalink to this headline">¶</a></h2>
<p>Up to now we have described properties of a <strong>full</strong> SVD in which shapes of <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> are <span class="math notranslate nohighlight">\(\left(m, m\right)\)</span>, <span class="math notranslate nohighlight">\(\left(m, n\right)\)</span>, <span class="math notranslate nohighlight">\(\left(n, n\right)\)</span>, respectively.</p>
<p>There is  an alternative bookkeeping convention called an <strong>economy</strong> or <strong>reduced</strong> SVD in which the shapes of <span class="math notranslate nohighlight">\(U, \Sigma\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are different from what they are in a full SVD.</p>
<p>Thus, note that because we assume that <span class="math notranslate nohighlight">\(X\)</span> has rank <span class="math notranslate nohighlight">\(p\)</span>, there are only <span class="math notranslate nohighlight">\(p\)</span> nonzero singular values, where <span class="math notranslate nohighlight">\(p=\textrm{rank}(X)\leq\min\left(m, n\right)\)</span>.</p>
<p>A <strong>reduced</strong> SVD uses this fact to express <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> as matrices with shapes <span class="math notranslate nohighlight">\(\left(m, p\right)\)</span>, <span class="math notranslate nohighlight">\(\left(p, p\right)\)</span>, <span class="math notranslate nohighlight">\(\left( n, p\right)\)</span>.</p>
<p>You can read about reduced and full SVD here
<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html</a></p>
<p>For a full SVD,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
UU^T &amp;  = I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V = I
\end{aligned}
\]</div>
<p>But not all these properties hold for a  <strong>reduced</strong> SVD.</p>
<p>Which properties hold depend on whether we are in a <strong>tall-skinny</strong> case or a <strong>short-fat</strong> case.</p>
<ul class="simple">
<li><p>In a <strong>tall-skinny</strong> case in which <span class="math notranslate nohighlight">\(m &gt; &gt; n\)</span>, for a <strong>reduced</strong> SVD</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
UU^T &amp;  \neq I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V = I
\end{aligned}
\]</div>
<ul class="simple">
<li><p>In a <strong>short-fat</strong> case in which <span class="math notranslate nohighlight">\(m &lt; &lt; n\)</span>, for a <strong>reduced</strong> SVD</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
UU^T &amp;  = I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V \neq I
\end{aligned}
\]</div>
<p>When we study Dynamic Mode Decomposition below, we shall want to remember these properties when we use a  reduced SVD to compute some DMD representations.</p>
<p>Let’s do an  exercise  to compare <strong>full</strong> and <strong>reduced</strong> SVD’s.</p>
<p>To review,</p>
<ul class="simple">
<li><p>in a <strong>full</strong> SVD</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(m \times m\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is <span class="math notranslate nohighlight">\(m \times n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is <span class="math notranslate nohighlight">\(n \times n\)</span></p></li>
</ul>
</li>
<li><p>in a <strong>reduced</strong> SVD</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(m \times p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is <span class="math notranslate nohighlight">\(p\times p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is <span class="math notranslate nohighlight">\(n \times p\)</span></p></li>
</ul>
</li>
</ul>
<p>First, let’s study a case in which <span class="math notranslate nohighlight">\(m = 5 &gt; n = 2\)</span>.</p>
<p>(This is a small example of the <strong>tall-skinny</strong> case that will concern us when we study <strong>Dynamic Mode Decompositions</strong> below.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># full SVD</span>
<span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># economy SVD</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;U, S, V =&#39;</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>U, S, V =
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-0.46, -0.35, -0.14, -0.5 , -0.63],
        [-0.53, -0.35, -0.53,  0.25,  0.5 ],
        [-0.35, -0.38,  0.83,  0.1 ,  0.19],
        [-0.42,  0.41,  0.03,  0.67, -0.46],
        [-0.46,  0.67,  0.1 , -0.48,  0.32]]),
 array([1.76, 0.92]),
 array([[-0.51, -0.86],
        [ 0.86, -0.51]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Uhat, Shat, Vhat = &#39;</span><span class="p">)</span>
<span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Uhat, Shat, Vhat = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-0.46, -0.35],
        [-0.53, -0.35],
        [-0.35, -0.38],
        [-0.42,  0.41],
        [-0.46,  0.67]]),
 array([1.76, 0.92]),
 array([[-0.51, -0.86],
        [ 0.86, -0.51]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;rank of X = </span><span class="si">{</span><span class="n">rr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>rank of X = 2
</pre></div>
</div>
</div>
</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p>Where <span class="math notranslate nohighlight">\(U\)</span> is constructed via a full SVD, <span class="math notranslate nohighlight">\(U^T U = I_{p\times p}\)</span> and  <span class="math notranslate nohighlight">\(U U^T = I_{m \times m}\)</span></p></li>
<li><p>Where <span class="math notranslate nohighlight">\(\hat U\)</span> is constructed via a reduced SVD, although <span class="math notranslate nohighlight">\(\hat U^T \hat U = I_{p\times p}\)</span> it happens that  <span class="math notranslate nohighlight">\(\hat U \hat U^T \neq I_{m \times m}\)</span></p></li>
</ul>
<p>We illustrate these properties for our example with the following code cells.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UTU</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="nd">@U</span>
<span class="n">UUT</span> <span class="o">=</span> <span class="n">U</span><span class="nd">@U</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;UUT, UTU = &#39;</span><span class="p">)</span>
<span class="n">UUT</span><span class="p">,</span> <span class="n">UTU</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>UUT, UTU = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[ 1.00e+00, -7.91e-17, -1.25e-17,  3.03e-16,  3.75e-16],
        [-7.91e-17,  1.00e+00, -9.38e-17,  7.89e-17,  4.90e-17],
        [-1.25e-17, -9.38e-17,  1.00e+00,  2.72e-17,  5.53e-17],
        [ 3.03e-16,  7.89e-17,  2.72e-17,  1.00e+00,  1.73e-16],
        [ 3.75e-16,  4.90e-17,  5.53e-17,  1.73e-16,  1.00e+00]]),
 array([[ 1.00e+00, -2.63e-16, -8.09e-17,  2.62e-16,  3.89e-16],
        [-2.63e-16,  1.00e+00, -1.44e-16, -7.84e-17, -5.77e-18],
        [-8.09e-17, -1.44e-16,  1.00e+00,  1.11e-17,  1.27e-16],
        [ 2.62e-16, -7.84e-17,  1.11e-17,  1.00e+00,  1.51e-16],
        [ 3.89e-16, -5.77e-18,  1.27e-16,  1.51e-16,  1.00e+00]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UhatUhatT</span> <span class="o">=</span> <span class="n">Uhat</span><span class="nd">@Uhat</span><span class="o">.</span><span class="n">T</span>
<span class="n">UhatTUhat</span> <span class="o">=</span> <span class="n">Uhat</span><span class="o">.</span><span class="n">T</span><span class="nd">@Uhat</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;UhatUhatT, UhatTUhat= &#39;</span><span class="p">)</span>
<span class="n">UhatUhatT</span><span class="p">,</span> <span class="n">UhatTUhat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>UhatUhatT, UhatTUhat= 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[ 0.33,  0.37,  0.29,  0.05, -0.02],
        [ 0.37,  0.41,  0.32,  0.08,  0.01],
        [ 0.29,  0.32,  0.26, -0.01, -0.09],
        [ 0.05,  0.08, -0.01,  0.34,  0.46],
        [-0.02,  0.01, -0.09,  0.46,  0.66]]),
 array([[ 1.00e+00, -2.63e-16],
        [-2.63e-16,  1.00e+00]]))
</pre></div>
</div>
</div>
</div>
<p><strong>Remarks:</strong></p>
<p>The cells above illustrate application of the  <code class="docutils literal notranslate"><span class="pre">fullmatrices=True</span></code> and <code class="docutils literal notranslate"><span class="pre">full-matrices=False</span></code> options.
Using <code class="docutils literal notranslate"><span class="pre">full-matrices=False</span></code> returns a reduced singular value decomposition.</p>
<p>This option implements an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius
norm of the discrepancy between the approximating matrix and the matrix being approximated.</p>
<p>Optimality in this sense is  established in the celebrated Eckart–Young theorem. See <a class="reference external" href="https://en.wikipedia.org/wiki/Low-rank_approximation">https://en.wikipedia.org/wiki/Low-rank_approximation</a>.</p>
<p>When we study Dynamic Mode Decompositions below, it  will be important for us to remember the preceding properties of full and reduced SVD’s in such tall-skinny cases.</p>
<p>Now let’s turn to a short-fat case.</p>
<p>To illustrate this case,  we’ll set <span class="math notranslate nohighlight">\(m = 2 &lt; 5 = n \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># full SVD</span>
<span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># economy SVD</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;U, S, V = &#39;</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>U, S, V = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-0.73, -0.68],
        [-0.68,  0.73]]),
 array([1.96, 0.53]),
 array([[-0.55, -0.55, -0.28, -0.3 , -0.48],
        [-0.2 , -0.14, -0.35,  0.9 ,  0.03],
        [-0.52,  0.07,  0.82,  0.22, -0.11],
        [ 0.52, -0.76,  0.36,  0.13, -0.  ],
        [-0.35, -0.29, -0.04, -0.17,  0.87]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Uhat, Shat, Vhat = &#39;</span><span class="p">)</span>
<span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Uhat, Shat, Vhat = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[-0.73, -0.68],
        [-0.68,  0.73]]),
 array([1.96, 0.53]),
 array([[-0.55, -0.55, -0.28, -0.3 , -0.48],
        [-0.2 , -0.14, -0.35,  0.9 ,  0.03]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;rank X = </span><span class="si">{</span><span class="n">rr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>rank X = 2
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="polar-decomposition">
<h2><span class="section-number">7.6. </span>Polar Decomposition<a class="headerlink" href="#polar-decomposition" title="Permalink to this headline">¶</a></h2>
<p>A singular value decomposition (SVD) of <span class="math notranslate nohighlight">\(X\)</span> is related to a <strong>polar decomposition</strong> of <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[
X  = SQ   
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
 S &amp; = U\Sigma U^T \cr
Q &amp; = U V^T 
\end{aligned}
\]</div>
<p>Here</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span> is  a symmetric matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(Q\)</span> is an orthogonal matrix</p></li>
</ul>
</div>
<div class="section" id="principal-components-analysis-pca">
<h2><span class="section-number">7.7. </span>Principal Components Analysis (PCA)<a class="headerlink" href="#principal-components-analysis-pca" title="Permalink to this headline">¶</a></h2>
<p>Let’s begin with a case in which <span class="math notranslate nohighlight">\(n &gt;&gt; m\)</span>, so that we have many  more individuals <span class="math notranslate nohighlight">\(n\)</span> than attributes <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>The  matrix <span class="math notranslate nohighlight">\(X\)</span> is <strong>short and fat</strong>  in an  <span class="math notranslate nohighlight">\(n &gt;&gt; m\)</span> case as opposed to a <strong>tall and skinny</strong> case with <span class="math notranslate nohighlight">\(m &gt; &gt; n \)</span> to be discussed later.</p>
<p>We regard  <span class="math notranslate nohighlight">\(X\)</span> as an  <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of <strong>data</strong>:</p>
<div class="math notranslate nohighlight">
\[
X =  \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_n\end{bmatrix}
\]</div>
<p>where for <span class="math notranslate nohighlight">\(j = 1, \ldots, n\)</span> the column vector <span class="math notranslate nohighlight">\(X_j = \begin{bmatrix}X_{1j}\\X_{2j}\\\vdots\\X_{mj}\end{bmatrix}\)</span> is a  vector of observations on variables <span class="math notranslate nohighlight">\(\begin{bmatrix}x_1\\x_2\\\vdots\\x_m\end{bmatrix}\)</span>.</p>
<p>In a <strong>time series</strong> setting, we would think of columns <span class="math notranslate nohighlight">\(j\)</span> as indexing different <strong>times</strong> at which random variables are observed, while rows index different random variables.</p>
<p>In a <strong>cross section</strong> setting, we would think of columns <span class="math notranslate nohighlight">\(j\)</span> as indexing different <strong>individuals</strong> for  which random variables are observed, while rows index different <strong>attributes</strong>.</p>
<p>The number of positive singular values equals the rank of  matrix <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Arrange the singular values  in decreasing order.</p>
<p>Arrange   the positive singular values on the main diagonal of the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> of into a vector <span class="math notranslate nohighlight">\(\sigma_R\)</span>.</p>
<p>Set all other entries of <span class="math notranslate nohighlight">\(\Sigma\)</span> to zero.</p>
</div>
<div class="section" id="relationship-of-pca-to-svd">
<h2><span class="section-number">7.8. </span>Relationship of PCA to SVD<a class="headerlink" href="#relationship-of-pca-to-svd" title="Permalink to this headline">¶</a></h2>
<p>To relate a SVD to a PCA (principal component analysis) of data set <span class="math notranslate nohighlight">\(X\)</span>, first construct  the  SVD of the data matrix <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-pca1">
<span class="eqno">(7.8)<a class="headerlink" href="#equation-eq-pca1" title="Permalink to this equation">¶</a></span>\[
X = U \Sigma V^T = \sigma_1 U_1 V_1^T + \sigma_2 U_2 V_2^T + \cdots + \sigma_p U_p V_p^T
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
U=\begin{bmatrix}U_1|U_2|\ldots|U_m\end{bmatrix}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
V^T = \begin{bmatrix}V_1^T\\V_2^T\\\ldots\\V_n^T\end{bmatrix}
\end{split}\]</div>
<p>In equation <a class="reference internal" href="#equation-eq-pca1">(7.8)</a>, each of the <span class="math notranslate nohighlight">\(m \times n\)</span> matrices <span class="math notranslate nohighlight">\(U_{j}V_{j}^T\)</span> is evidently
of rank <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>Thus, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-pca2">
<span class="eqno">(7.9)<a class="headerlink" href="#equation-eq-pca2" title="Permalink to this equation">¶</a></span>\[\begin{split}
X = \sigma_1 \begin{pmatrix}U_{11}V_{1}^T\\U_{21}V_{1}^T\\\cdots\\U_{m1}V_{1}^T\\\end{pmatrix} + \sigma_2\begin{pmatrix}U_{12}V_{2}^T\\U_{22}V_{2}^T\\\cdots\\U_{m2}V_{2}^T\\\end{pmatrix}+\ldots + \sigma_p\begin{pmatrix}U_{1p}V_{p}^T\\U_{2p}V_{p}^T\\\cdots\\U_{mp}V_{p}^T\\\end{pmatrix}
\end{split}\]</div>
<p>Here is how we would interpret the objects in the  matrix equation <a class="reference internal" href="#equation-eq-pca2">(7.9)</a> in
a time series context:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(  \textrm{for each} \   k=1, \ldots, n \)</span>, the object <span class="math notranslate nohighlight">\(\lbrace V_{kj} \rbrace_{j=1}^n\)</span> is a time series   for the <span class="math notranslate nohighlight">\(k\)</span>th <strong>principal component</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(U_j = \begin{bmatrix}U_{1k}\\U_{2k}\\\ldots\\U_{mk}\end{bmatrix} \  k=1, \ldots, m\)</span>
is a vector of <strong>loadings</strong> of variables <span class="math notranslate nohighlight">\(X_i\)</span> on the <span class="math notranslate nohighlight">\(k\)</span>th principal component,  <span class="math notranslate nohighlight">\(i=1, \ldots, m\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_k \)</span> for each <span class="math notranslate nohighlight">\(k=1, \ldots, p\)</span> is the strength of <span class="math notranslate nohighlight">\(k\)</span>th <strong>principal component</strong>, where strength means contribution to the overall covariance of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
</div>
<div class="section" id="pca-with-eigenvalues-and-eigenvectors">
<h2><span class="section-number">7.9. </span>PCA with Eigenvalues and Eigenvectors<a class="headerlink" href="#pca-with-eigenvalues-and-eigenvectors" title="Permalink to this headline">¶</a></h2>
<p>We now  use an eigen decomposition of a sample covariance matrix to do PCA.</p>
<p>Let <span class="math notranslate nohighlight">\(X_{m \times n}\)</span> be our <span class="math notranslate nohighlight">\(m \times n\)</span> data matrix.</p>
<p>Let’s assume that sample means of all variables are zero.</p>
<p>We can assure  this  by <strong>pre-processing</strong> the data by subtracting sample means.</p>
<p>Define a sample covariance matrix <span class="math notranslate nohighlight">\(\Omega\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\Omega = XX^T
\]</div>
<p>Then use an eigen decomposition to represent <span class="math notranslate nohighlight">\(\Omega\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
\Omega =P\Lambda P^T
\]</div>
<p>Here</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\)</span> is <span class="math notranslate nohighlight">\(m×m\)</span> matrix of eigenvectors of <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix of eigenvalues of <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
</ul>
<p>We can then represent <span class="math notranslate nohighlight">\(X\)</span> as</p>
<div class="math notranslate nohighlight">
\[
X=P\epsilon
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[ 
\epsilon = P^{-1} X
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\epsilon\epsilon^T=\Lambda .
\]</div>
<p>We can verify that</p>
<div class="math notranslate nohighlight" id="equation-eq-xxo">
<span class="eqno">(7.10)<a class="headerlink" href="#equation-eq-xxo" title="Permalink to this equation">¶</a></span>\[
XX^T=P\Lambda P^T .
\]</div>
<p>It follows that we can represent the data matrix <span class="math notranslate nohighlight">\(X\)</span>  as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X=\begin{bmatrix}X_1|X_2|\ldots|X_m\end{bmatrix} =\begin{bmatrix}P_1|P_2|\ldots|P_m\end{bmatrix}
\begin{bmatrix}\epsilon_1\\\epsilon_2\\\ldots\\\epsilon_m\end{bmatrix} 
= P_1\epsilon_1+P_2\epsilon_2+\ldots+P_m\epsilon_m
\end{equation*}\]</div>
<p>To reconcile the preceding representation with the PCA that we had obtained earlier through the SVD, we first note that <span class="math notranslate nohighlight">\(\epsilon_j^2=\lambda_j\equiv\sigma^2_j\)</span>.</p>
<p>Now define  <span class="math notranslate nohighlight">\(\tilde{\epsilon_j} = \frac{\epsilon_j}{\sqrt{\lambda_j}}\)</span>,
which  implies that <span class="math notranslate nohighlight">\(\tilde{\epsilon}_j\tilde{\epsilon}_j^T=1\)</span>.</p>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X&amp;=\sqrt{\lambda_1}P_1\tilde{\epsilon_1}+\sqrt{\lambda_2}P_2\tilde{\epsilon_2}+\ldots+\sqrt{\lambda_m}P_m\tilde{\epsilon_m}\\
&amp;=\sigma_1P_1\tilde{\epsilon_2}+\sigma_2P_2\tilde{\epsilon_2}+\ldots+\sigma_mP_m\tilde{\epsilon_m} ,
\end{aligned}
\end{split}\]</div>
<p>which  agrees with</p>
<div class="math notranslate nohighlight">
\[
X=\sigma_1U_1{V_1}^{T}+\sigma_2 U_2{V_2}^{T}+\ldots+\sigma_{r} U_{r}{V_{r}}^{T}
\]</div>
<p>provided that  we set</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U_j=P_j\)</span> (a vector of  loadings of variables on principal component <span class="math notranslate nohighlight">\(j\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\({V_k}^{T}=\tilde{\epsilon_k}\)</span> (the <span class="math notranslate nohighlight">\(k\)</span>th principal component)</p></li>
</ul>
<p>Because  there are alternative algorithms for  computing  <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(U\)</span> for  given a data matrix <span class="math notranslate nohighlight">\(X\)</span>, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.</p>
<p>We can resolve such ambiguities about  <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(P\)</span> by</p>
<ol class="simple">
<li><p>sorting eigenvalues and singular values in descending order</p></li>
<li><p>imposing positive diagonals on <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(U\)</span> and adjusting signs in <span class="math notranslate nohighlight">\(V^T\)</span> accordingly</p></li>
</ol>
</div>
<div class="section" id="connections">
<h2><span class="section-number">7.10. </span>Connections<a class="headerlink" href="#connections" title="Permalink to this headline">¶</a></h2>
<p>To pull things together, it is useful to assemble and compare some formulas presented above.</p>
<p>First, consider an  SVD of an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[
X = U\Sigma V^T
\]</div>
<p>Compute:</p>
<div class="math notranslate nohighlight" id="equation-eq-xxcompare">
<span class="eqno">(7.11)<a class="headerlink" href="#equation-eq-xxcompare" title="Permalink to this equation">¶</a></span>\[
\begin{aligned}
XX^T&amp;=U\Sigma V^TV\Sigma^T U^T\cr
&amp;\equiv U\Sigma\Sigma^TU^T\cr
&amp;\equiv U\Lambda U^T
\end{aligned}
\]</div>
<p>Compare representation <a class="reference internal" href="#equation-eq-xxcompare">(7.11)</a> with equation <a class="reference internal" href="#equation-eq-xxo">(7.10)</a> above.</p>
<p>Evidently, <span class="math notranslate nohighlight">\(U\)</span> in the SVD is the matrix <span class="math notranslate nohighlight">\(P\)</span>  of
eigenvectors of <span class="math notranslate nohighlight">\(XX^T\)</span> and <span class="math notranslate nohighlight">\(\Sigma \Sigma^T\)</span> is the matrix <span class="math notranslate nohighlight">\(\Lambda\)</span> of eigenvalues.</p>
<p>Second, let’s compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X^TX &amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T{\Sigma}V^T
\end{aligned}
\end{split}\]</div>
<p>Thus, the matrix <span class="math notranslate nohighlight">\(V\)</span> in the SVD is the matrix of eigenvectors of <span class="math notranslate nohighlight">\(X^TX\)</span></p>
<p>Summarizing and fitting things together, we have the eigen decomposition of the sample
covariance matrix</p>
<div class="math notranslate nohighlight">
\[
X X^T = P \Lambda P^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is an orthogonal matrix.</p>
<p>Further, from the SVD of <span class="math notranslate nohighlight">\(X\)</span>, we know that</p>
<div class="math notranslate nohighlight">
\[
X X^T = U \Sigma \Sigma^T U^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(U\)</span> is an orthonal matrix.</p>
<p>Thus, <span class="math notranslate nohighlight">\(P = U\)</span> and we have the representation of <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[
X = P \epsilon = U \Sigma V^T
\]</div>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[
U^T X = \Sigma V^T = \epsilon
\]</div>
<p>Note that the preceding implies that</p>
<div class="math notranslate nohighlight">
\[
\epsilon \epsilon^T = \Sigma V^T V \Sigma^T = \Sigma \Sigma^T = \Lambda ,
\]</div>
<p>so that everything fits together.</p>
<p>Below we define a class <code class="docutils literal notranslate"><span class="pre">DecomAnalysis</span></code> that wraps  PCA and SVD for a given a data matrix <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecomAnalysis</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class for conducting PCA and SVD.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">n_component</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Ω</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_component</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_component</span> <span class="o">=</span> <span class="n">n_component</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_component</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span>

    <span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">𝜆</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ω</span><span class="p">)</span>    <span class="c1"># columns of P are eigenvectors</span>

        <span class="n">ind</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">𝜆</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">𝜆</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># sort by eigenvalues</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span> <span class="o">=</span> <span class="n">𝜆</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">P</span><span class="p">[:,</span> <span class="n">ind</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">diag_sign</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Λ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">explained_ratio_pca</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># compute the N by T matrix of principal components </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">𝜖</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span>

        <span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">𝜖</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜖</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_pca</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">𝜖</span>

    <span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">U</span><span class="p">,</span> <span class="n">𝜎</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="n">ind</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">𝜎</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">𝜎</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># sort by eigenvalues</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">𝜎</span> <span class="o">=</span> <span class="n">𝜎</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="n">ind</span><span class="p">]</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">diag_sign</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">D</span>
        <span class="n">VT</span><span class="p">[:</span><span class="n">d</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">D</span> <span class="o">@</span> <span class="n">VT</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">VT</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span><span class="p">[:</span><span class="n">d</span><span class="p">,</span> <span class="p">:</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">𝜎</span><span class="p">)</span>

        <span class="n">𝜎_sq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜎</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explained_ratio_svd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">𝜎_sq</span><span class="p">)</span> <span class="o">/</span> <span class="n">𝜎_sq</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># slicing matrices by the number of components to use</span>
        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">Σ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">VT</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_svd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">Σ</span> <span class="o">@</span> <span class="n">VT</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_component</span><span class="p">):</span>

        <span class="c1"># pca</span>
        <span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">𝜖</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜖</span><span class="p">[:</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_pca</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">𝜖</span>

        <span class="c1"># svd</span>
        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">Σ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span><span class="p">[:</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">VT</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span><span class="p">[:</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_svd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">Σ</span> <span class="o">@</span> <span class="n">VT</span>

<span class="k">def</span> <span class="nf">diag_sign</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="s2">&quot;Compute the signs of the diagonal of matrix A&quot;</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">D</span>
</pre></div>
</div>
</div>
</div>
<p>We also define a function that prints out information so that we can compare  decompositions
obtained by different algorithms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare_pca_svd</span><span class="p">(</span><span class="n">da</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the outcomes of PCA and SVD.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">da</span><span class="o">.</span><span class="n">pca</span><span class="p">()</span>
    <span class="n">da</span><span class="o">.</span><span class="n">svd</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvalues and Singular values</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;λ = </span><span class="si">{</span><span class="n">da</span><span class="o">.</span><span class="n">λ</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;σ^2 = </span><span class="si">{</span><span class="n">da</span><span class="o">.</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># loading matrices</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;loadings&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">P</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;P&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># principal components</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;principal components&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">ε</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ε&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">VT</span><span class="p">[:</span><span class="n">da</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">λ</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;$V^T*\sqrt{\lambda}$&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>For an example  PCA applied to analyzing the structure of intelligence tests see this lecture <a class="reference internal" href="multivariate_normal.html"><span class="doc">Multivariable Normal Distribution</span></a>.</p>
<p>Look at  parts of that lecture that describe and illustrate the classic factor analysis model.</p>
</div>
<div class="section" id="vector-autoregressions">
<h2><span class="section-number">7.11. </span>Vector Autoregressions<a class="headerlink" href="#vector-autoregressions" title="Permalink to this headline">¶</a></h2>
<p>We want to fit a <strong>first-order vector autoregression</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-varfirstorder">
<span class="eqno">(7.12)<a class="headerlink" href="#equation-eq-varfirstorder" title="Permalink to this equation">¶</a></span>\[
X_{t+1} = A X_t + C \epsilon_{t+1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_{t+1}\)</span> is the time <span class="math notranslate nohighlight">\(t+1\)</span> instance of an i.i.d. <span class="math notranslate nohighlight">\(m \times 1\)</span> random vector with mean vector
zero and identity  covariance matrix and where
the <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector <span class="math notranslate nohighlight">\( X_t \)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-xvector">
<span class="eqno">(7.13)<a class="headerlink" href="#equation-eq-xvector" title="Permalink to this equation">¶</a></span>\[
X_t = \begin{bmatrix}  X_{1,t} &amp; X_{2,t} &amp; \cdots &amp; X_{m,t}     \end{bmatrix}^T
\]</div>
<p>and where <span class="math notranslate nohighlight">\( T \)</span> again denotes complex transposition and <span class="math notranslate nohighlight">\( X_{i,t} \)</span> is an observation on variable <span class="math notranslate nohighlight">\( i \)</span> at time <span class="math notranslate nohighlight">\( t \)</span>.</p>
<p>We want to fit equation <a class="reference internal" href="#equation-eq-varfirstorder">(7.12)</a>.</p>
<p>Our data are organized in   an <span class="math notranslate nohighlight">\( m \times (n+1) \)</span> matrix  <span class="math notranslate nohighlight">\( \tilde X \)</span></p>
<div class="math notranslate nohighlight">
\[
\tilde X =  \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_n \mid X_{n+1} \end{bmatrix}
\]</div>
<p>where for <span class="math notranslate nohighlight">\( t = 1, \ldots, n +1 \)</span>,  the <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector <span class="math notranslate nohighlight">\( X_t \)</span> is given by <a class="reference internal" href="#equation-eq-xvector">(7.13)</a>.</p>
<p>Thus, we want to estimate a  system  <a class="reference internal" href="#equation-eq-varfirstorder">(7.12)</a> that consists of <span class="math notranslate nohighlight">\( m \)</span> least squares regressions of <strong>everything</strong> on one lagged value of <strong>everything</strong>.</p>
<p>The <span class="math notranslate nohighlight">\(i\)</span>’th equation of <a class="reference internal" href="#equation-eq-varfirstorder">(7.12)</a> is a regression of <span class="math notranslate nohighlight">\(X_{i,t+1}\)</span> on the vector <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p>We proceed as follows.</p>
<p>From <span class="math notranslate nohighlight">\( \tilde X \)</span>,  we  form two <span class="math notranslate nohighlight">\(m \times n\)</span> matrices</p>
<div class="math notranslate nohighlight">
\[
X =  \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_{n}\end{bmatrix}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
X' =  \begin{bmatrix} X_2 \mid X_3 \mid \cdots \mid X_{n+1}\end{bmatrix}
\]</div>
<p>Here <span class="math notranslate nohighlight">\( ' \)</span>  is part of the name of the matrix <span class="math notranslate nohighlight">\( X' \)</span> and does not indicate matrix transposition.</p>
<p>We continue to use  <span class="math notranslate nohighlight">\(\cdot^T\)</span> to denote matrix transposition or its extension to complex matrices.</p>
<p>In forming <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( X' \)</span>, we have in each case  dropped a column from <span class="math notranslate nohighlight">\( \tilde X \)</span>,  the last column in the case of <span class="math notranslate nohighlight">\( X \)</span>, and  the first column in the case of <span class="math notranslate nohighlight">\( X' \)</span>.</p>
<p>Evidently, <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( X' \)</span> are both <span class="math notranslate nohighlight">\( m \times  n \)</span> matrices.</p>
<p>We denote the rank of <span class="math notranslate nohighlight">\( X \)</span> as <span class="math notranslate nohighlight">\( p \leq \min(m, n)  \)</span>.</p>
<p>Two possible cases are</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( n &gt; &gt; m\)</span>, so that we have many more time series  observations <span class="math notranslate nohighlight">\(n\)</span> than variables <span class="math notranslate nohighlight">\(m\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(m &gt; &gt; n\)</span>, so that we have many more variables <span class="math notranslate nohighlight">\(m \)</span> than time series observations <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<p>At a general level that includes both of these special cases, a common formula describes the least squares estimator <span class="math notranslate nohighlight">\(\hat A\)</span> of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>But important  details differ.</p>
<p>The common formula is</p>
<div class="math notranslate nohighlight" id="equation-eq-commona">
<span class="eqno">(7.14)<a class="headerlink" href="#equation-eq-commona" title="Permalink to this equation">¶</a></span>\[ 
\hat A = X' X^+ 
\]</div>
<p>where <span class="math notranslate nohighlight">\(X^+\)</span> is the pseudo-inverse of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>To read about the <strong>Moore-Penrose pseudo-inverse</strong> please see <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose pseudo-inverse</a></p>
<p>Applicable formulas for the pseudo-inverse differ for our two cases.</p>
<p><strong>Short-Fat Case:</strong></p>
<p>When <span class="math notranslate nohighlight">\( n &gt; &gt; m\)</span>, so that we have many more time series  observations <span class="math notranslate nohighlight">\(n\)</span> than variables <span class="math notranslate nohighlight">\(m\)</span> and when
<span class="math notranslate nohighlight">\(X\)</span> has linearly independent <strong>rows</strong>, <span class="math notranslate nohighlight">\(X X^T\)</span> has an inverse and the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span> is</p>
<div class="math notranslate nohighlight">
\[
X^+ = X^T (X X^T)^{-1} 
\]</div>
<p>Here <span class="math notranslate nohighlight">\(X^+\)</span> is a <strong>right-inverse</strong> that verifies <span class="math notranslate nohighlight">\( X X^+ = I_{m \times m}\)</span>.</p>
<p>In this case, our formula <a class="reference internal" href="#equation-eq-commona">(7.14)</a> for the least-squares estimator of the population matrix of regression coefficients  <span class="math notranslate nohighlight">\(A\)</span> becomes</p>
<div class="math notranslate nohighlight" id="equation-eq-ahatform101">
<span class="eqno">(7.15)<a class="headerlink" href="#equation-eq-ahatform101" title="Permalink to this equation">¶</a></span>\[ 
\hat A = X' X^T (X X^T)^{-1}
\]</div>
<p>This  formula for least-squares regression coefficients widely used in econometrics.</p>
<p>For example, it is used  to estimate vector autorgressions.</p>
<p>The right side of formula <a class="reference internal" href="#equation-eq-ahatform101">(7.15)</a> is proportional to the empirical cross second moment matrix of <span class="math notranslate nohighlight">\(X_{t+1}\)</span> and <span class="math notranslate nohighlight">\(X_t\)</span> times the inverse
of the second moment matrix of <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p><strong>Tall-Skinny Case:</strong></p>
<p>When <span class="math notranslate nohighlight">\(m &gt; &gt; n\)</span>, so that we have many more attributes <span class="math notranslate nohighlight">\(m \)</span> than time series observations <span class="math notranslate nohighlight">\(n\)</span> and when <span class="math notranslate nohighlight">\(X\)</span> has linearly independent <strong>columns</strong>,
<span class="math notranslate nohighlight">\(X^T X\)</span> has an inverse and the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span> is</p>
<div class="math notranslate nohighlight">
\[
X^+ = (X^T X)^{-1} X^T
\]</div>
<p>Here  <span class="math notranslate nohighlight">\(X^+\)</span> is a <strong>left-inverse</strong> that verifies <span class="math notranslate nohighlight">\(X^+ X = I_{n \times n}\)</span>.</p>
<p>In this case, our formula  <a class="reference internal" href="#equation-eq-commona">(7.14)</a> for a least-squares estimator of <span class="math notranslate nohighlight">\(A\)</span> becomes</p>
<div class="math notranslate nohighlight" id="equation-eq-hataversion0">
<span class="eqno">(7.16)<a class="headerlink" href="#equation-eq-hataversion0" title="Permalink to this equation">¶</a></span>\[
\hat A = X' (X^T X)^{-1} X^T
\]</div>
<p>Please compare formulas <a class="reference internal" href="#equation-eq-ahatform101">(7.15)</a> and <a class="reference internal" href="#equation-eq-hataversion0">(7.16)</a> for <span class="math notranslate nohighlight">\(\hat A\)</span>.</p>
<p>Here we are interested in formula <a class="reference internal" href="#equation-eq-hataversion0">(7.16)</a>.</p>
<p>The <span class="math notranslate nohighlight">\( i \)</span>th  row of <span class="math notranslate nohighlight">\( \hat A \)</span> is an <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector of regression coefficients of <span class="math notranslate nohighlight">\( X_{i,t+1} \)</span> on <span class="math notranslate nohighlight">\( X_{j,t}, j = 1, \ldots, m \)</span>.</p>
<p>If we use formula <a class="reference internal" href="#equation-eq-hataversion0">(7.16)</a> to calculate <span class="math notranslate nohighlight">\(\hat A X\)</span> we find that</p>
<div class="math notranslate nohighlight">
\[
\hat A X = X'
\]</div>
<p>so that the regression equation <strong>fits perfectly</strong>.</p>
<p>This is the usual outcome in an <strong>underdetermined least-squares</strong> model.</p>
<p>To reiterate, in our <strong>tall-skinny</strong> case  in which we have a number <span class="math notranslate nohighlight">\(n\)</span> of observations   that is small relative to the number <span class="math notranslate nohighlight">\(m\)</span> of
attributes that appear in the vector <span class="math notranslate nohighlight">\(X_t\)</span>,  we want to fit equation <a class="reference internal" href="#equation-eq-varfirstorder">(7.12)</a>.</p>
<p>To  offer  ideas about how we can efficiently calculate the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span>, as our  estimator <span class="math notranslate nohighlight">\(\hat A\)</span> of <span class="math notranslate nohighlight">\(A\)</span> we form an  <span class="math notranslate nohighlight">\(m \times m\)</span> matrix that  solves the least-squares best-fit problem</p>
<div class="math notranslate nohighlight" id="equation-eq-alseqn">
<span class="eqno">(7.17)<a class="headerlink" href="#equation-eq-alseqn" title="Permalink to this equation">¶</a></span>\[ 
\hat A = \textrm{argmin}_{\check A} || X' - \check  A X ||_F   
\]</div>
<p>where <span class="math notranslate nohighlight">\(|| \cdot ||_F\)</span> denotes the Frobenius (or Euclidean) norm of a matrix.</p>
<p>The Frobenius norm is defined as</p>
<div class="math notranslate nohighlight">
\[
 ||A||_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^m |A_{ij}|^2 }
\]</div>
<p>The minimizer of the right side of equation <a class="reference internal" href="#equation-eq-alseqn">(7.17)</a> is</p>
<div class="math notranslate nohighlight" id="equation-eq-hataform">
<span class="eqno">(7.18)<a class="headerlink" href="#equation-eq-hataform" title="Permalink to this equation">¶</a></span>\[
\hat A =  X'  X^{+}  
\]</div>
<p>where the (possibly huge) <span class="math notranslate nohighlight">\( n \times m \)</span> matrix <span class="math notranslate nohighlight">\( X^{+} = (X^T X)^{-1} X^T\)</span> is again a pseudo-inverse of <span class="math notranslate nohighlight">\( X \)</span>.</p>
<p>For some situations that we are interested in, <span class="math notranslate nohighlight">\(X^T X \)</span> can be close to singular, a situation that can make some numerical algorithms  be error-prone.</p>
<p>To acknowledge that possibility, we’ll use  efficient algorithms for computing and for constructing reduced rank approximations of  <span class="math notranslate nohighlight">\(\hat A\)</span> in formula <a class="reference internal" href="#equation-eq-hataversion0">(7.16)</a>.</p>
<p>The <span class="math notranslate nohighlight">\( i \)</span>th  row of <span class="math notranslate nohighlight">\( \hat A \)</span> is an <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector of regression coefficients of <span class="math notranslate nohighlight">\( X_{i,t+1} \)</span> on <span class="math notranslate nohighlight">\( X_{j,t}, j = 1, \ldots, m \)</span>.</p>
<p>An efficient way to compute the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span> is to start with  a singular value decomposition</p>
<div class="math notranslate nohighlight" id="equation-eq-svddmd">
<span class="eqno">(7.19)<a class="headerlink" href="#equation-eq-svddmd" title="Permalink to this equation">¶</a></span>\[
X =  U \Sigma  V^T 
\]</div>
<p>where we remind ourselves that for a <strong>reduced</strong> SVD, <span class="math notranslate nohighlight">\(X\)</span> is an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of data, <span class="math notranslate nohighlight">\(U\)</span> is an <span class="math notranslate nohighlight">\(m \times p\)</span> matrix, <span class="math notranslate nohighlight">\(\Sigma\)</span>  is a <span class="math notranslate nohighlight">\(p \times p\)</span> matrix, and <span class="math notranslate nohighlight">\(V\)</span> is an <span class="math notranslate nohighlight">\(n \times p\)</span> matrix.</p>
<p>We can    efficiently  construct the pertinent pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span>
by recognizing the following string of equalities.</p>
<div class="math notranslate nohighlight" id="equation-eq-efficientpseudoinverse">
<span class="eqno">(7.20)<a class="headerlink" href="#equation-eq-efficientpseudoinverse" title="Permalink to this equation">¶</a></span>\[\begin{split}
\begin{aligned}
X^{+} &amp; = (X^T X)^{-1} X^T \\
  &amp; = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T \\
  &amp; = (V \Sigma \Sigma V^T)^{-1} V \Sigma U^T \\
  &amp; = V \Sigma^{-1} \Sigma^{-1} V^T V \Sigma U^T \\
  &amp; = V \Sigma^{-1} U^T 
\end{aligned}
\end{split}\]</div>
<p>(Since we are in the <span class="math notranslate nohighlight">\(m &gt; &gt; n\)</span> case in which <span class="math notranslate nohighlight">\(V^T V = I_{p \times p}\)</span> in a reduced SVD, we can use the preceding
string of equalities for a reduced SVD as well as for a full SVD.)</p>
<p>Thus, we shall  construct a pseudo-inverse <span class="math notranslate nohighlight">\( X^+ \)</span>  of <span class="math notranslate nohighlight">\( X \)</span> by using
a singular value decomposition of <span class="math notranslate nohighlight">\(X\)</span> in equation <a class="reference internal" href="#equation-eq-svddmd">(7.19)</a>  to compute</p>
<div class="math notranslate nohighlight" id="equation-eq-xplusformula">
<span class="eqno">(7.21)<a class="headerlink" href="#equation-eq-xplusformula" title="Permalink to this equation">¶</a></span>\[
X^{+} =  V \Sigma^{-1}  U^T 
\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\( \Sigma^{-1} \)</span> is constructed by replacing each non-zero element of <span class="math notranslate nohighlight">\( \Sigma \)</span> with <span class="math notranslate nohighlight">\( \sigma_j^{-1} \)</span>.</p>
<p>We can  use formula <a class="reference internal" href="#equation-eq-xplusformula">(7.21)</a>   together with formula <a class="reference internal" href="#equation-eq-hataform">(7.18)</a> to compute the matrix  <span class="math notranslate nohighlight">\( \hat A \)</span> of regression coefficients.</p>
<p>Thus, our  estimator <span class="math notranslate nohighlight">\(\hat A = X' X^+\)</span> of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix of coefficients <span class="math notranslate nohighlight">\(A\)</span>    is</p>
<div class="math notranslate nohighlight" id="equation-eq-ahatsvdformula">
<span class="eqno">(7.22)<a class="headerlink" href="#equation-eq-ahatsvdformula" title="Permalink to this equation">¶</a></span>\[
\hat A = X' V \Sigma^{-1}  U^T 
\]</div>
</div>
<div class="section" id="dynamic-mode-decomposition-dmd">
<h2><span class="section-number">7.12. </span>Dynamic Mode Decomposition (DMD)<a class="headerlink" href="#dynamic-mode-decomposition-dmd" title="Permalink to this headline">¶</a></h2>
<p>We turn to the <strong>tall and skinny</strong> case  associated with <strong>Dynamic Mode Decomposition</strong>, the case in  which <span class="math notranslate nohighlight">\( m &gt;&gt;n \)</span>.</p>
<p>Here an <span class="math notranslate nohighlight">\( m \times n \)</span>  data matrix <span class="math notranslate nohighlight">\( \tilde X \)</span> contains many more attributes <span class="math notranslate nohighlight">\( m \)</span> than individuals <span class="math notranslate nohighlight">\( n \)</span>.</p>
<p>Dynamic mode decomposition was introduced by <span id="id1">[<a class="reference internal" href="zreferences.html#id16" title="Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics, 656:5–28, 2010.">Sch10</a>]</span>,</p>
<p>You can read  about Dynamic Mode Decomposition here <span id="id2">[<a class="reference internal" href="zreferences.html#id39" title="J. N. Kutz, S. L. Brunton, Brunton B. W, and J. L. Proctor. Dynamic mode decomposition: data-driven modeling of complex systems. SIAM, 2016.">KBBWP16</a>]</span> and here [<a class="reference external" href="https://python.quantecon.org/zreferences.html#id25">BK19</a>] (section 7.2).</p>
<p>The key idea underlying   <strong>Dynamic Mode Decomposition</strong> (DMD) is  to compute a rank <span class="math notranslate nohighlight">\( r &lt; p &gt; \)</span> approximation to the least square regression coefficients <span class="math notranslate nohighlight">\( \hat A \)</span> that we  described above by formula <a class="reference internal" href="#equation-eq-ahatsvdformula">(7.22)</a>.</p>
<p>We’ll  build up gradually  to a formulation that is typically used in applications of DMD.</p>
<p>We’ll do this by describing three  alternative representations of our first-order linear dynamic system, i.e.,
our vector autoregression.</p>
<p><strong>Guide to three representations:</strong> In practice, we’ll be interested in Representation 3.  We present the first 2 in order to set the stage for some intermediate steps that might help us understand what is under the hood of Representation 3.  In applications, we’ll use only a small  subset of the DMD to approximate dynamics.  To do that, we’ll want to use the  <strong>reduced</strong>  SVD’s affiliated with representation 3, not the <strong>full</strong> SVD’s affiliated with representations 1 and 2.</p>
<p><strong>Guide to impatient reader:</strong> In our applications, we’ll be using Representation 3.  You might want to skip
the stage-setting representations 1 and 2 on first reading.</p>
</div>
<div class="section" id="representation-1">
<h2><span class="section-number">7.13. </span>Representation 1<a class="headerlink" href="#representation-1" title="Permalink to this headline">¶</a></h2>
<p>In this representation, we shall use a <strong>full</strong> SVD of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>We use the <span class="math notranslate nohighlight">\(m\)</span>  <strong>columns</strong> of <span class="math notranslate nohighlight">\(U\)</span>, and thus the <span class="math notranslate nohighlight">\(m\)</span> <strong>rows</strong> of <span class="math notranslate nohighlight">\(U^T\)</span>,  to define   a <span class="math notranslate nohighlight">\(m \times 1\)</span>  vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-tildexdef2">
<span class="eqno">(7.23)<a class="headerlink" href="#equation-eq-tildexdef2" title="Permalink to this equation">¶</a></span>\[
\tilde b_t = U^T X_t .
\]</div>
<p>The original  data <span class="math notranslate nohighlight">\(X_t\)</span> can be represented as</p>
<div class="math notranslate nohighlight" id="equation-eq-xdecoder">
<span class="eqno">(7.24)<a class="headerlink" href="#equation-eq-xdecoder" title="Permalink to this equation">¶</a></span>\[ 
X_t = U \tilde b_t
\]</div>
<p>(Here we use <span class="math notranslate nohighlight">\(b\)</span> to remind ourselves that we are creating a <strong>basis</strong> vector.)</p>
<p>Since we are now using a <strong>full</strong> SVD, <span class="math notranslate nohighlight">\(U U^T = I_{m \times m}\)</span>.</p>
<p>So it follows from equation <a class="reference internal" href="#equation-eq-tildexdef2">(7.23)</a> that we can reconstruct  <span class="math notranslate nohighlight">\(X_t\)</span> from <span class="math notranslate nohighlight">\(\tilde b_t\)</span>.</p>
<p>In particular,</p>
<ul class="simple">
<li><p>Equation <a class="reference internal" href="#equation-eq-tildexdef2">(7.23)</a> serves as an <strong>encoder</strong> that  <strong>rotates</strong> the <span class="math notranslate nohighlight">\(m \times 1\)</span> vector <span class="math notranslate nohighlight">\(X_t\)</span> to become an <span class="math notranslate nohighlight">\(m \times 1\)</span> vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span></p></li>
<li><p>Equation <a class="reference internal" href="#equation-eq-xdecoder">(7.24)</a> serves as a <strong>decoder</strong> that <strong>reconstructs</strong> the <span class="math notranslate nohighlight">\(m \times 1\)</span> vector <span class="math notranslate nohighlight">\(X_t\)</span> by rotating  the <span class="math notranslate nohighlight">\(m \times 1\)</span> vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span></p></li>
</ul>
<p>Define a  transition matrix for an <span class="math notranslate nohighlight">\(m \times 1\)</span> basis vector  <span class="math notranslate nohighlight">\(\tilde b_t\)</span> by</p>
<div class="math notranslate nohighlight" id="equation-eq-atilde0">
<span class="eqno">(7.25)<a class="headerlink" href="#equation-eq-atilde0" title="Permalink to this equation">¶</a></span>\[ 
\tilde A = U^T \hat A U 
\]</div>
<p>We can  recover <span class="math notranslate nohighlight">\(\hat A\)</span> from</p>
<div class="math notranslate nohighlight">
\[
\hat A = U \tilde A U^T 
\]</div>
<p>Dynamics of the  <span class="math notranslate nohighlight">\(m \times 1\)</span> basis vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span> are governed by</p>
<div class="math notranslate nohighlight">
\[
\tilde b_{t+1} = \tilde A \tilde b_t 
\]</div>
<p>To construct forecasts <span class="math notranslate nohighlight">\(\overline X_t\)</span> of  future values of <span class="math notranslate nohighlight">\(X_t\)</span> conditional on <span class="math notranslate nohighlight">\(X_1\)</span>, we can apply  decoders
(i.e., rotators) to both sides of this
equation and deduce</p>
<div class="math notranslate nohighlight">
\[
\overline X_{t+1} = U \tilde A^t U^T X_1
\]</div>
<p>where we use <span class="math notranslate nohighlight">\(\overline X_{t+1}, t \geq 1 \)</span> to denote a forecast.</p>
</div>
<div class="section" id="representation-2">
<h2><span class="section-number">7.14. </span>Representation 2<a class="headerlink" href="#representation-2" title="Permalink to this headline">¶</a></h2>
<p>This representation is related to  one originally proposed by  <span id="id3">[<a class="reference internal" href="zreferences.html#id16" title="Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics, 656:5–28, 2010.">Sch10</a>]</span>.</p>
<p>It can be regarded as an intermediate step on the way  to obtaining  a related   representation 3 to be presented later</p>
<p>As with Representation 1, we continue to</p>
<ul class="simple">
<li><p>use a <strong>full</strong> SVD and <strong>not</strong> a reduced SVD</p></li>
</ul>
<p>As we observed and illustrated  earlier in this lecture</p>
<ul class="simple">
<li><p>(a) for a full SVD <span class="math notranslate nohighlight">\(U U^T = I_{m \times m} \)</span> and <span class="math notranslate nohighlight">\(U^T U = I_{p \times p}\)</span> are both identity matrices</p></li>
<li><p>(b)  for  a reduced SVD of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(U^T U \)</span> is not an identity matrix.</p></li>
</ul>
<p>As we shall see later, a full SVD is  too confining for what we ultimately want to do, namely,  cope with situations in which  <span class="math notranslate nohighlight">\(U^T U\)</span> is <strong>not</strong> an identity matrix because we  use a reduced SVD of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>But for now, let’s proceed under the assumption that we are using a full SVD so that  both of the  preceding two  requirements (a) and (b) are satisfied.</p>
<p>Form an eigendecomposition of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix <span class="math notranslate nohighlight">\(\tilde A = U^T \hat A U\)</span> defined in equation <a class="reference internal" href="#equation-eq-atilde0">(7.25)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-tildeaeigen">
<span class="eqno">(7.26)<a class="headerlink" href="#equation-eq-tildeaeigen" title="Permalink to this equation">¶</a></span>\[
\tilde A = W \Lambda W^{-1} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix of eigenvalues and <span class="math notranslate nohighlight">\(W\)</span> is an <span class="math notranslate nohighlight">\(m \times m\)</span>
matrix whose columns are eigenvectors  corresponding to rows (eigenvalues) in
<span class="math notranslate nohighlight">\(\Lambda\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(U U^T = I_{m \times m}\)</span>, as is true with a full SVD of <span class="math notranslate nohighlight">\(X\)</span>, it follows that</p>
<div class="math notranslate nohighlight" id="equation-eq-eqeigahat">
<span class="eqno">(7.27)<a class="headerlink" href="#equation-eq-eqeigahat" title="Permalink to this equation">¶</a></span>\[ 
\hat A = U \tilde A U^T = U W \Lambda W^{-1} U^T 
\]</div>
<p>According to equation <a class="reference internal" href="#equation-eq-eqeigahat">(7.27)</a>, the diagonal matrix <span class="math notranslate nohighlight">\(\Lambda\)</span> contains eigenvalues of
<span class="math notranslate nohighlight">\(\hat A\)</span> and corresponding eigenvectors of <span class="math notranslate nohighlight">\(\hat A\)</span> are columns of the matrix <span class="math notranslate nohighlight">\(UW\)</span>.</p>
<p>It follows that the systematic (i.e., not random) parts of the <span class="math notranslate nohighlight">\(X_t\)</span> dynamics captured by our first-order vector autoregressions   are described by</p>
<div class="math notranslate nohighlight">
\[
X_{t+1} = U W \Lambda W^{-1} U^T  X_t 
\]</div>
<p>Multiplying both sides of the above equation by <span class="math notranslate nohighlight">\(W^{-1} U^T\)</span> gives</p>
<div class="math notranslate nohighlight">
\[ 
W^{-1} U^T X_{t+1} = \Lambda W^{-1} U^T X_t 
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\hat b_{t+1} = \Lambda \hat b_t
\]</div>
<p>where our <strong>encoder</strong>  is now</p>
<div class="math notranslate nohighlight">
\[ 
\hat b_t = W^{-1} U^T X_t
\]</div>
<p>and our <strong>decoder</strong> is</p>
<div class="math notranslate nohighlight">
\[
X_t = U W \hat b_t
\]</div>
<p>We can use this representation to construct a predictor <span class="math notranslate nohighlight">\(\overline X_{t+1}\)</span> of <span class="math notranslate nohighlight">\(X_{t+1}\)</span> conditional on <span class="math notranslate nohighlight">\(X_1\)</span>  via:</p>
<div class="math notranslate nohighlight" id="equation-eq-dssebookrepr">
<span class="eqno">(7.28)<a class="headerlink" href="#equation-eq-dssebookrepr" title="Permalink to this equation">¶</a></span>\[
\overline X_{t+1} = U W \Lambda^t W^{-1} U^T X_1 
\]</div>
<p>In effect,
<span id="id4">[<a class="reference internal" href="zreferences.html#id16" title="Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics, 656:5–28, 2010.">Sch10</a>]</span> defined an <span class="math notranslate nohighlight">\(m \times m\)</span> matrix <span class="math notranslate nohighlight">\(\Phi_s\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-phisfull">
<span class="eqno">(7.29)<a class="headerlink" href="#equation-eq-phisfull" title="Permalink to this equation">¶</a></span>\[ 
\Phi_s = UW 
\]</div>
<p>and a generalized inverse</p>
<div class="math notranslate nohighlight" id="equation-eq-phisfullinv">
<span class="eqno">(7.30)<a class="headerlink" href="#equation-eq-phisfullinv" title="Permalink to this equation">¶</a></span>\[
\Phi_s^+ = W^{-1}U^T 
\]</div>
<p><span id="id5">[<a class="reference internal" href="zreferences.html#id16" title="Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics, 656:5–28, 2010.">Sch10</a>]</span> then  represented equation <a class="reference internal" href="#equation-eq-dssebookrepr">(7.28)</a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-schmidrep">
<span class="eqno">(7.31)<a class="headerlink" href="#equation-eq-schmidrep" title="Permalink to this equation">¶</a></span>\[
\overline X_{t+1} = \Phi_s \Lambda^t \Phi_s^+ X_1 
\]</div>
<p>Components of the  basis vector <span class="math notranslate nohighlight">\( \hat b_t = W^{-1} U^T X_t \equiv \Phi_s^+ X_t\)</span> are often  called DMD <strong>modes</strong>, or sometimes also
DMD <strong>projected modes</strong>.</p>
<p>To understand why they are called <strong>projected modes</strong>, notice that</p>
<div class="math notranslate nohighlight">
\[ 
\Phi_s^+ = ( \Phi_s^T \Phi_s)^{-1} \Phi_s^T
\]</div>
<p>so that the <span class="math notranslate nohighlight">\(m \times p\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[
\hat b =  \Phi_s^+ X
\]</div>
<p>is a matrix of regression coefficients of the <span class="math notranslate nohighlight">\(m \times n\)</span> matrix <span class="math notranslate nohighlight">\(X\)</span> on the <span class="math notranslate nohighlight">\(m \times p\)</span> matrix <span class="math notranslate nohighlight">\(\Phi_s\)</span>.</p>
<p>We’ll say more about this interpretation in a related context when we discuss representation 3.</p>
<p>We turn next  to an alternative  representation suggested by  Tu et al. <span id="id6">[<a class="reference internal" href="zreferences.html#id25" title="J. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz. On dynamic mode decomposition: theory and applications. Journal of Computational Dynamics, 1(2):391–421, 2014.">TRL+14</a>]</span>.</p>
<p>It is more appropriate to use this alternative representation  when, as is typically the case  in practice, we use a reduced SVD.</p>
</div>
<div class="section" id="representation-3">
<h2><span class="section-number">7.15. </span>Representation 3<a class="headerlink" href="#representation-3" title="Permalink to this headline">¶</a></h2>
<p>Departing from the procedures used to construct  Representations 1 and 2, each of which deployed a <strong>full</strong> SVD, we now use a <strong>reduced</strong> SVD.</p>
<p>Again, we let  <span class="math notranslate nohighlight">\(p \leq \textrm{min}(m,n)\)</span> be the rank of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Construct a <strong>reduced</strong> SVD</p>
<div class="math notranslate nohighlight">
\[
X = \tilde U \tilde \Sigma \tilde V^T, 
\]</div>
<p>where now <span class="math notranslate nohighlight">\(\tilde U\)</span> is <span class="math notranslate nohighlight">\(m \times p\)</span>, <span class="math notranslate nohighlight">\(\tilde \Sigma\)</span> is <span class="math notranslate nohighlight">\( p \times p\)</span>, and <span class="math notranslate nohighlight">\(\tilde V^T\)</span> is <span class="math notranslate nohighlight">\(p \times n\)</span>.</p>
<p>Our minimum-norm least-squares estimator  approximator of  <span class="math notranslate nohighlight">\(A\)</span> now has representation</p>
<div class="math notranslate nohighlight" id="equation-eq-ahatwithtildes">
<span class="eqno">(7.32)<a class="headerlink" href="#equation-eq-ahatwithtildes" title="Permalink to this equation">¶</a></span>\[
\hat A = X' \tilde V \tilde \Sigma^{-1} \tilde U^T
\]</div>
<p>Paralleling a step in Representation 1, define a  transition matrix for a rotated <span class="math notranslate nohighlight">\(p \times 1\)</span> state <span class="math notranslate nohighlight">\(\tilde b_t\)</span> by</p>
<div class="math notranslate nohighlight" id="equation-eq-atildered">
<span class="eqno">(7.33)<a class="headerlink" href="#equation-eq-atildered" title="Permalink to this equation">¶</a></span>\[ 
\tilde A =\tilde  U^T \hat A \tilde U 
\]</div>
<p><strong>Interpretation as projection coefficients</strong></p>
<p><span id="id7">[<a class="reference internal" href="zreferences.html#id40" title="Steven L. Brunton and J. Nathan Kutz. Data-Driven Science and Engineering, Second Edition. Cambridge University Press, New York, 2022.">BK22</a>]</span> remark that <span class="math notranslate nohighlight">\(\tilde A\)</span>  can be interpreted in terms of a projection of <span class="math notranslate nohighlight">\(\hat A\)</span> onto the <span class="math notranslate nohighlight">\(p\)</span> modes in <span class="math notranslate nohighlight">\(\tilde U\)</span>.</p>
<p>To verify this, first note that, because  <span class="math notranslate nohighlight">\( \tilde U^T \tilde U = I\)</span>, it follows that</p>
<div class="math notranslate nohighlight" id="equation-eq-tildeaverify">
<span class="eqno">(7.34)<a class="headerlink" href="#equation-eq-tildeaverify" title="Permalink to this equation">¶</a></span>\[
\tilde A = \tilde U^T \hat A \tilde U = \tilde U^T X' \tilde V \tilde \Sigma^{-1} \tilde U^T \tilde U 
= \tilde U^T X' \tilde V \tilde \Sigma^{-1} \tilde U^T
\]</div>
<p>Next, we’ll just  compute the regression coefficients in a projection of <span class="math notranslate nohighlight">\(\hat A\)</span> on <span class="math notranslate nohighlight">\(\tilde U\)</span> using the
standard least-square formula</p>
<div class="math notranslate nohighlight">
\[
(\tilde U^T \tilde U)^{-1} \tilde U^T \hat A = (\tilde U^T \tilde U)^{-1} \tilde U^T X' \tilde V \tilde \Sigma^{-1} \tilde U^T = 
\tilde U^T X' \tilde V \tilde \Sigma^{-1} \tilde U^T  = \tilde A .
\]</div>
<p>Note that because we are now working with a reduced SVD,  <span class="math notranslate nohighlight">\(\tilde U \tilde U^T \neq I\)</span>.</p>
<p>Consequently,</p>
<div class="math notranslate nohighlight">
\[
\hat A \neq \tilde U \tilde A \tilde U^T,
\]</div>
<p>and we can’t simply  recover <span class="math notranslate nohighlight">\(\hat A\)</span> from  <span class="math notranslate nohighlight">\(\tilde A\)</span> and <span class="math notranslate nohighlight">\(\tilde U\)</span>.</p>
<p>Nevertheless, we  hope for the best and proceed to construct an eigendecomposition of the
<span class="math notranslate nohighlight">\(p \times p\)</span> matrix <span class="math notranslate nohighlight">\(\tilde A\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-tildeaeigenred">
<span class="eqno">(7.35)<a class="headerlink" href="#equation-eq-tildeaeigenred" title="Permalink to this equation">¶</a></span>\[
 \tilde A =  \tilde  W  \Lambda \tilde  W^{-1} .
\]</div>
<p>Mimicking our procedure in Representation 2, we cross our fingers and compute an <span class="math notranslate nohighlight">\(m \times p\)</span> matrix</p>
<div class="math notranslate nohighlight" id="equation-eq-phisred">
<span class="eqno">(7.36)<a class="headerlink" href="#equation-eq-phisred" title="Permalink to this equation">¶</a></span>\[
\tilde \Phi_s = \tilde U \tilde W
\]</div>
<p>that  corresponds to <a class="reference internal" href="#equation-eq-phisfull">(7.29)</a> for a full SVD.</p>
<p>At this point, where <span class="math notranslate nohighlight">\(\hat A\)</span> is given by formula <a class="reference internal" href="#equation-eq-ahatwithtildes">(7.32)</a> it is interesting to compute <span class="math notranslate nohighlight">\(\hat A \tilde  \Phi_s\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat A \tilde \Phi_s &amp; = (X' \tilde V \tilde \Sigma^{-1} \tilde U^T) (\tilde U \tilde W) \\
  &amp; = X' \tilde V \tilde \Sigma^{-1} \tilde  W \\
  &amp; \neq (\tilde U \tilde  W) \Lambda \\
  &amp; = \tilde \Phi_s \Lambda
  \end{aligned}
\end{split}\]</div>
<p>That
<span class="math notranslate nohighlight">\( \hat A \tilde \Phi_s \neq \tilde \Phi_s \Lambda \)</span> means, that unlike the  corresponding situation in Representation 2, columns of <span class="math notranslate nohighlight">\(\tilde \Phi_s = \tilde U \tilde  W\)</span>
are <strong>not</strong> eigenvectors of <span class="math notranslate nohighlight">\(\hat A\)</span> corresponding to eigenvalues  on the diagonal of matix <span class="math notranslate nohighlight">\(\Lambda\)</span>.</p>
<p>But in a quest for eigenvectors of <span class="math notranslate nohighlight">\(\hat A\)</span> that we <strong>can</strong> compute with a reduced SVD,  let’s define  the <span class="math notranslate nohighlight">\(m \times p\)</span> matrix
<span class="math notranslate nohighlight">\(\Phi\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-phiformula">
<span class="eqno">(7.37)<a class="headerlink" href="#equation-eq-phiformula" title="Permalink to this equation">¶</a></span>\[
\Phi \equiv \hat A \tilde \Phi_s = X' \tilde V \tilde \Sigma^{-1}  \tilde  W
\]</div>
<p>It turns out that columns of <span class="math notranslate nohighlight">\(\Phi\)</span> <strong>are</strong> eigenvectors of <span class="math notranslate nohighlight">\(\hat A\)</span>.</p>
<p>This is
a consequence of a  result established by Tu et al. <span id="id8">[<a class="reference internal" href="zreferences.html#id25" title="J. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz. On dynamic mode decomposition: theory and applications. Journal of Computational Dynamics, 1(2):391–421, 2014.">TRL+14</a>]</span>, which we now present.</p>
<p><strong>Proposition</strong> The <span class="math notranslate nohighlight">\(p\)</span> columns of <span class="math notranslate nohighlight">\(\Phi\)</span> are eigenvectors of <span class="math notranslate nohighlight">\(\hat A\)</span>.</p>
<p><strong>Proof:</strong> From formula <a class="reference internal" href="#equation-eq-phiformula">(7.37)</a> we have</p>
<div class="math notranslate nohighlight">
\[  
\begin{aligned}
  \hat A \Phi &amp; =  (X' \tilde  V \tilde  \Sigma^{-1} \tilde  U^T) (X' \tilde  V \Sigma^{-1} \tilde  W) \cr
  &amp; = X' \tilde V \tilde  \Sigma^{-1} \tilde A \tilde  W \cr
  &amp; = X' \tilde  V \tilde  \Sigma^{-1}\tilde  W \Lambda \cr
  &amp; = \Phi \Lambda 
  \end{aligned}
\]</div>
<p>Thus, we  have deduced  that</p>
<div class="math notranslate nohighlight" id="equation-eq-aphilambda">
<span class="eqno">(7.38)<a class="headerlink" href="#equation-eq-aphilambda" title="Permalink to this equation">¶</a></span>\[  
\hat A \Phi = \Phi \Lambda
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\phi_i\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>th  column of <span class="math notranslate nohighlight">\(\Phi\)</span> and <span class="math notranslate nohighlight">\(\lambda_i\)</span> be the corresponding <span class="math notranslate nohighlight">\(i\)</span> eigenvalue of <span class="math notranslate nohighlight">\(\tilde A\)</span> from decomposition <a class="reference internal" href="#equation-eq-tildeaeigenred">(7.35)</a>.</p>
<p>Writing out the <span class="math notranslate nohighlight">\(m \times 1\)</span> vectors on both sides of  equation <a class="reference internal" href="#equation-eq-aphilambda">(7.38)</a> and equating them gives</p>
<div class="math notranslate nohighlight">
\[
\hat A \phi_i = \lambda_i \phi_i .
\]</div>
<p>This equation confirms that  <span class="math notranslate nohighlight">\(\phi_i\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\hat A\)</span> that corresponds to eigenvalue  <span class="math notranslate nohighlight">\(\lambda_i\)</span> of both  <span class="math notranslate nohighlight">\(\tilde A\)</span> and <span class="math notranslate nohighlight">\(\hat A\)</span>.</p>
<p>This concludes the proof.</p>
<p>Also see <span id="id9">[<a class="reference internal" href="zreferences.html#id40" title="Steven L. Brunton and J. Nathan Kutz. Data-Driven Science and Engineering, Second Edition. Cambridge University Press, New York, 2022.">BK22</a>]</span> (p. 238)</p>
<div class="section" id="decoder-of-x-as-a-linear-projection">
<h3><span class="section-number">7.15.1. </span>Decoder of  <span class="math notranslate nohighlight">\(X\)</span> as a linear projection<a class="headerlink" href="#decoder-of-x-as-a-linear-projection" title="Permalink to this headline">¶</a></h3>
<p>From  eigendecomposition <a class="reference internal" href="#equation-eq-aphilambda">(7.38)</a> we can represent <span class="math notranslate nohighlight">\(\hat A\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-aform12">
<span class="eqno">(7.39)<a class="headerlink" href="#equation-eq-aform12" title="Permalink to this equation">¶</a></span>\[ 
\hat A = \Phi \Lambda \Phi^+ .
\]</div>
<p>From formula <a class="reference internal" href="#equation-eq-aform12">(7.39)</a> we can deduce the reduced dimension dynamics</p>
<div class="math notranslate nohighlight">
\[ 
\check b_{t+1} = \Lambda \check b_t 
\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-decoder102">
<span class="eqno">(7.40)<a class="headerlink" href="#equation-eq-decoder102" title="Permalink to this equation">¶</a></span>\[
\check b_t  = \Phi^+ X_t  
\]</div>
<p>Since the <span class="math notranslate nohighlight">\(m \times p\)</span> matrix <span class="math notranslate nohighlight">\(\Phi\)</span> has <span class="math notranslate nohighlight">\(p\)</span> linearly independent columns, the generalized inverse of <span class="math notranslate nohighlight">\(\Phi\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\Phi^{+} = (\Phi^T \Phi)^{-1} \Phi^T
\]</div>
<p>and so</p>
<div class="math notranslate nohighlight" id="equation-eq-checkbform">
<span class="eqno">(7.41)<a class="headerlink" href="#equation-eq-checkbform" title="Permalink to this equation">¶</a></span>\[ 
\check b = (\Phi^T \Phi)^{-1} \Phi^T X
\]</div>
<p>The <span class="math notranslate nohighlight">\(p \times n\)</span>  matrix <span class="math notranslate nohighlight">\(\check b\)</span>  is recognizable as a  matrix of least squares regression coefficients of the <span class="math notranslate nohighlight">\(m \times n\)</span>  matrix
<span class="math notranslate nohighlight">\(X\)</span> on the <span class="math notranslate nohighlight">\(m \times p\)</span> matrix <span class="math notranslate nohighlight">\(\Phi\)</span> and consequently</p>
<div class="math notranslate nohighlight" id="equation-eq-xcheck">
<span class="eqno">(7.42)<a class="headerlink" href="#equation-eq-xcheck" title="Permalink to this equation">¶</a></span>\[
\check X = \Phi \check b
\]</div>
<p>is an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of least squares projections of <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(\Phi\)</span>.</p>
<p>By virtue of least-squares projection theory discussed in  this quantecon lecture e <a class="reference external" href="https://python-advanced.quantecon.org/orth_proj.html">https://python-advanced.quantecon.org/orth_proj.html</a>, we can represent <span class="math notranslate nohighlight">\(X\)</span> as the sum of the projection <span class="math notranslate nohighlight">\(\check X\)</span> of <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(\Phi\)</span>  plus a matrix of errors.</p>
<p>To verify this, note that the least squares projection <span class="math notranslate nohighlight">\(\check X\)</span> is related to <span class="math notranslate nohighlight">\(X\)</span> by</p>
<div class="math notranslate nohighlight">
\[ 
X = \check X + \epsilon 
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-xbcheck">
<span class="eqno">(7.43)<a class="headerlink" href="#equation-eq-xbcheck" title="Permalink to this equation">¶</a></span>\[
X = \Phi \check b + \epsilon
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of least squares errors satisfying the least squares
orthogonality conditions <span class="math notranslate nohighlight">\(\epsilon^T \Phi =0 \)</span> or</p>
<div class="math notranslate nohighlight" id="equation-eq-orthls">
<span class="eqno">(7.44)<a class="headerlink" href="#equation-eq-orthls" title="Permalink to this equation">¶</a></span>\[ 
(X - \Phi \check b)^T \Phi = 0_{m \times p}
\]</div>
<p>Rearranging  the orthogonality conditions <a class="reference internal" href="#equation-eq-orthls">(7.44)</a> gives <span class="math notranslate nohighlight">\(X^T \Phi = \check b \Phi^T \Phi\)</span>,
which implies formula <a class="reference internal" href="#equation-eq-checkbform">(7.41)</a>.</p>
</div>
<div class="section" id="a-useful-approximation">
<h3><span class="section-number">7.15.2. </span>A useful approximation<a class="headerlink" href="#a-useful-approximation" title="Permalink to this headline">¶</a></h3>
<p>There is a useful  way to approximate  the <span class="math notranslate nohighlight">\(p \times 1\)</span> vector <span class="math notranslate nohighlight">\(\check b_t\)</span> instead of using  formula
<a class="reference internal" href="#equation-eq-decoder102">(7.40)</a>.</p>
<p>In particular, the following argument adapted from <span id="id10">[<a class="reference internal" href="zreferences.html#id40" title="Steven L. Brunton and J. Nathan Kutz. Data-Driven Science and Engineering, Second Edition. Cambridge University Press, New York, 2022.">BK22</a>]</span> (page 240) provides a computationally efficient way
to approximate <span class="math notranslate nohighlight">\(\check b_t\)</span>.</p>
<p>For convenience, we’ll do this first for time <span class="math notranslate nohighlight">\(t=1\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(t=1\)</span>, from equation <a class="reference internal" href="#equation-eq-xbcheck">(7.43)</a> we have</p>
<div class="math notranslate nohighlight" id="equation-eq-x1proj">
<span class="eqno">(7.45)<a class="headerlink" href="#equation-eq-x1proj" title="Permalink to this equation">¶</a></span>\[ 
   \check X_1 = \Phi \check b_1
\]</div>
<p>where <span class="math notranslate nohighlight">\(\check b_1\)</span> is a <span class="math notranslate nohighlight">\(p \times 1\)</span> vector.</p>
<p>Recall from representation 1 above that  <span class="math notranslate nohighlight">\(X_1 =  U \tilde b_1\)</span>, where <span class="math notranslate nohighlight">\(\tilde b_1\)</span> is a time <span class="math notranslate nohighlight">\(1\)</span>  basis vector for representation 1 and <span class="math notranslate nohighlight">\(U\)</span> is from a full SVD of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>It  then follows from equation <a class="reference internal" href="#equation-eq-xbcheck">(7.43)</a> that</p>
<div class="math notranslate nohighlight">
\[ 
  U \tilde b_1 = X' \tilde V \tilde \Sigma^{-1} \tilde  W \check b_1 + \epsilon_1
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_1\)</span> is a least-squares error vector from equation <a class="reference internal" href="#equation-eq-xbcheck">(7.43)</a>.</p>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[
\tilde b_1 = U^T X' V \tilde \Sigma^{-1} \tilde W \check b_1 + U^T \epsilon_1
\]</div>
<p>Replacing the error term <span class="math notranslate nohighlight">\(U^T \epsilon_1\)</span> by zero, and replacing <span class="math notranslate nohighlight">\(U\)</span> from a full SVD of <span class="math notranslate nohighlight">\(X\)</span> with
<span class="math notranslate nohighlight">\(\tilde U\)</span> from a reduced SVD,  we obtain  an approximation <span class="math notranslate nohighlight">\(\hat b_1\)</span> to <span class="math notranslate nohighlight">\(\tilde b_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[ 
  \hat b_1 = \tilde U^T X' \tilde V \tilde \Sigma^{-1} \tilde  W \check b_1
\]</div>
<p>Recall that  from equation <a class="reference internal" href="#equation-eq-tildeaverify">(7.34)</a>,  <span class="math notranslate nohighlight">\( \tilde A = \tilde U^T X' \tilde V \tilde \Sigma^{-1}\)</span>.</p>
<p>It then follows  that</p>
<div class="math notranslate nohighlight">
\[ 
  \hat  b_1 = \tilde   A \tilde W \check b_1
\]</div>
<p>and therefore, by the  eigendecomposition  <a class="reference internal" href="#equation-eq-tildeaeigenred">(7.35)</a> of <span class="math notranslate nohighlight">\(\tilde A\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[ 
  \hat b_1 = \tilde W \Lambda \check b_1
\]</div>
<p>Consequently,</p>
<div class="math notranslate nohighlight">
\[ 
  \hat b_1 = ( \tilde W \Lambda)^{-1} \tilde b_1
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-beqnsmall">
<span class="eqno">(7.46)<a class="headerlink" href="#equation-eq-beqnsmall" title="Permalink to this equation">¶</a></span>\[ 
   \hat b_1 = ( \tilde W \Lambda)^{-1} \tilde U^T X_1 ,
\]</div>
<p>which is a computationally efficient approximation to  the following instance of  equation <a class="reference internal" href="#equation-eq-decoder102">(7.40)</a> for  the initial vector <span class="math notranslate nohighlight">\(\check b_1\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-bphieqn">
<span class="eqno">(7.47)<a class="headerlink" href="#equation-eq-bphieqn" title="Permalink to this equation">¶</a></span>\[
  \check b_1= \Phi^{+} X_1
\]</div>
<p>(To highlight that <a class="reference internal" href="#equation-eq-beqnsmall">(7.46)</a> is an approximation, users of  DMD sometimes call  components of the  basis vector <span class="math notranslate nohighlight">\(\check b_t  = \Phi^+ X_t \)</span>  the  <strong>exact</strong> DMD modes.)</p>
<p>Conditional on <span class="math notranslate nohighlight">\(X_t\)</span>, we can compute our decoded <span class="math notranslate nohighlight">\(\check X_{t+j},   j = 1, 2, \ldots \)</span>  from
either</p>
<div class="math notranslate nohighlight" id="equation-eq-checkxevoln">
<span class="eqno">(7.48)<a class="headerlink" href="#equation-eq-checkxevoln" title="Permalink to this equation">¶</a></span>\[
\check X_{t+j} = \Phi \Lambda^j \Phi^{+} X_t
\]</div>
<p>or  use the approximation</p>
<div class="math notranslate nohighlight" id="equation-eq-checkxevoln2">
<span class="eqno">(7.49)<a class="headerlink" href="#equation-eq-checkxevoln2" title="Permalink to this equation">¶</a></span>\[ 
  \hat X_{t+j} = \Phi \Lambda^j (W \Lambda)^{-1}  \tilde U^T X_t .
\]</div>
<p>We can then use <span class="math notranslate nohighlight">\(\check X_{t+j}\)</span> or <span class="math notranslate nohighlight">\(\hat X_{t+j}\)</span> to forecast <span class="math notranslate nohighlight">\(X_{t+j}\)</span>.</p>
</div>
<div class="section" id="using-fewer-modes">
<h3><span class="section-number">7.15.3. </span>Using Fewer Modes<a class="headerlink" href="#using-fewer-modes" title="Permalink to this headline">¶</a></h3>
<p>In applications, we’ll actually want to just a few modes, often three or less.</p>
<p>Some of the preceding formulas assume that we have retained all <span class="math notranslate nohighlight">\(p\)</span> modes associated with the positive
singular values of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>We can  adjust our  formulas to describe a situation in which we instead retain only
the <span class="math notranslate nohighlight">\(r &lt; p\)</span> largest singular values.</p>
<p>In that case, we simply replace <span class="math notranslate nohighlight">\(\tilde \Sigma\)</span> with the appropriate <span class="math notranslate nohighlight">\(r\times r\)</span> matrix of singular values,
<span class="math notranslate nohighlight">\(\tilde U\)</span> with the <span class="math notranslate nohighlight">\(m \times r\)</span> matrix  whose columns correspond to the <span class="math notranslate nohighlight">\(r\)</span> largest singular values,
and <span class="math notranslate nohighlight">\(\tilde V\)</span> with the <span class="math notranslate nohighlight">\(n \times r\)</span> matrix whose columns correspond to the <span class="math notranslate nohighlight">\(r\)</span> largest  singular values.</p>
<p>Counterparts of all of the salient formulas above then apply.</p>
</div>
</div>
<div class="section" id="source-for-some-python-code">
<h2><span class="section-number">7.16. </span>Source for Some Python Code<a class="headerlink" href="#source-for-some-python-code" title="Permalink to this headline">¶</a></h2>
<p>You can find a Python implementation of DMD here:</p>
<p><a class="reference external" href="https://mathlab.github.io/PyDMD/">https://mathlab.github.io/PyDMD/</a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="geom_series.html">
   1. Geometric Series for Elementary Economics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sir_model.html">
   2. Modeling COVID 19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   3. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="qr_decomp.html">
   4. QR Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="complex_and_trig.html">
   5. Complex Numbers and Trigonometry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eig_circulant.html">
   6. Circulant Matrices
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   7. Singular Value Decomposition (SVD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newton_method.html">
   8. Using Newton’s Method to Solve Economic Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Elementary Statistics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="prob_matrix.html">
   9. Elementary Probability with Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="time_series_with_matrices.html">
   10. Univariate Time Series with Matrix Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   11. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prob_meaning.html">
   12. Two Meanings of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multi_hyper.html">
   13. Multivariate Hypergeometric Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_normal.html">
   14. Multivariate Normal Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heavy_tails.html">
   15. Heavy-Tailed Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hoist_failure.html">
   16. Fault Tree Uncertainties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="back_prop.html">
   17. Introduction to Artificial Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rand_resp.html">
   18. Randomized Response Surveys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="util_rand_resp.html">
   19. Expected Utilities of Random Responses
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Programming
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lp_intro.html">
   20. Linear Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="opt_transport.html">
   21. Optimal Transport
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="von_neumann_model.html">
   22. Von Neumann Growth Model (and a Generalization)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="scalar_dynam.html">
   23. Dynamics in One Dimension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_processes.html">
   24. AR1 Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="finite_markov.html">
   25. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inventory_dynamics.html">
   26. Inventory Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_models.html">
   27. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="samuelson.html">
   28. Samuelson Multiplier-Accelerator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kesten_processes.html">
   29. Kesten Processes and Firm Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wealth_dynamics.html">
   30. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman.html">
   31. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="short_path.html">
   32. Shortest Paths
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Search
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model.html">
   33. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_separation.html">
   34. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_fitted_vfi.html">
   35. Job Search III: Fitted Value Function Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_correlated.html">
   36. Job Search IV: Correlated Wage Offers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="career.html">
   37. Job Search V: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jv.html">
   38. Job Search VI: On-the-Job Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_q.html">
   39. Job Search VII: A McCall Worker Q-Learns
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Consumption, Savings and Capital
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_1.html">
   40. Cass-Koopmans Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_2.html">
   41. Cass-Koopmans Competitive Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_problem.html">
   42. Cake Eating I: Introduction to Optimal Saving
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_numerical.html">
   43. Cake Eating II: Numerical Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth.html">
   44. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth_fast.html">
   45. Optimal Growth II: Accelerating the Code with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coleman_policy_iter.html">
   46. Optimal Growth III: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="egm_policy_iter.html">
   47. Optimal Growth IV: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp.html">
   48. The Income Fluctuation Problem I: Basic Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp_advanced.html">
   49. The Income Fluctuation Problem II: Stochastic Returns on Assets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bayes Law
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="bayes_nonconj.html">
   50. Non-Conjugate Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_bayes.html">
   51. Posterior Distributions for  AR(1) Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_turningpts.html">
   52. Forecasting  an AR(1) process
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Information
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="odu.html">
   53. Job Search VII: Search with Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process.html">
   54. Likelihood Ratio Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="imp_sample.html">
   55. Computing Mean of a Likelihood Ratio Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman.html">
   56. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exchangeable.html">
   57. Exchangeability and Bayesian Updating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_bayes.html">
   58. Likelihood Ratio Processes and Bayesian Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mix_model.html">
   59. Incorrect Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="navy_captain.html">
   60. Bayesian versus Frequentist Decision Rules
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lqcontrol.html">
   61. LQ Control: Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lagrangian_lqdp.html">
   62. Lagrangian for LQ Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cross_product_trick.html">
   63. Eliminating Cross Products
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income.html">
   64. The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income_cons.html">
   65. Permanent Income II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lq_inventories.html">
   66. Production Smoothing via Inventories
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="schelling.html">
   67. Schelling’s Segregation Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lake_model.html">
   68. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rational_expectations.html">
   69. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="re_with_feedback.html">
   70. Stability in Linear Rational Expectations Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_perf.html">
   71. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncertainty_traps.html">
   72. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aiyagari.html">
   73. The Aiyagari Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markov_asset.html">
   74. Asset Pricing: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ge_arrow.html">
   75. Competitive Equilibria with Arrow Securities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrison_kreps.html">
   76. Heterogeneous Beliefs and Bubbles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data and Empirics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_panel.html">
   77. Pandas for Panel Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ols.html">
   78. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mle.html">
   79. Maximum Likelihood Estimation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Auctions
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="two_auctions.html">
   80. First-Price and Second-Price Auctions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="house_auction.html">
   81. Multiple Good Allocation Mechanisms
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   82. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   83. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   84. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                    <!-- <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="qe-toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/svd_intro.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst/tree/master/lectures/svd_intro.md" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="/_pdf/quantecon-python.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://mybinder.org/v2/gh/QuantEcon/lecture-python.notebooks/master?urlpath=tree/svd_intro.ipynb">BinderHub</option>
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-python.notebooks/blob/master/svd_intro.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python.notebooks" data-urlpath="tree/lecture-python.notebooks/svd_intro.ipynb" data-branch=master>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://mybinder.org/v2/gh/QuantEcon/lecture-python.notebooks/master?urlpath=tree/svd_intro.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "svd_intro";
                const repoURL = "https://github.com/QuantEcon/lecture-python.notebooks";
                const urlPath = "tree/lecture-python.notebooks/svd_intro.ipynb";
                const branch = "master"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-54984338-10', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>