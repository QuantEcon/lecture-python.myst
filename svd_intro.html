
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>6. Singular Value Decomposition (SVD) &#8212; Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/quantecon-book-theme.1ef59f8f4e91ec8319176e8479c6af4e.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">


    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="_static/quantecon-book-theme.15b0c36fffe88f468997fa7b698991d3.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://python.quantecon.org/svd_intro.html" />
    <link rel="shortcut icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Complex Numbers and Trigonometry" href="complex_and_trig.html" />
    <link rel="prev" title="5. QR Decomposition" href="qr_decomp.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Singular Value Decomposition (SVD)"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Singular Value Decomposition (SVD)" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python.quantecon.org/svd_intro.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=svd_intro>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   6.1. Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-setup">
   6.2. The Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#singular-value-decomposition">
   6.3. Singular Value Decomposition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reduced-versus-full-svd">
   6.4. Reduced Versus Full SVD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#digression-polar-decomposition">
   6.5. Digression:  Polar Decomposition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principle-components-analysis-pca">
   6.6. Principle Components Analysis (PCA)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relationship-of-pca-to-svd">
   6.7. Relationship of PCA to SVD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-with-eigenvalues-and-eigenvectors">
   6.8. PCA with Eigenvalues and Eigenvectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#connections">
   6.9. Connections
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-mode-decomposition-dmd">
   6.10. Dynamic Mode Decomposition (DMD)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representation-1">
   6.11. Representation 1
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representation-2">
   6.12. Representation 2
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representation-3">
   6.13. Representation 3
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-fewer-modes">
   6.14. Using Fewer Modes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#source-for-some-python-code">
   6.15. Source for Some Python Code
  </a>
 </li>
</ul>

                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo" alt="logo"></a>
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Quantitative Economics with Python</a></p>

                        <p class="qe-page__header-subheading">Singular Value Decomposition (SVD)</p>

                    </div>

                    <p class="qe-page__header-authors">Thomas J. Sargent & John Stachurski</p>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <section class="tex2jax_ignore mathjax_ignore" id="singular-value-decomposition-svd">
<h1><span class="section-number">6. </span>Singular Value Decomposition (SVD)<a class="headerlink" href="#singular-value-decomposition-svd" title="Permalink to this headline">¶</a></h1>
<p>In addition to regular packages contained in Anaconda by default, this lecture also requires:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install quandl
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting quandl
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  Downloading Quandl-3.7.0-py2.py3-none-any.whl (26 kB)
Requirement already satisfied: six in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (1.16.0)
Requirement already satisfied: requests&gt;=2.7.0 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (2.26.0)
Requirement already satisfied: more-itertools in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (8.10.0)
Requirement already satisfied: numpy&gt;=1.8 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (1.20.3)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pandas&gt;=0.14 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (1.3.4)
Requirement already satisfied: python-dateutil in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (2.8.2)
Requirement already satisfied: inflection&gt;=0.3.1 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from quandl) (0.5.1)
Requirement already satisfied: pytz&gt;=2017.3 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from pandas&gt;=0.14-&gt;quandl) (2021.3)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;quandl) (2.0.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;quandl) (3.2)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;quandl) (2021.10.8)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/share/miniconda3/envs/quantecon/lib/python3.8/site-packages (from requests&gt;=2.7.0-&gt;quandl) (1.26.7)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Installing collected packages: quandl
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Successfully installed quandl-3.7.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">LA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">quandl</span> <span class="k">as</span> <span class="nn">ql</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<section id="overview">
<h2><span class="section-number">6.1. </span>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The <strong>singular value decomposition</strong> is a work-horse in applications of least squares projection that
form a foundation for  some important machine learning methods.</p>
<p>This lecture describes the singular value decomposition and two of its uses:</p>
<ul class="simple">
<li><p>principal components analysis (PCA)</p></li>
<li><p>dynamic mode decomposition (DMD)</p></li>
</ul>
<p>Each of these can be thought of as a data-reduction procedure  designed to capture salient patterns by projecting data onto a limited set of factors.</p>
</section>
<section id="the-setup">
<h2><span class="section-number">6.2. </span>The Setup<a class="headerlink" href="#the-setup" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of rank <span class="math notranslate nohighlight">\(r\)</span>.</p>
<p>Necessarily, <span class="math notranslate nohighlight">\(r \leq \min(m,n)\)</span>.</p>
<p>In this lecture, we’ll think of <span class="math notranslate nohighlight">\(X\)</span> as a matrix of <strong>data</strong>.</p>
<ul class="simple">
<li><p>each column is an <strong>individual</strong> – a time period or person, depending on the application</p></li>
<li><p>each row is a <strong>random variable</strong> measuring an attribute of a time period or a person, depending on the application</p></li>
</ul>
<p>We’ll be interested in  two  cases</p>
<ul class="simple">
<li><p>A <strong>short and fat</strong> case in which <span class="math notranslate nohighlight">\(m &lt;&lt; n\)</span>, so that there are many more columns than rows.</p></li>
<li><p>A  <strong>tall and skinny</strong> case in which <span class="math notranslate nohighlight">\(m &gt;&gt; n\)</span>, so that there are many more rows than columns.</p></li>
</ul>
<p>We’ll apply a <strong>singular value decomposition</strong> of <span class="math notranslate nohighlight">\(X\)</span> in both situations.</p>
<p>In the first case in which there are many more observations <span class="math notranslate nohighlight">\(n\)</span> than random variables <span class="math notranslate nohighlight">\(m\)</span>, we learn about a joint distribution  by taking averages  across observations of functions of the observations.</p>
<p>Here we’ll look for <strong>patterns</strong> by using a <strong>singular value decomposition</strong> to do a <strong>principal components analysis</strong> (PCA).</p>
<p>In the second case in which there are many more random variables <span class="math notranslate nohighlight">\(m\)</span> than observations <span class="math notranslate nohighlight">\(n\)</span>, we’ll proceed in a different way.</p>
<p>We’ll again use a <strong>singular value decomposition</strong>,  but now to do a <strong>dynamic mode decomposition</strong> (DMD)</p>
</section>
<section id="singular-value-decomposition">
<h2><span class="section-number">6.3. </span>Singular Value Decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">¶</a></h2>
<p>A <strong>singular value decomposition</strong> of an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix <span class="math notranslate nohighlight">\(X\)</span> of rank <span class="math notranslate nohighlight">\(r \leq \min(m,n)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
X  = U \Sigma V^T
\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
UU^T &amp;  = I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V = I
\end{align*}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U\)</span> is an <span class="math notranslate nohighlight">\(m \times m\)</span> matrix whose columns are eigenvectors of <span class="math notranslate nohighlight">\(X^T X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix whose columns are eigenvectors of <span class="math notranslate nohighlight">\(X X^T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix in which the first <span class="math notranslate nohighlight">\(r\)</span> places on its main diagonal are positive numbers <span class="math notranslate nohighlight">\(\sigma_1, \sigma_2, \ldots, \sigma_r\)</span> called <strong>singular values</strong>; remaining entries of <span class="math notranslate nohighlight">\(\Sigma\)</span> are all zero</p></li>
<li><p>The <span class="math notranslate nohighlight">\(r\)</span> singular values are square roots of the eigenvalues of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix  <span class="math notranslate nohighlight">\(X X^T\)</span> and the <span class="math notranslate nohighlight">\(n \times n\)</span> matrix <span class="math notranslate nohighlight">\(X^T X\)</span></p></li>
<li><p>When <span class="math notranslate nohighlight">\(U\)</span> is a complex valued matrix, <span class="math notranslate nohighlight">\(U^T\)</span> denotes the <strong>conjugate-transpose</strong> or <strong>Hermitian-transpose</strong> of <span class="math notranslate nohighlight">\(U\)</span>, meaning that
<span class="math notranslate nohighlight">\(U_{ij}^T\)</span> is the complex conjugate of <span class="math notranslate nohighlight">\(U_{ji}\)</span>.</p></li>
<li><p>Similarly, when <span class="math notranslate nohighlight">\(V\)</span> is a complex valued matrix, <span class="math notranslate nohighlight">\(V^T\)</span> denotes the <strong>conjugate-transpose</strong> or <strong>Hermitian-transpose</strong> of <span class="math notranslate nohighlight">\(V\)</span></p></li>
</ul>
<p>In what is called a <strong>full</strong> SVD, the  shapes of <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> are <span class="math notranslate nohighlight">\(\left(m, m\right)\)</span>, <span class="math notranslate nohighlight">\(\left(m, n\right)\)</span>, <span class="math notranslate nohighlight">\(\left(n, n\right)\)</span>, respectively.</p>
<p>There is also an alternative shape convention called an <strong>economy</strong> or <strong>reduced</strong> SVD .</p>
<p>Thus, note that because we assume that <span class="math notranslate nohighlight">\(A\)</span> has rank <span class="math notranslate nohighlight">\(r\)</span>, there are only <span class="math notranslate nohighlight">\(r \)</span> nonzero singular values, where <span class="math notranslate nohighlight">\(r=\textrm{rank}(A)\leq\min\left(m, n\right)\)</span>.</p>
<p>A <strong>reduced</strong> SVD uses this fact to express <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> as matrices with shapes <span class="math notranslate nohighlight">\(\left(m, r\right)\)</span>, <span class="math notranslate nohighlight">\(\left(r, r\right)\)</span>, <span class="math notranslate nohighlight">\(\left(r, n\right)\)</span>.</p>
<p>Sometimes, we will use a full SVD</p>
<p>At other times, we’ll use a reduced SVD  in which <span class="math notranslate nohighlight">\(\Sigma\)</span> is an <span class="math notranslate nohighlight">\(r \times r\)</span>  diagonal matrix.</p>
<p><strong>Caveat:</strong>
The properties</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
UU^T &amp;  = I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V = I
\end{align*}\]</div>
<p>apply to a <strong>full</strong> SVD but not to a <strong>reduced</strong> SVD.</p>
<p>In the <strong>tall-skinny</strong> case in which <span class="math notranslate nohighlight">\(m &gt; &gt; n\)</span>, for a <strong>reduced</strong> SVD</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
UU^T &amp;  \neq I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V = I
\end{align*}\]</div>
<p>while in the <strong>short-fat</strong> case in which <span class="math notranslate nohighlight">\(m &lt; &lt; n\)</span>, for a <strong>reduced</strong> SVD</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
UU^T &amp;  = I  &amp;  \quad U^T U = I \cr    
VV^T &amp; = I &amp; \quad V^T V \neq I
\end{align*}\]</div>
<p>When we study Dynamic Mode Decomposition below, we shall want to remember this caveat because we’ll be using reduced SVD’s to compute key objects.</p>
</section>
<section id="reduced-versus-full-svd">
<h2><span class="section-number">6.4. </span>Reduced Versus Full SVD<a class="headerlink" href="#reduced-versus-full-svd" title="Permalink to this headline">¶</a></h2>
<p>Earlier, we mentioned <strong>full</strong> and <strong>reduced</strong> SVD’s.</p>
<p>You can read about reduced and full SVD here
<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html</a></p>
<p>In a <strong>full</strong> SVD</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(m \times m\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is <span class="math notranslate nohighlight">\(m \times n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is <span class="math notranslate nohighlight">\(n \times n\)</span></p></li>
</ul>
<p>In a <strong>reduced</strong> SVD</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U\)</span> is <span class="math notranslate nohighlight">\(m \times r\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is <span class="math notranslate nohighlight">\(r \times r\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is <span class="math notranslate nohighlight">\(n \times r\)</span></p></li>
</ul>
<p>Let’s do a some  small exercise  to compare <strong>full</strong> and <strong>reduced</strong> SVD’s.</p>
<p>First, let’s study a case in which <span class="math notranslate nohighlight">\(m = 5 &gt; n = 2\)</span>.</p>
<p>(This is a small example of the <strong>tall-skinny</strong> that will concern us when we study <strong>Dynamic Mode Decompositions</strong> below.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># full SVD</span>
<span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># economy SVD</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;U, S, V =&#39;</span><span class="p">),</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>U, S, V =
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None,
 array([[-4.48344496e-01, -4.26389967e-01, -4.99724292e-01,
         -3.60863683e-02, -6.05105126e-01],
        [-4.61175854e-01, -5.76282313e-01,  1.01760355e-01,
         -3.70132051e-02,  6.65950734e-01],
        [-5.19964545e-01,  1.47624808e-01,  7.65044195e-01,
         -1.16229744e-02, -3.49880085e-01],
        [-4.32034128e-02, -3.03687319e-02, -8.08665900e-03,
          9.98571741e-01,  5.37439310e-04],
        [-5.60423395e-01,  6.80716496e-01, -3.93144308e-01,
         -6.86882718e-03,  2.60655026e-01]]),
 array([1.82283507, 0.35307303]),
 array([[-0.46199003, -0.88688512],
        [ 0.88688512, -0.46199003]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Uhat, Shat, Vhat = &#39;</span><span class="p">),</span> <span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Uhat, Shat, Vhat = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None,
 array([[-0.4483445 , -0.42638997],
        [-0.46117585, -0.57628231],
        [-0.51996454,  0.14762481],
        [-0.04320341, -0.03036873],
        [-0.56042339,  0.6807165 ]]),
 array([1.82283507, 0.35307303]),
 array([[-0.46199003, -0.88688512],
        [ 0.88688512, -0.46199003]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;rank of X - &#39;</span><span class="p">),</span> <span class="n">rr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>rank of X - 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None, 2)
</pre></div>
</div>
</div>
</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p>Where <span class="math notranslate nohighlight">\(U\)</span> is constructed via a full SVD, <span class="math notranslate nohighlight">\(U^T U = I_{r \times r}\)</span> and  <span class="math notranslate nohighlight">\(U U^T = I_{m \times m}\)</span></p></li>
<li><p>Where <span class="math notranslate nohighlight">\(\hat U\)</span> is constructed via a reduced SVD, although <span class="math notranslate nohighlight">\(\hat U^T \hat U = I_{r \times r}\)</span> it happens that  <span class="math notranslate nohighlight">\(\hat U \hat U^T \neq I_{m \times m}\)</span></p></li>
</ul>
<p>We illustrate these properties for our example with the following code cells.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UTU</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="nd">@U</span>
<span class="n">UUT</span> <span class="o">=</span> <span class="n">U</span><span class="nd">@U</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;UUT, UTU = &#39;</span><span class="p">),</span> <span class="n">UUT</span><span class="p">,</span> <span class="n">UTU</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>UUT, UTU = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None,
 array([[ 1.00000000e+00, -1.35496100e-16, -1.69779013e-16,
         -1.45609764e-17, -1.00640438e-16],
        [-1.35496100e-16,  1.00000000e+00, -1.33742253e-17,
         -7.58915724e-18,  4.50414955e-17],
        [-1.69779013e-16, -1.33742253e-17,  1.00000000e+00,
         -4.02335007e-18, -7.03584346e-17],
        [-1.45609764e-17, -7.58915724e-18, -4.02335007e-18,
          1.00000000e+00,  1.20056006e-18],
        [-1.00640438e-16,  4.50414955e-17, -7.03584346e-17,
          1.20056006e-18,  1.00000000e+00]]),
 array([[ 1.00000000e+00, -1.70602795e-16, -1.05090054e-16,
         -8.35931698e-18, -1.76517584e-16],
        [-1.70602795e-16,  1.00000000e+00,  2.85104474e-18,
         -2.24427970e-18, -2.78003606e-17],
        [-1.05090054e-16,  2.85104474e-18,  1.00000000e+00,
         -3.91651883e-18, -7.63403816e-17],
        [-8.35931698e-18, -2.24427970e-18, -3.91651883e-18,
          1.00000000e+00, -1.69220592e-19],
        [-1.76517584e-16, -2.78003606e-17, -7.63403816e-17,
         -1.69220592e-19,  1.00000000e+00]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UhatUhatT</span> <span class="o">=</span> <span class="n">Uhat</span><span class="nd">@Uhat</span><span class="o">.</span><span class="n">T</span>
<span class="n">UhatTUhat</span> <span class="o">=</span> <span class="n">Uhat</span><span class="o">.</span><span class="n">T</span><span class="nd">@Uhat</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;UhatUhatT, UhatTUhat= &#39;</span><span class="p">),</span> <span class="n">UhatUhatT</span><span class="p">,</span> <span class="n">UhatTUhat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>UhatUhatT, UhatTUhat= 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None,
 array([[ 0.38282119,  0.45248665,  0.17017751,  0.03231893, -0.03898794],
        [ 0.45248665,  0.54478447,  0.15472153,  0.03742533, -0.13383114],
        [ 0.17017751,  0.15472153,  0.29215621,  0.01798106,  0.39189094],
        [ 0.03231893,  0.03742533,  0.01798106,  0.00278879,  0.00353971],
        [-0.03898794, -0.13383114,  0.39189094,  0.00353971,  0.77744933]]),
 array([[ 1.00000000e+00, -2.26113947e-16],
        [-2.26113947e-16,  1.00000000e+00]]))
</pre></div>
</div>
</div>
</div>
<p><strong>Remark:</strong> The cells above illustrate application of the  <code class="docutils literal notranslate"><span class="pre">fullmatrices=True</span></code> and <code class="docutils literal notranslate"><span class="pre">full-matrices=False</span></code> options.
Using <code class="docutils literal notranslate"><span class="pre">full-matrices=False</span></code> returns a reduced singular value decomposition. This option implements
an optimal reduced rank approximation of a matrix, in the sense of  minimizing the Frobenius
norm of the discrepancy between the approximating matrix and the matrix being approximated.
Optimality in this sense is  established in the celebrated Eckart–Young theorem. See <a class="reference external" href="https://en.wikipedia.org/wiki/Low-rank_approximation">https://en.wikipedia.org/wiki/Low-rank_approximation</a>.</p>
<p>When we study Dynamic Mode Decompositions below, it  will be important for us to remember the following important properties of full and reduced SVD’s in such tall-skinny cases.</p>
<p>Let’s do another exercise, but now we’ll set <span class="math notranslate nohighlight">\(m = 2 &lt; 5 = n \)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># full SVD</span>
<span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># economy SVD</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;U, S, V =&#39;</span><span class="p">),</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>U, S, V =
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None,
 array([[ 0.77189317, -0.63575226],
        [ 0.63575226,  0.77189317]]),
 array([1.33731443, 0.41573769]),
 array([[ 0.04048152,  0.4916775 ,  0.42815119,  0.31996177,  0.68624012],
        [ 0.02038739, -0.52894473,  0.80816069,  0.16129603, -0.20164752],
        [-0.6139661 ,  0.52225891,  0.31907946, -0.30314855, -0.39570256],
        [-0.37308616,  0.00867497, -0.22019629,  0.86573984, -0.25047871],
        [-0.69411667, -0.45347667, -0.1151727 , -0.17379164,  0.51874143]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Uhat, Shat, Vhat = &#39;</span><span class="p">),</span> <span class="n">Uhat</span><span class="p">,</span> <span class="n">Shat</span><span class="p">,</span> <span class="n">Vhat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Uhat, Shat, Vhat = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None,
 array([[ 0.77189317, -0.63575226],
        [ 0.63575226,  0.77189317]]),
 array([1.33731443, 0.41573769]),
 array([[ 0.04048152,  0.4916775 ,  0.42815119,  0.31996177,  0.68624012],
        [ 0.02038739, -0.52894473,  0.80816069,  0.16129603, -0.20164752]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;rank X = &#39;</span><span class="p">),</span> <span class="n">rr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>rank X = 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(None, 2)
</pre></div>
</div>
</div>
</div>
</section>
<section id="digression-polar-decomposition">
<h2><span class="section-number">6.5. </span>Digression:  Polar Decomposition<a class="headerlink" href="#digression-polar-decomposition" title="Permalink to this headline">¶</a></h2>
<p>A singular value decomposition (SVD) is related to the <strong>polar decomposition</strong> of <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[
X  = SQ   
\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
 S &amp; = U\Sigma U^T \cr
Q &amp; = U V^T 
\end{align*}\]</div>
<p>and <span class="math notranslate nohighlight">\(S\)</span> is evidently a symmetric matrix and <span class="math notranslate nohighlight">\(Q\)</span> is an orthogonal matrix.</p>
</section>
<section id="principle-components-analysis-pca">
<h2><span class="section-number">6.6. </span>Principle Components Analysis (PCA)<a class="headerlink" href="#principle-components-analysis-pca" title="Permalink to this headline">¶</a></h2>
<p>Let’s begin with a case in which <span class="math notranslate nohighlight">\(n &gt;&gt; m\)</span>, so that we have many  more observations <span class="math notranslate nohighlight">\(n\)</span> than random variables <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>The  matrix <span class="math notranslate nohighlight">\(X\)</span> is <strong>short and fat</strong>  in an  <span class="math notranslate nohighlight">\(n &gt;&gt; m\)</span> case as opposed to a <strong>tall and skinny</strong> case with <span class="math notranslate nohighlight">\(m &gt; &gt; n \)</span> to be discussed later.</p>
<p>We regard  <span class="math notranslate nohighlight">\(X\)</span> as an  <span class="math notranslate nohighlight">\(m \times n\)</span> matrix of <strong>data</strong>:</p>
<div class="math notranslate nohighlight">
\[
X =  \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_n\end{bmatrix}
\]</div>
<p>where for <span class="math notranslate nohighlight">\(j = 1, \ldots, n\)</span> the column vector <span class="math notranslate nohighlight">\(X_j = \begin{bmatrix}X_{1j}\\X_{2j}\\\vdots\\X_{mj}\end{bmatrix}\)</span> is a  vector of observations on variables <span class="math notranslate nohighlight">\(\begin{bmatrix}x_1\\x_2\\\vdots\\x_m\end{bmatrix}\)</span>.</p>
<p>In a <strong>time series</strong> setting, we would think of columns <span class="math notranslate nohighlight">\(j\)</span> as indexing different <strong>times</strong> at which random variables are observed, while rows index different random variables.</p>
<p>In a <strong>cross section</strong> setting, we would think of columns <span class="math notranslate nohighlight">\(j\)</span> as indexing different <strong>individuals</strong> for  which random variables are observed, while rows index different <strong>random variables</strong>.</p>
<p>The number of positive singular values equals the rank of  matrix <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Arrange the singular values  in decreasing order.</p>
<p>Arrange   the positive singular values on the main diagonal of the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> of into a vector <span class="math notranslate nohighlight">\(\sigma_R\)</span>.</p>
<p>Set all other entries of <span class="math notranslate nohighlight">\(\Sigma\)</span> to zero.</p>
</section>
<section id="relationship-of-pca-to-svd">
<h2><span class="section-number">6.7. </span>Relationship of PCA to SVD<a class="headerlink" href="#relationship-of-pca-to-svd" title="Permalink to this headline">¶</a></h2>
<p>To relate a SVD to a PCA (principal component analysis) of data set <span class="math notranslate nohighlight">\(X\)</span>, first construct  the  SVD of the data matrix <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-pca1">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-eq-pca1" title="Permalink to this equation">¶</a></span>\[
X = U \Sigma V^T = \sigma_1 U_1 V_1^T + \sigma_2 U_2 V_2^T + \cdots + \sigma_r U_r V_r^T
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
U=\begin{bmatrix}U_1|U_2|\ldots|U_m\end{bmatrix}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
V^T = \begin{bmatrix}V_1^T\\V_2^T\\\ldots\\V_n^T\end{bmatrix}
\end{split}\]</div>
<p>In equation <a class="reference internal" href="#equation-eq-pca1">(6.1)</a>, each of the <span class="math notranslate nohighlight">\(m \times n\)</span> matrices <span class="math notranslate nohighlight">\(U_{j}V_{j}^T\)</span> is evidently
of rank <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>Thus, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-pca2">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-eq-pca2" title="Permalink to this equation">¶</a></span>\[\begin{split}
X = \sigma_1 \begin{pmatrix}U_{11}V_{1}^T\\U_{21}V_{1}^T\\\cdots\\U_{m1}V_{1}^T\\\end{pmatrix} + \sigma_2\begin{pmatrix}U_{12}V_{2}^T\\U_{22}V_{2}^T\\\cdots\\U_{m2}V_{2}^T\\\end{pmatrix}+\ldots + \sigma_r\begin{pmatrix}U_{1r}V_{r}^T\\U_{2r}V_{r}^T\\\cdots\\U_{mr}V_{r}^T\\\end{pmatrix}
\end{split}\]</div>
<p>Here is how we would interpret the objects in the  matrix equation <a class="reference internal" href="#equation-eq-pca2">(6.2)</a> in
a time series context:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( V_{k}^T= \begin{bmatrix}V_{k1} &amp;  V_{k2} &amp; \ldots &amp; V_{kn}\end{bmatrix}  \quad \textrm{for each} \   k=1, \ldots, n \)</span> is a time series  <span class="math notranslate nohighlight">\(\lbrace V_{kj} \rbrace_{j=1}^n\)</span> for the <span class="math notranslate nohighlight">\(k\)</span>th <strong>principal component</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(U_j = \begin{bmatrix}U_{1k}\\U_{2k}\\\ldots\\U_{mk}\end{bmatrix} \  k=1, \ldots, m\)</span>
is a vector of <strong>loadings</strong> of variables <span class="math notranslate nohighlight">\(X_i\)</span> on the <span class="math notranslate nohighlight">\(k\)</span>th principle component,  <span class="math notranslate nohighlight">\(i=1, \ldots, m\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_k \)</span> for each <span class="math notranslate nohighlight">\(k=1, \ldots, r\)</span> is the strength of <span class="math notranslate nohighlight">\(k\)</span>th <strong>principal component</strong></p></li>
</ul>
</section>
<section id="pca-with-eigenvalues-and-eigenvectors">
<h2><span class="section-number">6.8. </span>PCA with Eigenvalues and Eigenvectors<a class="headerlink" href="#pca-with-eigenvalues-and-eigenvectors" title="Permalink to this headline">¶</a></h2>
<p>We now  use an eigen decomposition of a sample covariance matrix to do PCA.</p>
<p>Let <span class="math notranslate nohighlight">\(X_{m \times n}\)</span> be our <span class="math notranslate nohighlight">\(m \times n\)</span> data matrix.</p>
<p>Let’s assume that sample means of all variables are zero.</p>
<p>We can assure  this  by <strong>pre-processing</strong> the data by subtracting sample means.</p>
<p>Define the sample covariance matrix <span class="math notranslate nohighlight">\(\Omega\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\Omega = XX^T
\]</div>
<p>Then use an eigen decomposition to represent <span class="math notranslate nohighlight">\(\Omega\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
\Omega =P\Lambda P^T
\]</div>
<p>Here</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\)</span> is <span class="math notranslate nohighlight">\(m×m\)</span> matrix of eigenvectors of <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix of eigenvalues of <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
</ul>
<p>We can then represent <span class="math notranslate nohighlight">\(X\)</span> as</p>
<div class="math notranslate nohighlight">
\[
X=P\epsilon
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\epsilon\epsilon^T=\Lambda .
\]</div>
<p>We can verify that</p>
<div class="math notranslate nohighlight">
\[
XX^T=P\Lambda P^T .
\]</div>
<p>It follows that we can represent the data matrix as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X=\begin{bmatrix}X_1|X_2|\ldots|X_m\end{bmatrix} =\begin{bmatrix}P_1|P_2|\ldots|P_m\end{bmatrix}
\begin{bmatrix}\epsilon_1\\\epsilon_2\\\ldots\\\epsilon_m\end{bmatrix} 
= P_1\epsilon_1+P_2\epsilon_2+\ldots+P_m\epsilon_m
\end{equation*}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\epsilon\epsilon^T=\Lambda .
\]</div>
<p>To reconcile the preceding representation with the PCA that we obtained through the SVD above, we first note that <span class="math notranslate nohighlight">\(\epsilon_j^2=\lambda_j\equiv\sigma^2_j\)</span>.</p>
<p>Now define  <span class="math notranslate nohighlight">\(\tilde{\epsilon_j} = \frac{\epsilon_j}{\sqrt{\lambda_j}}\)</span>,
which evidently implies that <span class="math notranslate nohighlight">\(\tilde{\epsilon}_j\tilde{\epsilon}_j^T=1\)</span>.</p>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X&amp;=\sqrt{\lambda_1}P_1\tilde{\epsilon_1}+\sqrt{\lambda_2}P_2\tilde{\epsilon_2}+\ldots+\sqrt{\lambda_m}P_m\tilde{\epsilon_m}\\
&amp;=\sigma_1P_1\tilde{\epsilon_2}+\sigma_2P_2\tilde{\epsilon_2}+\ldots+\sigma_mP_m\tilde{\epsilon_m} ,
\end{aligned}
\end{split}\]</div>
<p>which evidently agrees with</p>
<div class="math notranslate nohighlight">
\[
X=\sigma_1U_1{V_1}^{T}+\sigma_2 U_2{V_2}^{T}+\ldots+\sigma_{r} U_{r}{V_{r}}^{T}
\]</div>
<p>provided that  we set</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U_j=P_j\)</span> (the loadings of variables on principal components)</p></li>
<li><p><span class="math notranslate nohighlight">\({V_k}^{T}=\tilde{\epsilon_k}\)</span> (the principal components)</p></li>
</ul>
<p>Since there are several possible ways of computing  <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(U\)</span> for  given a data matrix <span class="math notranslate nohighlight">\(X\)</span>, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.</p>
<p>We can resolve such ambiguities about  <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(P\)</span> by</p>
<ol class="simple">
<li><p>sorting eigenvalues and singular values in descending order</p></li>
<li><p>imposing positive diagonals on <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(U\)</span> and adjusting signs in <span class="math notranslate nohighlight">\(V^T\)</span> accordingly</p></li>
</ol>
</section>
<section id="connections">
<h2><span class="section-number">6.9. </span>Connections<a class="headerlink" href="#connections" title="Permalink to this headline">¶</a></h2>
<p>To pull things together, it is useful to assemble and compare some formulas presented above.</p>
<p>First, consider the following SVD of an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[
X = U\Sigma V^T
\]</div>
<p>Compute:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
XX^T&amp;=U\Sigma V^TV\Sigma^T U^T\cr
&amp;\equiv U\Sigma\Sigma^TU^T\cr
&amp;\equiv U\Lambda U^T
\end{align*}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(U\)</span> in the SVD is the matrix <span class="math notranslate nohighlight">\(P\)</span>  of
eigenvectors of <span class="math notranslate nohighlight">\(XX^T\)</span> and <span class="math notranslate nohighlight">\(\Sigma \Sigma^T\)</span> is the matrix <span class="math notranslate nohighlight">\(\Lambda\)</span> of eigenvalues.</p>
<p>Second, let’s compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
X^TX &amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T{\Sigma}V^T
\end{align*}\]</div>
<p>Thus, the matrix <span class="math notranslate nohighlight">\(V\)</span> in the SVD is the matrix of eigenvectors of <span class="math notranslate nohighlight">\(X^TX\)</span></p>
<p>Summarizing and fitting things together, we have the eigen decomposition of the sample
covariance matrix</p>
<div class="math notranslate nohighlight">
\[
X X^T = P \Lambda P^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is an orthogonal matrix.</p>
<p>Further, from the SVD of <span class="math notranslate nohighlight">\(X\)</span>, we know that</p>
<div class="math notranslate nohighlight">
\[
X X^T = U \Sigma \Sigma^T U^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(U\)</span> is an orthonal matrix.</p>
<p>Thus, <span class="math notranslate nohighlight">\(P = U\)</span> and we have the representation of <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="math notranslate nohighlight">
\[
X = P \epsilon = U \Sigma V^T
\]</div>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[
U^T X = \Sigma V^T = \epsilon
\]</div>
<p>Note that the preceding implies that</p>
<div class="math notranslate nohighlight">
\[
\epsilon \epsilon^T = \Sigma V^T V \Sigma^T = \Sigma \Sigma^T = \Lambda ,
\]</div>
<p>so that everything fits together.</p>
<p>Below we define a class <code class="docutils literal notranslate"><span class="pre">DecomAnalysis</span></code> that wraps  PCA and SVD for a given a data matrix <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecomAnalysis</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class for conducting PCA and SVD.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">n_component</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Ω</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_component</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_component</span> <span class="o">=</span> <span class="n">n_component</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_component</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span>

    <span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">𝜆</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ω</span><span class="p">)</span>    <span class="c1"># columns of P are eigenvectors</span>

        <span class="n">ind</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">𝜆</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">𝜆</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># sort by eigenvalues</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span> <span class="o">=</span> <span class="n">𝜆</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">P</span><span class="p">[:,</span> <span class="n">ind</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">diag_sign</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Λ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">explained_ratio_pca</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜆</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># compute the N by T matrix of principal components </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">𝜖</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span>

        <span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">𝜖</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜖</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_pca</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">𝜖</span>

    <span class="k">def</span> <span class="nf">svd</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">U</span><span class="p">,</span> <span class="n">𝜎</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="n">ind</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">𝜎</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">𝜎</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># sort by eigenvalues</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">𝜎</span> <span class="o">=</span> <span class="n">𝜎</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="n">ind</span><span class="p">]</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">diag_sign</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">D</span>
        <span class="n">VT</span><span class="p">[:</span><span class="n">d</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">D</span> <span class="o">@</span> <span class="n">VT</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="p">:]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">VT</span> <span class="o">=</span> <span class="n">VT</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span><span class="p">[:</span><span class="n">d</span><span class="p">,</span> <span class="p">:</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">𝜎</span><span class="p">)</span>

        <span class="n">𝜎_sq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜎</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explained_ratio_svd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">𝜎_sq</span><span class="p">)</span> <span class="o">/</span> <span class="n">𝜎_sq</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># slicing matrices by the number of components to use</span>
        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">Σ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">VT</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_svd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">Σ</span> <span class="o">@</span> <span class="n">VT</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_component</span><span class="p">):</span>

        <span class="c1"># pca</span>
        <span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">𝜖</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">𝜖</span><span class="p">[:</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_pca</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">𝜖</span>

        <span class="c1"># svd</span>
        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">Σ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Σ</span><span class="p">[:</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:</span><span class="n">n_component</span><span class="p">]</span>
        <span class="n">VT</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">VT</span><span class="p">[:</span><span class="n">n_component</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># transform data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_svd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">Σ</span> <span class="o">@</span> <span class="n">VT</span>

<span class="k">def</span> <span class="nf">diag_sign</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="s2">&quot;Compute the signs of the diagonal of matrix A&quot;</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">D</span>
</pre></div>
</div>
</div>
</div>
<p>We also define a function that prints out information so that we can compare  decompositions
obtained by different algorithms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare_pca_svd</span><span class="p">(</span><span class="n">da</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the outcomes of PCA and SVD.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">da</span><span class="o">.</span><span class="n">pca</span><span class="p">()</span>
    <span class="n">da</span><span class="o">.</span><span class="n">svd</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eigenvalues and Singular values</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;λ = </span><span class="si">{</span><span class="n">da</span><span class="o">.</span><span class="n">λ</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;σ^2 = </span><span class="si">{</span><span class="n">da</span><span class="o">.</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># loading matrices</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;loadings&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">P</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;P&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># principal components</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;principal components&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">ε</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ε&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">VT</span><span class="p">[:</span><span class="n">da</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">da</span><span class="o">.</span><span class="n">λ</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;$V^T*\sqrt{\lambda}$&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>For an example  PCA applied to analyzing the structure of intelligence tests see this lecture <a class="reference internal" href="multivariate_normal.html"><span class="doc">Multivariable Normal Distribution</span></a>.</p>
<p>Look at the parts of that lecture that describe and illustrate the classic factor analysis model.</p>
</section>
<section id="dynamic-mode-decomposition-dmd">
<h2><span class="section-number">6.10. </span>Dynamic Mode Decomposition (DMD)<a class="headerlink" href="#dynamic-mode-decomposition-dmd" title="Permalink to this headline">¶</a></h2>
<p>We turn to the case in which <span class="math notranslate nohighlight">\( m &gt;&gt;n \)</span>.</p>
<p>Here an <span class="math notranslate nohighlight">\( m \times n \)</span>  data matrix <span class="math notranslate nohighlight">\( \tilde X \)</span> contains many more random variables <span class="math notranslate nohighlight">\( m \)</span> than observations <span class="math notranslate nohighlight">\( n \)</span>.</p>
<p>This  <strong>tall and skinny</strong> case is associated with <strong>Dynamic Mode Decomposition</strong>.</p>
<p>Dynamic mode decomposition was introduced by <span id="id1">[<a class="reference internal" href="zreferences.html#id3">Sch10</a>]</span>,</p>
<p>You can read more about Dynamic Mode Decomposition here [<a class="reference external" href="https://python.quantecon.org/zreferences.html#id24">KBBWP16</a>] and here [<a class="reference external" href="https://python.quantecon.org/zreferences.html#id25">BK19</a>] (section 7.2).</p>
<p>We want to fit a <strong>first-order vector autoregression</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-varfirstorder">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-eq-varfirstorder" title="Permalink to this equation">¶</a></span>\[
X_{t+1} = A X_t + C \epsilon_{t+1}
\]</div>
<p>where
the <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector <span class="math notranslate nohighlight">\( X_t \)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-xvector">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-eq-xvector" title="Permalink to this equation">¶</a></span>\[
X_t = \begin{bmatrix}  X_{1,t} &amp; X_{2,t} &amp; \cdots &amp; X_{m,t}     \end{bmatrix}^T
\]</div>
<p>and where <span class="math notranslate nohighlight">\( T \)</span> again denotes complex transposition and <span class="math notranslate nohighlight">\( X_{i,t} \)</span> is an observation on variable <span class="math notranslate nohighlight">\( i \)</span> at time <span class="math notranslate nohighlight">\( t \)</span>.</p>
<p>We want to fit equation <a class="reference internal" href="#equation-eq-varfirstorder">(6.3)</a>.</p>
<p>Our data is assembled in the form of  an <span class="math notranslate nohighlight">\( m \times n \)</span> matrix  <span class="math notranslate nohighlight">\( \tilde X \)</span></p>
<div class="math notranslate nohighlight">
\[
\tilde X =  \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_n\end{bmatrix}
\]</div>
<p>where for <span class="math notranslate nohighlight">\( t = 1, \ldots, n \)</span>,  the <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector <span class="math notranslate nohighlight">\( X_t \)</span> is given by <a class="reference internal" href="#equation-eq-xvector">(6.4)</a>.</p>
<p>We want to estimate system  <a class="reference internal" href="#equation-eq-varfirstorder">(6.3)</a> consisting of <span class="math notranslate nohighlight">\( m \)</span> least squares regressions of <strong>everything</strong> on one lagged value of <strong>everything</strong>.</p>
<p>The <span class="math notranslate nohighlight">\(i\)</span>’th equation of <a class="reference internal" href="#equation-eq-varfirstorder">(6.3)</a> is a regression of <span class="math notranslate nohighlight">\(X_{i,t+1}\)</span> on the vector <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p>We proceed as follows.</p>
<p>From <span class="math notranslate nohighlight">\( \tilde X \)</span>,  we  form two matrices</p>
<div class="math notranslate nohighlight">
\[
X =  \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_{n-1}\end{bmatrix}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
X' =  \begin{bmatrix} X_2 \mid X_3 \mid \cdots \mid X_n\end{bmatrix}
\]</div>
<p>Here <span class="math notranslate nohighlight">\( ' \)</span> does not indicate matrix transposition but instead is part of the name of the matrix <span class="math notranslate nohighlight">\( X' \)</span>.</p>
<p>In forming <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( X' \)</span>, we have in each case  dropped a column from <span class="math notranslate nohighlight">\( \tilde X \)</span>,  the last column in the case of <span class="math notranslate nohighlight">\( X \)</span>, and  the first column in the case of <span class="math notranslate nohighlight">\( X' \)</span>.</p>
<p>Evidently, <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( X' \)</span> are both <span class="math notranslate nohighlight">\( m \times \tilde n \)</span> matrices where <span class="math notranslate nohighlight">\( \tilde n = n - 1 \)</span>.</p>
<p>We denote the rank of <span class="math notranslate nohighlight">\( X \)</span> as <span class="math notranslate nohighlight">\( p \leq \min(m, \tilde n)  \)</span>.</p>
<p>Two possible cases are</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \tilde n &gt; &gt; m\)</span>, so that we have many more time series  observations <span class="math notranslate nohighlight">\(\tilde n\)</span> than variables <span class="math notranslate nohighlight">\(m\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(m &gt; &gt; \tilde n\)</span>, so that we have many more variables <span class="math notranslate nohighlight">\(m \)</span> than time series observations <span class="math notranslate nohighlight">\(\tilde n\)</span></p></li>
</ul>
<p>At a general level that includes both of these special cases, a common formula describes the least squares estimator <span class="math notranslate nohighlight">\(\hat A\)</span> of <span class="math notranslate nohighlight">\(A\)</span> for both cases, but important  details differ.</p>
<p>The common formula is</p>
<div class="math notranslate nohighlight" id="equation-eq-commona">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-eq-commona" title="Permalink to this equation">¶</a></span>\[ 
\hat A = X' X^+ 
\]</div>
<p>where <span class="math notranslate nohighlight">\(X^+\)</span> is the pseudo-inverse of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Formulas for the pseudo-inverse differ for our two cases.</p>
<p>When <span class="math notranslate nohighlight">\( \tilde n &gt; &gt; m\)</span>, so that we have many more time series  observations <span class="math notranslate nohighlight">\(\tilde n\)</span> than variables <span class="math notranslate nohighlight">\(m\)</span> and when
<span class="math notranslate nohighlight">\(X\)</span> has linearly independent <strong>rows</strong>, <span class="math notranslate nohighlight">\(X X^T\)</span> has an inverse and the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span> is</p>
<div class="math notranslate nohighlight">
\[
X^+ = X^T (X X^T)^{-1} 
\]</div>
<p>Here <span class="math notranslate nohighlight">\(X^+\)</span> is a <strong>right-inverse</strong> that verifies <span class="math notranslate nohighlight">\( X X^+ = I_{m \times m}\)</span>.</p>
<p>In this case, our formula <a class="reference internal" href="#equation-eq-commona">(6.5)</a> for the least-squares estimator of the population matrix of regression coefficients  <span class="math notranslate nohighlight">\(A\)</span> becomes</p>
<div class="math notranslate nohighlight">
\[ 
\hat A = X' X^T (X X^T)^{-1}
\]</div>
<p>This formula is widely used in economics to estimate vector autorgressions.</p>
<p>The right side is proportional to the empirical cross second moment matrix of <span class="math notranslate nohighlight">\(X_{t+1}\)</span> and <span class="math notranslate nohighlight">\(X_t\)</span> times the inverse
of the second moment matrix of <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p>This least-squares formula widely used in econometrics.</p>
<p><strong>Tall-Skinny Case:</strong></p>
<p>When <span class="math notranslate nohighlight">\(m &gt; &gt; \tilde n\)</span>, so that we have many more variables <span class="math notranslate nohighlight">\(m \)</span> than time series observations <span class="math notranslate nohighlight">\(\tilde n\)</span> and when <span class="math notranslate nohighlight">\(X\)</span> has linearly independent <strong>columns</strong>,
<span class="math notranslate nohighlight">\(X^T X\)</span> has an inverse and the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span> is</p>
<div class="math notranslate nohighlight">
\[
X^+ = (X^T X)^{-1} X^T
\]</div>
<p>Here  <span class="math notranslate nohighlight">\(X^+\)</span> is a <strong>left-inverse</strong> that verifies <span class="math notranslate nohighlight">\(X^+ X = I_{\tilde n \times \tilde n}\)</span>.</p>
<p>In this case, our formula  <a class="reference internal" href="#equation-eq-commona">(6.5)</a> for a least-squares estimator of <span class="math notranslate nohighlight">\(A\)</span> becomes</p>
<div class="math notranslate nohighlight" id="equation-eq-hataversion0">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-eq-hataversion0" title="Permalink to this equation">¶</a></span>\[
\hat A = X' (X^T X)^{-1} X^T
\]</div>
<p>This is the case that we are interested in here.</p>
<p>If we use formula <a class="reference internal" href="#equation-eq-hataversion0">(6.6)</a> to calculate <span class="math notranslate nohighlight">\(\hat A X\)</span> we find that</p>
<div class="math notranslate nohighlight">
\[
\hat A X = X'
\]</div>
<p>so that the regression equation <strong>fits perfectly</strong>, the usual outcome in an <strong>underdetermined least-squares</strong> model.</p>
<p>Thus, we want to fit equation <a class="reference internal" href="#equation-eq-varfirstorder">(6.3)</a> in a situation in which we have a number <span class="math notranslate nohighlight">\(n\)</span> of observations  that is small relative to the number <span class="math notranslate nohighlight">\(m\)</span> of
variables that appear in the vector <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p>To reiterate and provide more  detail about how we can efficiently calculate the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span>, as our  estimator <span class="math notranslate nohighlight">\(\hat A\)</span> of <span class="math notranslate nohighlight">\(A\)</span> we form an  <span class="math notranslate nohighlight">\(m \times m\)</span> matrix that  solves the least-squares best-fit problem</p>
<div class="math notranslate nohighlight" id="equation-eq-alseqn">
<span class="eqno">(6.7)<a class="headerlink" href="#equation-eq-alseqn" title="Permalink to this equation">¶</a></span>\[ 
\hat A = \textrm{argmin}_{\check A} || X' - \check  A X ||_F   
\]</div>
<p>where <span class="math notranslate nohighlight">\(|| \cdot ||_F\)</span> denotes the Frobeneus norm of a matrix.</p>
<p>The solution of the problem on the right side of equation <a class="reference internal" href="#equation-eq-alseqn">(6.7)</a> is</p>
<div class="math notranslate nohighlight" id="equation-eq-hataform">
<span class="eqno">(6.8)<a class="headerlink" href="#equation-eq-hataform" title="Permalink to this equation">¶</a></span>\[
\hat A =  X'  X^{+}  
\]</div>
<p>where the (possibly huge) <span class="math notranslate nohighlight">\( \tilde n \times m \)</span> matrix <span class="math notranslate nohighlight">\( X^{+} = (X^T X)^{-1} X^T\)</span> is again a pseudo-inverse of <span class="math notranslate nohighlight">\( X \)</span>.</p>
<p>For some situations that we are interested in, <span class="math notranslate nohighlight">\(X^T X \)</span> can be close to singular, a situation that can make some numerical algorithms  be error-prone.</p>
<p>To confront that possibility, we’ll use  efficient algorithms for computing and for constructing reduced rank approximations of  <span class="math notranslate nohighlight">\(\hat A\)</span> in formula <a class="reference internal" href="#equation-eq-hataversion0">(6.6)</a>.</p>
<p>The <span class="math notranslate nohighlight">\( i \)</span>th  row of <span class="math notranslate nohighlight">\( \hat A \)</span> is an <span class="math notranslate nohighlight">\( m \times 1 \)</span> vector of regression coefficients of <span class="math notranslate nohighlight">\( X_{i,t+1} \)</span> on <span class="math notranslate nohighlight">\( X_{j,t}, j = 1, \ldots, m \)</span>.</p>
<p>An efficient way to compute the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span> is to start with  the (reduced) singular value decomposition</p>
<div class="math notranslate nohighlight" id="equation-eq-svddmd">
<span class="eqno">(6.9)<a class="headerlink" href="#equation-eq-svddmd" title="Permalink to this equation">¶</a></span>\[
X =  U \Sigma  V^T 
\]</div>
<p>where <span class="math notranslate nohighlight">\( U \)</span> is <span class="math notranslate nohighlight">\( m \times p \)</span>, <span class="math notranslate nohighlight">\( \Sigma \)</span> is a <span class="math notranslate nohighlight">\( p \times p \)</span> diagonal  matrix, and <span class="math notranslate nohighlight">\( V^T \)</span> is a <span class="math notranslate nohighlight">\( p \times \tilde n \)</span> matrix.</p>
<p>Here <span class="math notranslate nohighlight">\( p \)</span> is the rank of <span class="math notranslate nohighlight">\( X \)</span>, where necessarily <span class="math notranslate nohighlight">\( p \leq \tilde n \)</span> because we are in a situation in which <span class="math notranslate nohighlight">\(m &gt; &gt; \tilde n\)</span>.</p>
<p>Since we are in the <span class="math notranslate nohighlight">\(m &gt; &gt; \tilde n\)</span> case, we can use the singular value decomposition <a class="reference internal" href="#equation-eq-svddmd">(6.9)</a> efficiently to construct the pseudo-inverse <span class="math notranslate nohighlight">\(X^+\)</span>
by recognizing the following string of equalities.</p>
<div class="math notranslate nohighlight" id="equation-eq-efficientpseudoinverse">
<span class="eqno">(6.10)<a class="headerlink" href="#equation-eq-efficientpseudoinverse" title="Permalink to this equation">¶</a></span>\[\begin{split}
\begin{aligned}
X^{+} &amp; = (X^T X)^{-1} X^T \\
  &amp; = (V \Sigma U^T U \Sigma V^T)^{-1} V \Sigma U^T \\
  &amp; = (V \Sigma \Sigma V^T)^{-1} V \Sigma U^T \\
  &amp; = V \Sigma^{-1} \Sigma^{-1} V^T V \Sigma U^T \\
  &amp; = V \Sigma^{-1} U^T 
\end{aligned}
\end{split}\]</div>
<p>Thus, we shall  construct a pseudo-inverse <span class="math notranslate nohighlight">\( X^+ \)</span>  of <span class="math notranslate nohighlight">\( X \)</span> by using
a singular value decomposition of <span class="math notranslate nohighlight">\(X\)</span> in equation <a class="reference internal" href="#equation-eq-svddmd">(6.9)</a>  to compute</p>
<div class="math notranslate nohighlight" id="equation-eq-xplusformula">
<span class="eqno">(6.11)<a class="headerlink" href="#equation-eq-xplusformula" title="Permalink to this equation">¶</a></span>\[
X^{+} =  V \Sigma^{-1}  U^T 
\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\( \Sigma^{-1} \)</span> is constructed by replacing each non-zero element of <span class="math notranslate nohighlight">\( \Sigma \)</span> with <span class="math notranslate nohighlight">\( \sigma_j^{-1} \)</span>.</p>
<p>We can  use formula <a class="reference internal" href="#equation-eq-xplusformula">(6.11)</a>   together with formula <a class="reference internal" href="#equation-eq-hataform">(6.8)</a> to compute the matrix  <span class="math notranslate nohighlight">\( \hat A \)</span> of regression coefficients.</p>
<p>Thus, our  estimator <span class="math notranslate nohighlight">\(\hat A = X' X^+\)</span> of the <span class="math notranslate nohighlight">\(m \times m\)</span> matrix of coefficients <span class="math notranslate nohighlight">\(A\)</span>    is</p>
<div class="math notranslate nohighlight">
\[
\hat A = X' V \Sigma^{-1}  U^T 
\]</div>
<p>In addition to doing that, we’ll eventually use <strong>dynamic mode decomposition</strong> to compute a rank <span class="math notranslate nohighlight">\( r \)</span> approximation to <span class="math notranslate nohighlight">\( A \)</span>,
where <span class="math notranslate nohighlight">\( r &lt;  p \)</span>.</p>
<p><strong>Remark:</strong> We  described and illustrated a <strong>reduced</strong> singular value decomposition above, and compared it with a <strong>full</strong> singular value decomposition.
In our Python code, we’ll typically use  a reduced SVD.</p>
<p>Next, we describe some alternative <strong>reduced order</strong> representations of our first-order linear dynamic system.</p>
</section>
<section id="representation-1">
<h2><span class="section-number">6.11. </span>Representation 1<a class="headerlink" href="#representation-1" title="Permalink to this headline">¶</a></h2>
<p>In constructing this representation and also whenever we use it, we use a <strong>full</strong> SVD of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>We use the <span class="math notranslate nohighlight">\(p\)</span>  columns of <span class="math notranslate nohighlight">\(U\)</span>, and thus the <span class="math notranslate nohighlight">\(p\)</span> rows of <span class="math notranslate nohighlight">\(U^T\)</span>,  to define   a <span class="math notranslate nohighlight">\(p \times 1\)</span>  vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span> as follows</p>
<div class="math notranslate nohighlight" id="equation-eq-tildexdef2">
<span class="eqno">(6.12)<a class="headerlink" href="#equation-eq-tildexdef2" title="Permalink to this equation">¶</a></span>\[
\tilde b_t = U^T X_t 
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-eq-xdecoder">
<span class="eqno">(6.13)<a class="headerlink" href="#equation-eq-xdecoder" title="Permalink to this equation">¶</a></span>\[ 
X_t = U \tilde b_t
\]</div>
<p>(Here we use the notation <span class="math notranslate nohighlight">\(b\)</span> to remind ourselves that we are creating a <strong>b</strong>asis vector.)</p>
<p>Since we are using a <strong>full</strong> SVD, <span class="math notranslate nohighlight">\(U U^T\)</span> is an <span class="math notranslate nohighlight">\(m \times m\)</span> identity matrix.</p>
<p>So it follows from equation <a class="reference internal" href="#equation-eq-tildexdef2">(6.12)</a> that we can reconstruct  <span class="math notranslate nohighlight">\(X_t\)</span> from <span class="math notranslate nohighlight">\(\tilde b_t\)</span> by using</p>
<ul class="simple">
<li><p>Equation <a class="reference internal" href="#equation-eq-tildexdef2">(6.12)</a> serves as an <strong>encoder</strong> that  summarizes the <span class="math notranslate nohighlight">\(m \times 1\)</span> vector <span class="math notranslate nohighlight">\(X_t\)</span> by a <span class="math notranslate nohighlight">\(p \times 1\)</span> vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span></p></li>
<li><p>Equation <a class="reference internal" href="#equation-eq-xdecoder">(6.13)</a> serves as a <strong>decoder</strong> that recovers the <span class="math notranslate nohighlight">\(m \times 1\)</span> vector <span class="math notranslate nohighlight">\(X_t\)</span> from the <span class="math notranslate nohighlight">\(p \times 1\)</span> vector <span class="math notranslate nohighlight">\(\tilde b_t\)</span></p></li>
</ul>
<p>Define the  transition matrix for a reduced <span class="math notranslate nohighlight">\(p \times 1\)</span> state <span class="math notranslate nohighlight">\(\tilde b_t\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-atilde0">
<span class="eqno">(6.14)<a class="headerlink" href="#equation-eq-atilde0" title="Permalink to this equation">¶</a></span>\[ 
\tilde A = U^T \hat A U 
\]</div>
<p>We can evidently recover <span class="math notranslate nohighlight">\(\hat A\)</span> from</p>
<div class="math notranslate nohighlight">
\[
\hat A = U \tilde A U^T 
\]</div>
<p>Dynamics of the reduced <span class="math notranslate nohighlight">\(p \times 1\)</span> state <span class="math notranslate nohighlight">\(\tilde b_t\)</span> are governed by</p>
<div class="math notranslate nohighlight">
\[
\tilde b_{t+1} = \tilde A \tilde b_t 
\]</div>
<p>To construct forecasts <span class="math notranslate nohighlight">\(\overline X_t\)</span> of  future values of <span class="math notranslate nohighlight">\(X_t\)</span> conditional on <span class="math notranslate nohighlight">\(X_1\)</span>, we can apply  decoders to both sides of this
equation and deduce</p>
<div class="math notranslate nohighlight">
\[
\overline X_{t+1} = U \tilde A^t U^T X_1
\]</div>
<p>where we use <span class="math notranslate nohighlight">\(\overline X_t\)</span> to denote a forecast.</p>
</section>
<section id="representation-2">
<h2><span class="section-number">6.12. </span>Representation 2<a class="headerlink" href="#representation-2" title="Permalink to this headline">¶</a></h2>
<p>This representation is the one originally proposed by  <span id="id2">[<a class="reference internal" href="zreferences.html#id3">Sch10</a>]</span>.</p>
<p>It can be regarded as an intermediate step to  a related and perhaps more useful  representation 3.</p>
<p>As with Representation 1, we continue to</p>
<ul class="simple">
<li><p>use all <span class="math notranslate nohighlight">\(p\)</span> singular values of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>use a <strong>full</strong> SVD and <strong>not</strong> a reduced SVD</p></li>
</ul>
<p>As we observed and illustrated  earlier in this lecture, under these two requirements,
<span class="math notranslate nohighlight">\(U U^T\)</span> and <span class="math notranslate nohighlight">\(U^T U\)</span> are both identity matrices; but under a reduced SVD of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(U^T U\)</span> is not an identity matrix.</p>
<p>As we shall see, these requirements will be too confining for what we ultimately want to do; these are situations in which  <span class="math notranslate nohighlight">\(U^T U\)</span> is <strong>not</strong> an identity matrix because we want to use a reduced SVD of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>But for now, let’s proceed under the assumption that both of the  preceding two  requirements are satisfied.</p>
<p>Form an eigendecomposition of the <span class="math notranslate nohighlight">\(p \times p\)</span> matrix <span class="math notranslate nohighlight">\(\tilde A\)</span> defined in equation <a class="reference internal" href="#equation-eq-atilde0">(6.14)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-tildeaeigen">
<span class="eqno">(6.15)<a class="headerlink" href="#equation-eq-tildeaeigen" title="Permalink to this equation">¶</a></span>\[
\tilde A = W \Lambda W^{-1} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix of eigenvalues and <span class="math notranslate nohighlight">\(W\)</span> is a <span class="math notranslate nohighlight">\(p \times p\)</span>
matrix whose columns are eigenvectors  corresponding to rows (eigenvalues) in
<span class="math notranslate nohighlight">\(\Lambda\)</span>.</p>
<p>Note that when <span class="math notranslate nohighlight">\(U U^T = I_{m \times m}\)</span>, as is true with a full SVD of X (but <strong>not</strong> true with a reduced SVD)</p>
<div class="math notranslate nohighlight">
\[ 
\hat A = U \tilde A U^T = U W \Lambda W^{-1} U^T 
\]</div>
<p>Thus, the systematic (i.e., not random) parts of the <span class="math notranslate nohighlight">\(X_t\)</span> dynamics captured by our first-order vector autoregressions   are described by</p>
<div class="math notranslate nohighlight">
\[
X_{t+1} = U W \Lambda W^{-1} U^T  X_t 
\]</div>
<p>Multiplying both sides of the above equation by <span class="math notranslate nohighlight">\(W^{-1} U^T\)</span> gives</p>
<div class="math notranslate nohighlight">
\[ 
W^{-1} U^T X_{t+1} = \Lambda W^{-1} U^T X_t 
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
\hat b_{t+1} = \Lambda \hat b_t
\]</div>
<p>where now our endoder is</p>
<div class="math notranslate nohighlight">
\[ 
\hat b_t = W^{-1} U^T X_t
\]</div>
<p>and our decoder is</p>
<div class="math notranslate nohighlight">
\[
X_t = U W \hat b_t
\]</div>
<p>We can use this representation to constructor a predictor <span class="math notranslate nohighlight">\(\overline X_{t+1}\)</span> of <span class="math notranslate nohighlight">\(X_{t+1}\)</span> conditional on <span class="math notranslate nohighlight">\(X_1\)</span>  via:</p>
<div class="math notranslate nohighlight" id="equation-eq-dssebookrepr">
<span class="eqno">(6.16)<a class="headerlink" href="#equation-eq-dssebookrepr" title="Permalink to this equation">¶</a></span>\[
\overline X_{t+1} = U W \Lambda^t W^{-1} U^T X_1 
\]</div>
<p>In effect,
<span id="id3">[<a class="reference internal" href="zreferences.html#id3">Sch10</a>]</span> defined an <span class="math notranslate nohighlight">\(m \times p\)</span> matrix <span class="math notranslate nohighlight">\(\Phi_s\)</span> as</p>
<div class="math notranslate nohighlight">
\[ 
\Phi_s = UW 
\]</div>
<p>and represented equation <a class="reference internal" href="#equation-eq-dssebookrepr">(6.16)</a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-schmidrep">
<span class="eqno">(6.17)<a class="headerlink" href="#equation-eq-schmidrep" title="Permalink to this equation">¶</a></span>\[
\overline X_{t+1} = \Phi_s \Lambda^t \Phi_s^+ X_1 
\]</div>
<p>Components of the  basis vector <span class="math notranslate nohighlight">\( \hat b_t = W^{-1} U^T X_t \equiv \Phi_s^+\)</span> are often  called DMD <strong>modes</strong>, or sometimes also
DMD <strong>projected nodes</strong>.</p>
<p>An alternative definition of DMD notes is motivate by the following observation.</p>
<p>A peculiar feature of representation <a class="reference internal" href="#equation-eq-schmidrep">(6.17)</a> is that while the diagonal components of <span class="math notranslate nohighlight">\(\Lambda\)</span> are square roots of singular
values of <span class="math notranslate nohighlight">\(\check A\)</span>, the columns of <span class="math notranslate nohighlight">\(\Phi_s\)</span> are <strong>not</strong> eigenvectors  corresponding to eigenvalues of <span class="math notranslate nohighlight">\(\check A\)</span>.</p>
<p>This feature led Tu et al. <span id="id4">[<a class="reference internal" href="zreferences.html#id12">TRL+14</a>]</span> to suggest an alternative representation that replaces <span class="math notranslate nohighlight">\(\Phi_s\)</span> with another
<span class="math notranslate nohighlight">\(m \times p\)</span> matrix whose columns are eigenvectors of <span class="math notranslate nohighlight">\(\check A\)</span>.</p>
<p>We turn to that representation next.</p>
</section>
<section id="representation-3">
<h2><span class="section-number">6.13. </span>Representation 3<a class="headerlink" href="#representation-3" title="Permalink to this headline">¶</a></h2>
<p>As we did with representation 2, it is useful to  construct an eigencomposition of the <span class="math notranslate nohighlight">\(p \times p\)</span> transition matrix  <span class="math notranslate nohighlight">\(\tilde A\)</span>
according the equation <a class="reference internal" href="#equation-eq-tildeaeigen">(6.15)</a>.</p>
<p>Now where <span class="math notranslate nohighlight">\( 1 \leq r \leq p\)</span>,  construct an <span class="math notranslate nohighlight">\(m \times r\)</span> matrix</p>
<div class="math notranslate nohighlight" id="equation-eq-phiformula">
<span class="eqno">(6.18)<a class="headerlink" href="#equation-eq-phiformula" title="Permalink to this equation">¶</a></span>\[
  \Phi = X'  V  \Sigma^{-1} W
\]</div>
<p>Tu et al. <span id="id5">[<a class="reference internal" href="zreferences.html#id12">TRL+14</a>]</span> established the following</p>
<p><strong>Proposition</strong> The <span class="math notranslate nohighlight">\(r\)</span> columns of <span class="math notranslate nohighlight">\(\Phi\)</span> are eigenvectors of <span class="math notranslate nohighlight">\(\check A\)</span> that correspond to the largest <span class="math notranslate nohighlight">\(r\)</span> eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>Proof:</strong> From formula <a class="reference internal" href="#equation-eq-phiformula">(6.18)</a> we have</p>
<div class="math notranslate nohighlight">
\[  
\begin{aligned}
  \check A \Phi &amp; =  (X' V \Sigma^{-1} U^T) (X' V \Sigma^{-1} W) \cr
  &amp; = X' V \Sigma^{-1} \tilde A W \cr
  &amp; = X' V \Sigma^{-1} W \Lambda \cr
  &amp; = \Phi \Lambda 
  \end{aligned}
\]</div>
<p>Thus, we  have deduced  that</p>
<div class="math notranslate nohighlight" id="equation-eq-aphilambda">
<span class="eqno">(6.19)<a class="headerlink" href="#equation-eq-aphilambda" title="Permalink to this equation">¶</a></span>\[  
\check A \Phi = \Phi \Lambda
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\phi_i\)</span> be the the <span class="math notranslate nohighlight">\(i\)</span>the column of <span class="math notranslate nohighlight">\(\Phi\)</span> and <span class="math notranslate nohighlight">\(\lambda_i\)</span> be the corresponding <span class="math notranslate nohighlight">\(i\)</span> eigenvalue of <span class="math notranslate nohighlight">\(\tilde A\)</span> from decomposition <a class="reference internal" href="#equation-eq-tildeaeigen">(6.15)</a>.</p>
<p>Writing out the <span class="math notranslate nohighlight">\(m \times 1\)</span> vectors on both sides of  equation <a class="reference internal" href="#equation-eq-aphilambda">(6.19)</a> and equating them gives</p>
<div class="math notranslate nohighlight">
\[
\check A \phi_i = \lambda_i \phi_i .
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\phi_i\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(A\)</span> that corresponds to eigenvalue  <span class="math notranslate nohighlight">\(\lambda_i\)</span> of <span class="math notranslate nohighlight">\(\check A\)</span>.</p>
<p>This concludes the proof.</p>
<p>We also have the following</p>
<p><strong>Corollary:</strong>  Assume that the integer <span class="math notranslate nohighlight">\(r\)</span> satisfies <span class="math notranslate nohighlight">\(1 \leq r &lt; p\)</span>.  Instead of defining <span class="math notranslate nohighlight">\(\tilde A\)</span> according to  equation
<a class="reference internal" href="#equation-eq-atilde0">(6.14)</a>, define it as the following <span class="math notranslate nohighlight">\(r \times r\)</span> counterpart</p>
<div class="math notranslate nohighlight" id="equation-eq-atilde10">
<span class="eqno">(6.20)<a class="headerlink" href="#equation-eq-atilde10" title="Permalink to this equation">¶</a></span>\[ 
\tilde A = \tilde U^T \hat A U 
\]</div>
<p>where  in equation <a class="reference internal" href="#equation-eq-atilde10">(6.20)</a> <span class="math notranslate nohighlight">\(\tilde U\)</span> is now  the <span class="math notranslate nohighlight">\(m \times r\)</span> matrix consisting of the eigevectors of <span class="math notranslate nohighlight">\(X X^T\)</span> corresponding to the <span class="math notranslate nohighlight">\(r\)</span>
largest singular values of <span class="math notranslate nohighlight">\(X\)</span>.
The conclusions of the proposition remain true when we replace <span class="math notranslate nohighlight">\(U\)</span> by <span class="math notranslate nohighlight">\(\tilde U\)</span>.</p>
<p>Also see <span id="id6">[<a class="reference internal" href="zreferences.html#id26">BK19</a>]</span> (p. 238)</p>
<p>From  eigendecomposition <a class="reference internal" href="#equation-eq-aphilambda">(6.19)</a> we can represent <span class="math notranslate nohighlight">\(\hat A\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-aform12">
<span class="eqno">(6.21)<a class="headerlink" href="#equation-eq-aform12" title="Permalink to this equation">¶</a></span>\[ 
\hat A = \Phi \Lambda \Phi^+ .
\]</div>
<p>From formula <a class="reference internal" href="#equation-eq-aform12">(6.21)</a> we can deduce the reduced dimension dynamics</p>
<div class="math notranslate nohighlight">
\[ 
\check b_{t+1} = \Lambda \check b_t 
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\check b_t &amp; = \Phi^+ X_t \cr 
X_t &amp; = \Phi \check b_t 
\end{aligned}
\]</div>
<p>But there is a better way to compute the <span class="math notranslate nohighlight">\(r \times 1\)</span> vector <span class="math notranslate nohighlight">\(\check b_t\)</span></p>
<p>In particular, the following argument from <span id="id7">[<a class="reference internal" href="zreferences.html#id26">BK19</a>]</span> (page 240) provides a computationally efficient way
to compute <span class="math notranslate nohighlight">\(\check b_t\)</span>.</p>
<p>For convenience, we’ll do this first for time <span class="math notranslate nohighlight">\(t=1\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(t=1\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-x1proj">
<span class="eqno">(6.22)<a class="headerlink" href="#equation-eq-x1proj" title="Permalink to this equation">¶</a></span>\[ 
   X_1 = \Phi \check b_1
\]</div>
<p>where <span class="math notranslate nohighlight">\(\check b_1\)</span> is an <span class="math notranslate nohighlight">\(r \times 1\)</span> vector.</p>
<p>Recall from representation 1 above that  <span class="math notranslate nohighlight">\(X_1 =  U \tilde b_1\)</span>, where <span class="math notranslate nohighlight">\(\tilde b_1\)</span> is the time <span class="math notranslate nohighlight">\(1\)</span>  basis vector for representation 1.</p>
<p>It  then follows that</p>
<div class="math notranslate nohighlight">
\[ 
  U \tilde b_1 = X' V \Sigma^{-1} W \check b_1
\]</div>
<p>and consequently</p>
<div class="math notranslate nohighlight">
\[ 
  \tilde b_1 = U^T X' V \Sigma^{-1} W \check b_1
\]</div>
<p>Since <span class="math notranslate nohighlight">\( \tilde A = U^T X' V \Sigma^{-1}\)</span>, it follows  that</p>
<div class="math notranslate nohighlight">
\[ 
  \tilde  b_1 = \tilde A W \check b_1
\]</div>
<p>and therefore, by the  eigendecomposition  <a class="reference internal" href="#equation-eq-tildeaeigen">(6.15)</a> of <span class="math notranslate nohighlight">\(\tilde A\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[ 
  \tilde b_1 = W \Lambda \check b_1
\]</div>
<p>Consesquently,</p>
<div class="math notranslate nohighlight">
\[ 
  \check b_1 = ( W \Lambda)^{-1} \tilde b_1
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-beqnsmall">
<span class="eqno">(6.23)<a class="headerlink" href="#equation-eq-beqnsmall" title="Permalink to this equation">¶</a></span>\[ 
  \check b_1 = ( W \Lambda)^{-1} U^T X_1
\]</div>
<p>which is  computationally more efficient than the following instance of our earlier equation for computing the initial vector <span class="math notranslate nohighlight">\(\check b_1\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-bphieqn">
<span class="eqno">(6.24)<a class="headerlink" href="#equation-eq-bphieqn" title="Permalink to this equation">¶</a></span>\[
  \check b_1= \Phi^{+} X_1
\]</div>
<p>Components of the  basis vector <span class="math notranslate nohighlight">\(\check b_t  = \Phi^+ X_t \equiv (W \Lambda)^{-1} U^T X_t\)</span>  are often called <strong>exact</strong> DMD nodes.</p>
<p>Conditional on <span class="math notranslate nohighlight">\(X_t\)</span>, we can construct forecasts <span class="math notranslate nohighlight">\(\overline X_{t+j} \)</span> of <span class="math notranslate nohighlight">\(X_{t+j}, j = 1, 2, \ldots, \)</span>  from
either</p>
<div class="math notranslate nohighlight" id="equation-eq-checkxevoln">
<span class="eqno">(6.25)<a class="headerlink" href="#equation-eq-checkxevoln" title="Permalink to this equation">¶</a></span>\[
\overline X_{t+j} = \Phi \Lambda^j \Phi^{+} X_t
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-checkxevoln2">
<span class="eqno">(6.26)<a class="headerlink" href="#equation-eq-checkxevoln2" title="Permalink to this equation">¶</a></span>\[ 
  \overline X_{t+j} = \Phi \Lambda^j (W \Lambda)^{-1}  U^T X_t
\]</div>
</section>
<section id="using-fewer-modes">
<h2><span class="section-number">6.14. </span>Using Fewer Modes<a class="headerlink" href="#using-fewer-modes" title="Permalink to this headline">¶</a></h2>
<p>Some of the preceding formulas assume that we have retained all <span class="math notranslate nohighlight">\(p\)</span> modes associated with the positive
singular values of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>We can easily adapt all of the formulas to describe a situation in which we instead retain only
the <span class="math notranslate nohighlight">\(r &lt; p\)</span> largest singular values.</p>
<p>In that case, we simply replace <span class="math notranslate nohighlight">\(\Sigma\)</span> with the appropriate <span class="math notranslate nohighlight">\(r \times r\)</span> matrix of singular values,
<span class="math notranslate nohighlight">\(U\)</span> with the <span class="math notranslate nohighlight">\(m \times r\)</span> matrix of whose columns correspond to the <span class="math notranslate nohighlight">\(r\)</span> largest singular values,
and <span class="math notranslate nohighlight">\(V\)</span> with the <span class="math notranslate nohighlight">\(\tilde n \times r\)</span> matrix whose columns correspond to the <span class="math notranslate nohighlight">\(r\)</span> largest  singular values.</p>
<p>Counterparts of all of the salient formulas above then apply.</p>
</section>
<section id="source-for-some-python-code">
<h2><span class="section-number">6.15. </span>Source for Some Python Code<a class="headerlink" href="#source-for-some-python-code" title="Permalink to this headline">¶</a></h2>
<p>You can find a Python implementation of DMD here:</p>
<p><a class="reference external" href="https://mathlab.github.io/PyDMD/">https://mathlab.github.io/PyDMD/</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="geom_series.html">
   1. Geometric Series for Elementary Economics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multi_hyper.html">
   2. Multivariate Hypergeometric Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sir_model.html">
   3. Modeling COVID 19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   4. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="qr_decomp.html">
   5. QR Decomposition
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   6. Singular Value Decomposition (SVD)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="complex_and_trig.html">
   7. Complex Numbers and Trigonometry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eig_circulant.html">
   8. Circulant Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   9. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="prob_meaning.html">
   10. Two Meanings of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heavy_tails.html">
   11. Heavy-Tailed Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_normal.html">
   12. Multivariate Normal Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hoist_failure.html">
   13. Fault Tree Uncertainties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="time_series_with_matrices.html">
   14. Univariate Time Series with Matrix Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="back_prop.html">
   15. Introduction to Artificial Neural Networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Programming
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lp_intro.html">
   16. Linear Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="opt_transport.html">
   17. Optimal Transport
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="von_neumann_model.html">
   18. Von Neumann Growth Model (and a Generalization)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="scalar_dynam.html">
   19. Dynamics in One Dimension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ar1_processes.html">
   20. AR1 Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="finite_markov.html">
   21. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inventory_dynamics.html">
   22. Inventory Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_models.html">
   23. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="samuelson.html">
   24. Samuelson Multiplier-Accelerator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kesten_processes.html">
   25. Kesten Processes and Firm Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wealth_dynamics.html">
   26. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kalman.html">
   27. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="short_path.html">
   28. Shortest Paths
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_1.html">
   29. Cass-Koopmans Planning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cass_koopmans_2.html">
   30. Cass-Koopmans Competitive Equilibrium
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Search
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model.html">
   31. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_model_with_separation.html">
   32. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_fitted_vfi.html">
   33. Job Search III: Fitted Value Function Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mccall_correlated.html">
   34. Job Search IV: Correlated Wage Offers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="career.html">
   35. Job Search V: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jv.html">
   36. Job Search VI: On-the-Job Search
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Consumption, Savings and Growth
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_problem.html">
   37. Cake Eating I: Introduction to Optimal Saving
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cake_eating_numerical.html">
   38. Cake Eating II: Numerical Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth.html">
   39. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optgrowth_fast.html">
   40. Optimal Growth II: Accelerating the Code with Numba
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coleman_policy_iter.html">
   41. Optimal Growth III: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="egm_policy_iter.html">
   42. Optimal Growth IV: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp.html">
   43. The Income Fluctuation Problem I: Basic Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ifp_advanced.html">
   44. The Income Fluctuation Problem II: Stochastic Returns on Assets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Information
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="odu.html">
   45. Job Search VII: Search with Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_ratio_process.html">
   46. Likelihood Ratio Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="imp_sample.html">
   47. Computing Mean of a Likelihood Ratio Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wald_friedman.html">
   48. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exchangeable.html">
   49. Exchangeability and Bayesian Updating
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="likelihood_bayes.html">
   50. Likelihood Ratio Processes and Bayesian Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="navy_captain.html">
   51. Bayesian versus Frequentist Decision Rules
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lqcontrol.html">
   52. LQ Control: Foundations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lagrangian_lqdp.html">
   53. Lagrangian for LQ Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cross_product_trick.html">
   54. Eliminating Cross Products
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income.html">
   55. The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perm_income_cons.html">
   56. Permanent Income II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lq_inventories.html">
   57. Production Smoothing via Inventories
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="schelling.html">
   58. Schelling’s Segregation Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lake_model.html">
   59. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rational_expectations.html">
   60. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="re_with_feedback.html">
   61. Stability in Linear Rational Expectations Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_perf.html">
   62. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncertainty_traps.html">
   63. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="aiyagari.html">
   64. The Aiyagari Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="markov_asset.html">
   65. Asset Pricing: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ge_arrow.html">
   66. Competitive Equilibria with Arrow Securities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrison_kreps.html">
   67. Heterogeneous Beliefs and Bubbles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data and Empirics
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pandas_panel.html">
   68. Pandas for Panel Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ols.html">
   69. Linear Regression in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mle.html">
   70. Maximum Likelihood Estimation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Auctions
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="two_auctions.html">
   71. First-Price and Second-Price Auctions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="house_auction.html">
   72. Multiple Good Allocation Mechanisms
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   73. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   74. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   75. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                    <!-- <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="qe-toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="_notebooks/svd_intro.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python.myst/tree/master/lectures/svd_intro.md" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="_pdf/quantecon-python.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://mybinder.org/v2/gh/QuantEcon/lecture-python.notebooks/master?urlpath=tree/svd_intro.ipynb">BinderHub</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python.notebooks" data-urlpath="tree/lecture-python.notebooks/svd_intro.ipynb" data-branch=master>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://mybinder.org/v2/gh/QuantEcon/lecture-python.notebooks/master?urlpath=tree/svd_intro.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "svd_intro";
                const repoURL = "https://github.com/QuantEcon/lecture-python.notebooks";
                const urlPath = "tree/lecture-python.notebooks/svd_intro.ipynb";
                const branch = "master"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-54984338-10', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>